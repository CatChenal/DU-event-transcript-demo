{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Run the first cell! (collapsed in JupyterLab)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path added to sys.path: C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/resources/EventManagement\n",
      "\n",
      "Python ver: 3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]\n",
      "Python env: du37\n",
      "OS:         win32\n",
      "Current dir: C:\\Users\\catch\\Documents\\GitHub\\DU-event-transcript-demo\\resources\\EventManagement\\notebooks\n",
      "\n",
      "Last updated: 2021-10-11T09:27:18.499184-04:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.7.11\n",
      "IPython version      : 7.27.0\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "pandas: 1.3.3\n",
      "numpy : 1.20.3\n",
      "sys   : 3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To get multiple outputs from one code cell (without using print()):\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython import get_ipython\n",
    "from IPython.display import HTML, Markdown, Image\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# For documenting the current environment:\n",
    "def sys_info():\n",
    "    frmt = '\\nPython ver: {}\\nPython env: {}\\n'\n",
    "    frmt += 'OS:         {}\\nCurrent dir: {}\\n'\n",
    "    print(frmt.format(sys.version, \n",
    "                      Path(sys.prefix).name,\n",
    "                      sys.platform,\n",
    "                      Path.cwd()))\n",
    "\n",
    "# For enabling imports from current project code:\n",
    "def add_to_sys_path(this_path, up=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Prepend this_path to sys.path.\n",
    "    If up=True, path refers to parent folder (1 level up).\n",
    "    \"\"\"\n",
    "    newp = Path(this_path).as_posix() # no str method (?)\n",
    "    if up:\n",
    "        newp = Path(this_path).parent.as_posix()\n",
    "\n",
    "    msg = F'Path already in sys.path: {newp}'\n",
    "    if newp not in sys.path:\n",
    "        sys.path.insert(1, newp)\n",
    "        msg = F'Path added to sys.path: {newp}'\n",
    "    if verbose:\n",
    "        print(msg)\n",
    "\n",
    "# If this ipynb file is inside a folder, eg ./notebooks, \n",
    "# the project code is assumed to reside 1 level up:\n",
    "nb_folder = 'notebooks'\n",
    "add_to_sys_path(Path.cwd(), up=Path.cwd().name.startswith(nb_folder))\n",
    "\n",
    "\n",
    "# For py modules/methods discovery:\n",
    "def filter_dir(mdl, filter_str=None, start_with_str='_', exclude=True):\n",
    "    \"\"\"Filter dir(mdl) for method discovery.\n",
    "       Input:\n",
    "       :param mdl (object): module, optionally with submodule path(s), e.g. mdl.submdl1.submdl2.\n",
    "       :param filter_str (str, None): filter all method names containing that string.\n",
    "       :param start_with_str (str, '_'), exclude (bool, True): start_with_str and exclude work \n",
    "              together to perform search on non-dunder methods (default).\n",
    "       Example:\n",
    "       >filter_dir(re) # lists the public methods of the re module.\n",
    "    \"\"\"\n",
    "    search_dir = [d for d in dir(mdl) if not d.startswith(start_with_str) == exclude]\n",
    "    if filter_str is None:\n",
    "        return search_dir\n",
    "    else:\n",
    "        filter_str = filter_str.lower()\n",
    "        return [d for d in search_dir if d.lower().find(filter_str) != -1]\n",
    "\n",
    "# To create often-used subfolders:\n",
    "def get_project_dirs(which=['data', 'images'],\n",
    "                     use_parent=True):\n",
    "    '''Create folder(s) named in `which` at the ipynb parent level.'''\n",
    "    if use_parent:\n",
    "        dir_fn = Path.cwd().parent.joinpath\n",
    "    else:\n",
    "        dir_fn = Path.cwd().joinpath\n",
    "        \n",
    "    dir_lst = []    \n",
    "    for d in which:\n",
    "        DIR = dir_fn(d)\n",
    "        if not DIR.exists():\n",
    "            Path.mkdir(DIR)\n",
    "        dir_lst.append(DIR)\n",
    "    return dir_lst\n",
    "\n",
    "#DIR_DATA, DIR_IMG = get_project_dirs()\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.set_option(\"display.max_colwidth\", 200)\n",
    "from pprint import pprint as pp\n",
    "\n",
    "\n",
    "# For documenting the current environment:\n",
    "def show_versions():\n",
    "    txt = '<pre><br>'\n",
    "    txt += F'Python:\\t\\t{sys.version}<br>'\n",
    "    txt += F'Python env:\\t{Path(sys.prefix).name}<br>'\n",
    "    txt += F'Numpy:\\t\\t{np.__version__}<br>'\n",
    "    txt += F'Scipy:\\t\\t{sp.__version__}<br>'\n",
    "    txt += F'Pandas:\\t\\t{pd.__version__}<br>'\n",
    "    txt += F'Matplotlib:\\t{mpl.__version__}<br>'\n",
    "    txt += F'Currrent dir: {Path.cwd()}'\n",
    "    txt += '</pre>'\n",
    "    div = f\"\"\"<div class=\"alert alert-info\"><b>Versions:</b><br>{txt}</div>\"\"\"\n",
    "    return HTML(div)\n",
    "\n",
    "\n",
    "# autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "#..................\n",
    "sys_info()\n",
    "\n",
    "no_wmark = False\n",
    "try:\n",
    "    %load_ext watermark\n",
    "    %watermark\n",
    "except ModuleNotFoundError:\n",
    "    no_wmark = True\n",
    "\n",
    "if no_wmark:\n",
    "    show_versions()\n",
    "else:\n",
    "    %watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manage import (EventMeta as Meta,\n",
    "                    EventTranscription as TRX,\n",
    "                    Controls as CTR,\n",
    "                    Utils as UTL,\n",
    "                    Audit as AUD)\n",
    "\n",
    "from collections import OrderedDict, Counter\n",
    "import ipywidgets as ipw\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e68de6d4a94b59be0f477dd0cfb1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<em>Vertical Box Example</em>'), IntSlider(value=0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slider_title = ipw.HTML('<em>Vertical Box Example</em>')\n",
    "slider = ipw.IntSlider()\n",
    "ipw.VBox([slider_title, slider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manage.tests import test_EventMeta as testMeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add dummy event:\n",
      " - video url: https://www.youtube.com/watch?v=MHAjCcBfT_A\n",
      " - meetup url: https://www.meetup.com/data-umbrella/events/274778387/\n",
      ".update_dict - Assigning new dummy event dict to Meta object...\n",
      "Updated dict:\n",
      "OrderedDict([('presenter', 'Cat Chenal, Reshama Shaikh'), ('title', 'Automating Audio Transcription.'), ('event_url', 'https://www.meetup.com/data-umbrella/events/274778387/'), ('yt_video_id', 'MHAjCcBfT_A'), ('slides_url', 'N.A.'), ('repo_url', 'https://github.com/CatChenal'), ('notebook_url', 'N.A.'), ('transcriber', '?'), ('extra_references', '## Other References\\n- Binder:  <url>\\n- Paper:  <Paper url or citation>  \\n- Wiki:  This is an excellent [wiki on audio foo](http://en.wikipedia.org/wiki/Main_Page)  \\n'), ('video_href', 'http://www.youtube.com/watch?feature=player_embedded&v=MHAjCcBfT_A'), ('video_href_src', 'http://img.youtube.com/vi/MHAjCcBfT_A/0.jpg'), ('video_href_alt', 'Automating Audio Transcription.'), ('video_href_w', '25%'), ('formatted_transcript', 'N.A.'), ('year', 2021), ('idn', '15'), ('video_url', 'https://www.youtube.com/watch?v=MHAjCcBfT_A'), ('title_kw', 'audio-foo'), ('transcript_md', '15-cat-reshama-audio-foo.md'), ('audio_track', WindowsPath('C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/resources/EventManagement/data/meta/2021_15_MHAjCcBfT_A.mp4')), ('audio_text', WindowsPath('C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/resources/EventManagement/data/meta/2021_15_MHAjCcBfT_A.txt')), ('has_transcript', False), ('trans_idx', 730), ('status', 'Not yet processed (editor needed)'), ('notes', 'Dummy entry for demo.'), ('video_embed', '\\n<iframe width=\"560\" height=\"315\" \\n        src=\"https://www.youtube-nocookie.com/embed/MHAjCcBfT_A?cc_load_policy=1&autoplay=0\" \\n        frameborder=\"0\">\\n</iframe>\\n')])\n",
      ".update_readme() - Saving Readme file...\n",
      ".save_transcript_md - Saving Transcript Markdown file...\n"
     ]
    }
   ],
   "source": [
    "dum = testMeta.test_add_event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Speakers: Cat Chenal and  Reshama Shaikh \n",
       "- Transcript:  https://github.com/data-umbrella/event-transcripts/blob/main/15-cat-reshama-audio-foo.md \n",
       "- Meetup Event:  https://www.meetup.com/data-umbrella/events/274778387/ \n",
       "- Video:  https://www.youtube.com/watch?v=MHAjCcBfT_A \n",
       "## Other References\n",
       "- Binder:  <url>\n",
       "- Paper:  <Paper url or citation>  \n",
       "- Wiki:  This is an excellent [wiki on audio foo](http://en.wikipedia.org/wiki/Main_Page)  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(dum.get_video_desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Test: text wrap fn as externally defined TW obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08-cat-reshama-audio-demo.md\n"
     ]
    }
   ],
   "source": [
    "# Instantiate existing event\n",
    "idn, year = 8, 2021\n",
    "\n",
    "tm = Meta.TranscriptMeta(idn, year)\n",
    "print(tm.event_dict['transcript_md'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4131"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_trx = tm.event_dict['formatted_transcript']\n",
    "new_txt = tm.redo_transcript_cleanup(formatted_trx)\n",
    "\n",
    "i = new_txt.find('Bayesian')\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<!-- Editing Guide: The pipe (|) position in this comment is 120:                                                       | -->\n",
       "### Introduction\n",
       "\n",
       "Okay hello and welcome to Data Umbrella's webinar for October so I'm just going to go over the agenda I'm going to do a  \n",
       "brief introduction then there will be the workshop by Hugo and James and you can ask questions along the way in the chat  \n",
       "or actually the best place to ask questions is the Q&A and there's an option to upvote as well so yet asking the Q&A if  \n",
       "you happen to post it on the chat by mistake I can also transfer it over to Q&A so that would be fine too and this  \n",
       "webinar is being recorded briefly about me I am a statistician and data scientist and I am the founder of Data Umbrella  \n",
       "I am on a lot of platforms as Reshama so feel free to follow me on Twitter and LinkedIn we have a code of conduct we're  \n",
       "dedicated to providing harassment free experience for everyone thank you for helping to make this a welcoming friendly  \n",
       "professional community for all "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/markdown": [
       "<!-- Editing Guide: The pipe (|) position in this comment is 120:                                                       | -->\n",
       "### Introduction\n",
       "\n",
       "Okay hello and welcome to Data Umbrella's webinar for October so I'm just going to go over the agenda I'm going to do a  \n",
       "brief introduction then there will be the workshop by Hugo and James and you can ask questions along the way in the chat  \n",
       "or actually the best place to ask questions is the Q&A and there's an option to upvote as well so yet asking the Q&A if  \n",
       "you happen to post it on the chat by mistake I can also transfer it over to Q&A so that would be fine too and this  \n",
       "webinar is being recorded briefly about me I am a statistician and data scientist and I am the founder of Data Umbrella  \n",
       "I am on a lot of platforms as Reshama so feel free to follow me on Twitter and LinkedIn we have a code of conduct we're  \n",
       "dedicated to providing harassment free experience for everyone thank you for helping to make this a welcoming friendly  \n",
       "professional community for all "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(formatted_trx[:1024])\n",
    "Markdown(new_txt[:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"\"\"I gave either at the end of February or early March was data umbrella's inaugural  \n",
    "tutorial and Meetup if I recall correctly on bayesian thinking and hacker statistics and simulation and  \n",
    "that type of stuff so it's just wonderful to be back particularly with my colleague and friend James we're  \n",
    "building really cool distributed data science products at coiled we'll say a bit about that but we'll do some  \n",
    "introductions in a bit I just wanted to get you all accustomed to it was February thank you Reshama we're working \"\"\"\n",
    "\n",
    "formatted_trx == new_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update text_processing files (see How_Tos.ipynb)\n",
    "Do these with gui:\n",
    "```\n",
    "# For the 'upper' file, textbox entry is list of lower-case names/acronyms to upper-case\n",
    "upper\tnyc\n",
    "# For the 'corrections' file, textbox entry is a list of tuples:\n",
    "corrections\t [('ipad', iPad'),('iphone', iPhone'),('coyle','coil'),('job lib', 'joblib'),('dars', 'Dask')]\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_people = ['theodore', 'alex']\n",
    "TRX.update_conversions('people', new_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "uppercase_list = TRX.readcsv(TRX.upper_file).upper.tolist()\n",
    "titlecase_list = (TRX.readcsv(TRX.people_file).people.tolist()\n",
    "                + TRX.readcsv(TRX.names_file).names.tolist()\n",
    "                + TRX.readcsv(TRX.places_file).places.tolist())\n",
    "corrections = TRX.get_corrections_dict()\n",
    "\n",
    "current_txt = tm.event_dict['formatted_transcript']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Test: New transcript\n",
    "## 5 steps to new transcript:\n",
    "```\n",
    "# 0. Instantiate new event\n",
    "tm = Meta.TranscriptMeta()\n",
    "\n",
    "# 1. Get dummy or actual data:\n",
    "# Load the starter kv pairs\n",
    "demo_kvs = Meta.dummy_kv_pairs.copy()\n",
    "# Modify at will\n",
    "demo_kvs.append(('transcriber', 'cat chenal'))\n",
    "d = Meta.get_dummy_data(new_kv_pairs=demo_kvs)\n",
    "\n",
    "# 2. update starter event_dict:\n",
    "tm.update_dict(d)\n",
    "# updated:\n",
    "#tm.event_dict\n",
    "\n",
    "# 3. update_readme()\n",
    "tm.update_readme()\n",
    "\n",
    "# 4. save_transcript_md()\n",
    "tm.save_transcript_md()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK: New event in current year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Instantiate new event\n",
    "tm = Meta.TranscriptMeta()\n",
    "\n",
    "# 1. Get dummy or actual data:\n",
    "# Load the starter kv pairs\n",
    "demo_kvs = Meta.dummy_kv_pairs.copy()\n",
    "# modify\n",
    "demo_kvs.append(('transcriber', 'max chenal'))\n",
    "d = Meta.get_dummy_data(new_kv_pairs=demo_kvs)\n",
    "\n",
    "# 2. update starter event_dict:\n",
    "tm.update_dict(d)\n",
    "# updated:\n",
    "#tm.event_dict\n",
    "\n",
    "# 3. update_readme()\n",
    "tm.update_readme()\n",
    "\n",
    "# 4. save_transcript_md()\n",
    "tm.save_transcript_md()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in Meta:\n",
    "dummy_kv_pairs = [('presenter', 'cat Chenal, Reshama Shaikh'),\n",
    "                  ('title','Automating Audio tanscription.'),\n",
    "                  ('title_kw','audio demo'),\n",
    "                  ('video_url','https://youtu.be/MHAjCcBfT_A'),\n",
    "                  ('extra_references', \n",
    "                  \"\"\"## Other References\\n\n",
    "                  - Binder:  <url>\\n- Paper:  <Paper url or citation>  \\n\n",
    "                  - Wiki:  This is an excellent [wiki]  \\n\n",
    "                  (http://en.wikipedia.org/wiki/Main_Page)  \\n\"\"\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK: New event in other year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2020', '24-cat-chenal-foo-demo.md')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Talk Transcript</th>\n",
       "      <th>Transcriber</th>\n",
       "      <th>Status</th>\n",
       "      <th>Notes</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>Hugo Bowne-Anderson</td>\n",
       "      <td>Bayesian Data Science</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>Not recorded</td>\n",
       "      <td></td>\n",
       "      <td>N.A.</td>\n",
       "      <td>N.A.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02</td>\n",
       "      <td>Bruno Goncalves</td>\n",
       "      <td>Time Series Modeling</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>Not recorded</td>\n",
       "      <td></td>\n",
       "      <td>N.A.</td>\n",
       "      <td>N.A.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03</td>\n",
       "      <td>Ty Shaikh</td>\n",
       "      <td>[Webscraping Poshmark](2020/03-ty-shaikh-websc...</td>\n",
       "      <td>Ty Shaikh</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>03-ty-shaikh-webscraping.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04</td>\n",
       "      <td>Ali Spittel</td>\n",
       "      <td>[Navigating Your Tech Career](2020/04-ali-spit...</td>\n",
       "      <td>Janine</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>04-ali-spittel-career.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05</td>\n",
       "      <td>Andreas Mueller</td>\n",
       "      <td>[Crash Course in Contributing to Scikit-learn]...</td>\n",
       "      <td>Reshama Shaikh</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>05-andreas-mueller-contributing.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>06</td>\n",
       "      <td>Reshama Shaikh</td>\n",
       "      <td>[Example PR for Scikit-learn](2020/06-reshama-...</td>\n",
       "      <td>Reshama Shaikh, Mark</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>06-reshama-shaikh-sklearn-pr.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>07</td>\n",
       "      <td>Shailvi Wakhlu</td>\n",
       "      <td>[Fixing Bad Data and Using SQL](2020/07-shailv...</td>\n",
       "      <td>Juanita</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>07-shailvi-wakhlu-fixing-data.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08</td>\n",
       "      <td>Matt Brems</td>\n",
       "      <td>[Data Science with Missing Data](2020/08-matt-...</td>\n",
       "      <td>Barbara</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>08-matt-brems-missing-data.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>09</td>\n",
       "      <td>Sam Bail</td>\n",
       "      <td>[Intro to Terminal](2020/09-sam-bail-terminal.md)</td>\n",
       "      <td>Isaack</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>09-sam-bail-terminal.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Emily Robinson</td>\n",
       "      <td>[Build a Career in Data Science](2020/10-emily...</td>\n",
       "      <td>Kevin</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>10-emily-robinson-career.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Rebecca Kelly</td>\n",
       "      <td>[Kdb Time Series Database](2020/11-rebecca-kel...</td>\n",
       "      <td>Coretta</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td>Paragraphs are too long</td>\n",
       "      <td>2020</td>\n",
       "      <td>11-rebecca-kelly-kdb.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Mridu Bhatnagar</td>\n",
       "      <td>[Build a Bot](2020/12-mridu-bhatnagar-bot.md)</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>12-mridu-bhatnagar-bot.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Liz DiLuzio</td>\n",
       "      <td>[Creating Nimble Data Processes](2020/13-liz-d...</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>13-liz-diluzio-data-process.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Megan Robertson</td>\n",
       "      <td>[3 Lessons From 3 Years of Data Science](2020/...</td>\n",
       "      <td>Sethupathy</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td>Headers should not be in capital letters, etc</td>\n",
       "      <td>2020</td>\n",
       "      <td>14-megan-robertson-career.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Emma Gouillart</td>\n",
       "      <td>[Data Visualization with Plotly](2020/15-emma-...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>15-emma-gouillart-plotly.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Hugo Bowne-Anderson, James Bourbeau</td>\n",
       "      <td>[Data Science and Machine Learning at Scale](2...</td>\n",
       "      <td>Cynthia</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>16-hugo-james-dask.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Carol Willing</td>\n",
       "      <td>[Contributing to Core Python](2020/17-carol-wi...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>17-carol-willing-python.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Thomas Fan</td>\n",
       "      <td>[Streamlit for Data Science](2020/18-thomas-fa...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>18-thomas-fan-streamlit.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Matti Picus</td>\n",
       "      <td>[Contributing to NumPy](2020/19-matti-picus-nu...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>19-matti-picus-numpy.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Marco Gorelli</td>\n",
       "      <td>[Contributing to pandas](2020/20-marco-gorelli...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>20-marco-gorelli-pandas.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Cat Chenal, Reshama Shaikh</td>\n",
       "      <td>[Automating Audio Tanscription.](2020/21-cat-r...</td>\n",
       "      <td>Sing Song</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>21-cat-reshama-audio-demo.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Cat Chenal, Reshama Shaikh</td>\n",
       "      <td>[Automating Audio Tanscription.](2020/22-cat-r...</td>\n",
       "      <td>Sam Tell</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>22-cat-reshama-new-demo.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Cat Chenal</td>\n",
       "      <td>[Automating Audio Tanscription.](2020/23-cat-c...</td>\n",
       "      <td>Bill Yip</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>23-cat-chenal-foo-demo.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Cat Chenal</td>\n",
       "      <td>[Automating Audio Tanscription.](2020/24-cat-c...</td>\n",
       "      <td>Billy Pip</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>24-cat-chenal-foo-demo.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>01</td>\n",
       "      <td>Nick Janetakis</td>\n",
       "      <td>[Creating a Command Line Focused Development E...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2021</td>\n",
       "      <td>01-nick-janetakis-command.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>02</td>\n",
       "      <td>Cat Chenal, Sing Song</td>\n",
       "      <td>[Better Demo Presentation Ii](2021/02-cat-sing...</td>\n",
       "      <td>Samtoo Me</td>\n",
       "      <td>Partial (w.i.p.)</td>\n",
       "      <td>Test for existing update from EventMeta.class</td>\n",
       "      <td>2021</td>\n",
       "      <td>02-cat-sing-demo-foo.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>03</td>\n",
       "      <td>Cat Chenal</td>\n",
       "      <td>[Brand New Year!](2021/03-cat-chenal-bar-demo.md)</td>\n",
       "      <td>me too</td>\n",
       "      <td>Partial (w.i.p.)</td>\n",
       "      <td>Test for updating exisitng.</td>\n",
       "      <td>2021</td>\n",
       "      <td>03-cat-chenal-bar-demo.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>04</td>\n",
       "      <td>Cat Chenal, Reshama Shaik</td>\n",
       "      <td>[My Demo Presentation](2021/04-cat-reshama-dem...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td>Dummy event 2</td>\n",
       "      <td>2021</td>\n",
       "      <td>04-cat-reshama-demo-foo.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>05</td>\n",
       "      <td>Mini Max</td>\n",
       "      <td>[New Demo Presentation](2021/05-mini-max-demo-...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td>Dummy event</td>\n",
       "      <td>2021</td>\n",
       "      <td>05-mini-max-demo-baz.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>06</td>\n",
       "      <td>Cat Chenal, Reshama Shaikh</td>\n",
       "      <td>[Automating Audio Tanscription.](2021/06-cat-r...</td>\n",
       "      <td>Mae Song</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2021</td>\n",
       "      <td>06-cat-reshama-better-demo.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     N                              Speaker  \\\n",
       "0   01                  Hugo Bowne-Anderson   \n",
       "1   02                      Bruno Goncalves   \n",
       "2   03                            Ty Shaikh   \n",
       "3   04                          Ali Spittel   \n",
       "4   05                      Andreas Mueller   \n",
       "5   06                       Reshama Shaikh   \n",
       "6   07                       Shailvi Wakhlu   \n",
       "7   08                           Matt Brems   \n",
       "8   09                             Sam Bail   \n",
       "9   10                       Emily Robinson   \n",
       "10  11                        Rebecca Kelly   \n",
       "11  12                      Mridu Bhatnagar   \n",
       "12  13                          Liz DiLuzio   \n",
       "13  14                      Megan Robertson   \n",
       "14  15                       Emma Gouillart   \n",
       "15  16  Hugo Bowne-Anderson, James Bourbeau   \n",
       "16  17                        Carol Willing   \n",
       "17  18                           Thomas Fan   \n",
       "18  19                          Matti Picus   \n",
       "19  20                        Marco Gorelli   \n",
       "20  21           Cat Chenal, Reshama Shaikh   \n",
       "21  22           Cat Chenal, Reshama Shaikh   \n",
       "22  23                           Cat Chenal   \n",
       "23  24                           Cat Chenal   \n",
       "24  01                       Nick Janetakis   \n",
       "25  02                Cat Chenal, Sing Song   \n",
       "26  03                           Cat Chenal   \n",
       "27  04            Cat Chenal, Reshama Shaik   \n",
       "28  05                             Mini Max   \n",
       "29  06           Cat Chenal, Reshama Shaikh   \n",
       "\n",
       "                                      Talk Transcript           Transcriber  \\\n",
       "0                               Bayesian Data Science                  N.A.   \n",
       "1                                Time Series Modeling                  N.A.   \n",
       "2   [Webscraping Poshmark](2020/03-ty-shaikh-websc...             Ty Shaikh   \n",
       "3   [Navigating Your Tech Career](2020/04-ali-spit...                Janine   \n",
       "4   [Crash Course in Contributing to Scikit-learn]...        Reshama Shaikh   \n",
       "5   [Example PR for Scikit-learn](2020/06-reshama-...  Reshama Shaikh, Mark   \n",
       "6   [Fixing Bad Data and Using SQL](2020/07-shailv...               Juanita   \n",
       "7   [Data Science with Missing Data](2020/08-matt-...               Barbara   \n",
       "8   [Intro to Terminal](2020/09-sam-bail-terminal.md)                Isaack   \n",
       "9   [Build a Career in Data Science](2020/10-emily...                 Kevin   \n",
       "10  [Kdb Time Series Database](2020/11-rebecca-kel...               Coretta   \n",
       "11      [Build a Bot](2020/12-mridu-bhatnagar-bot.md)                     ?   \n",
       "12  [Creating Nimble Data Processes](2020/13-liz-d...                  Lily   \n",
       "13  [3 Lessons From 3 Years of Data Science](2020/...            Sethupathy   \n",
       "14  [Data Visualization with Plotly](2020/15-emma-...                     ?   \n",
       "15  [Data Science and Machine Learning at Scale](2...               Cynthia   \n",
       "16  [Contributing to Core Python](2020/17-carol-wi...                     ?   \n",
       "17  [Streamlit for Data Science](2020/18-thomas-fa...                     ?   \n",
       "18  [Contributing to NumPy](2020/19-matti-picus-nu...                     ?   \n",
       "19  [Contributing to pandas](2020/20-marco-gorelli...                     ?   \n",
       "20  [Automating Audio Tanscription.](2020/21-cat-r...             Sing Song   \n",
       "21  [Automating Audio Tanscription.](2020/22-cat-r...              Sam Tell   \n",
       "22  [Automating Audio Tanscription.](2020/23-cat-c...              Bill Yip   \n",
       "23  [Automating Audio Tanscription.](2020/24-cat-c...             Billy Pip   \n",
       "24  [Creating a Command Line Focused Development E...                     ?   \n",
       "25  [Better Demo Presentation Ii](2021/02-cat-sing...             Samtoo Me   \n",
       "26  [Brand New Year!](2021/03-cat-chenal-bar-demo.md)                me too   \n",
       "27  [My Demo Presentation](2021/04-cat-reshama-dem...                     ?   \n",
       "28  [New Demo Presentation](2021/05-mini-max-demo-...                     ?   \n",
       "29  [Automating Audio Tanscription.](2021/06-cat-r...              Mae Song   \n",
       "\n",
       "                               Status  \\\n",
       "0                        Not recorded   \n",
       "1                        Not recorded   \n",
       "2                      Needs reviewer   \n",
       "3                      Needs reviewer   \n",
       "4                            Complete   \n",
       "5                            Complete   \n",
       "6                            Complete   \n",
       "7                      Needs reviewer   \n",
       "8                            Complete   \n",
       "9                            Complete   \n",
       "10                     Needs reviewer   \n",
       "11  Not yet processed (editor needed)   \n",
       "12                           Complete   \n",
       "13                     Needs reviewer   \n",
       "14  Not yet processed (editor needed)   \n",
       "15                     Needs reviewer   \n",
       "16  Not yet processed (editor needed)   \n",
       "17  Not yet processed (editor needed)   \n",
       "18  Not yet processed (editor needed)   \n",
       "19  Not yet processed (editor needed)   \n",
       "20  Not yet processed (editor needed)   \n",
       "21  Not yet processed (editor needed)   \n",
       "22  Not yet processed (editor needed)   \n",
       "23  Not yet processed (editor needed)   \n",
       "24  Not yet processed (editor needed)   \n",
       "25                   Partial (w.i.p.)   \n",
       "26                   Partial (w.i.p.)   \n",
       "27  Not yet processed (editor needed)   \n",
       "28  Not yet processed (editor needed)   \n",
       "29  Not yet processed (editor needed)   \n",
       "\n",
       "                                            Notes  year  \\\n",
       "0                                                  N.A.   \n",
       "1                                                  N.A.   \n",
       "2                                                  2020   \n",
       "3                                                  2020   \n",
       "4                                                  2020   \n",
       "5                                                  2020   \n",
       "6                                                  2020   \n",
       "7                                                  2020   \n",
       "8                                                  2020   \n",
       "9                                                  2020   \n",
       "10                        Paragraphs are too long  2020   \n",
       "11                                                 2020   \n",
       "12                                                 2020   \n",
       "13  Headers should not be in capital letters, etc  2020   \n",
       "14                                                 2020   \n",
       "15                                                 2020   \n",
       "16                                                 2020   \n",
       "17                                                 2020   \n",
       "18                                                 2020   \n",
       "19                                                 2020   \n",
       "20                                                 2020   \n",
       "21                                                 2020   \n",
       "22                                                 2020   \n",
       "23                                                 2020   \n",
       "24                                                 2021   \n",
       "25  Test for existing update from EventMeta.class  2021   \n",
       "26                    Test for updating exisitng.  2021   \n",
       "27                                  Dummy event 2  2021   \n",
       "28                                    Dummy event  2021   \n",
       "29                                                 2021   \n",
       "\n",
       "                                  name  \n",
       "0                                 N.A.  \n",
       "1                                 N.A.  \n",
       "2          03-ty-shaikh-webscraping.md  \n",
       "3             04-ali-spittel-career.md  \n",
       "4   05-andreas-mueller-contributing.md  \n",
       "5      06-reshama-shaikh-sklearn-pr.md  \n",
       "6     07-shailvi-wakhlu-fixing-data.md  \n",
       "7        08-matt-brems-missing-data.md  \n",
       "8              09-sam-bail-terminal.md  \n",
       "9          10-emily-robinson-career.md  \n",
       "10             11-rebecca-kelly-kdb.md  \n",
       "11           12-mridu-bhatnagar-bot.md  \n",
       "12      13-liz-diluzio-data-process.md  \n",
       "13        14-megan-robertson-career.md  \n",
       "14         15-emma-gouillart-plotly.md  \n",
       "15               16-hugo-james-dask.md  \n",
       "16          17-carol-willing-python.md  \n",
       "17          18-thomas-fan-streamlit.md  \n",
       "18             19-matti-picus-numpy.md  \n",
       "19          20-marco-gorelli-pandas.md  \n",
       "20        21-cat-reshama-audio-demo.md  \n",
       "21          22-cat-reshama-new-demo.md  \n",
       "22           23-cat-chenal-foo-demo.md  \n",
       "23           24-cat-chenal-foo-demo.md  \n",
       "24        01-nick-janetakis-command.md  \n",
       "25             02-cat-sing-demo-foo.md  \n",
       "26           03-cat-chenal-bar-demo.md  \n",
       "27          04-cat-reshama-demo-foo.md  \n",
       "28             05-mini-max-demo-baz.md  \n",
       "29       06-cat-reshama-better-demo.md  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0. Instantiate new event\n",
    "tm = Meta.TranscriptMeta()\n",
    "\n",
    "# 1. Get dummy or actual data:\n",
    "# Load the starter kv pairs\n",
    "demo_kvs = Meta.dummy_kv_pairs.copy()\n",
    "# modify\n",
    "demo_kvs[0] = (demo_kvs[0][0], 'cat chenal')\n",
    "demo_kvs[2] = (demo_kvs[2][0], 'foo demo')\n",
    "demo_kvs.append(('transcriber', 'billy Pip'))\n",
    "\n",
    "d = Meta.get_dummy_data(year=2020,\n",
    "                        new_kv_pairs=demo_kvs)\n",
    "\n",
    "# 2. update starter event_dict:\n",
    "tm.update_dict(d)\n",
    "# updated:\n",
    "tm.event_dict['year'], tm.event_dict['transcript_md']\n",
    "\n",
    "# 3. update_readme()\n",
    "tm.update_readme()\n",
    "\n",
    "# 4. save_transcript_md()\n",
    "tm.save_transcript_md()\n",
    "\n",
    "tm.df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mdfile = Meta.REPO_PATH.joinpath(Meta.MAIN_README)\n",
    "UTL.show_md_file(mdfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, tbl_delims = Meta.df_from_readme_tbl()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test: update of existing transcript: need to keep same year & idn\n",
    "\n",
    "##  steps to update transcript:\n",
    "```\n",
    "# 0. Instantiate existing event\n",
    "year = 2021\n",
    "idn = '06'  # or 6\n",
    "tm = Meta.TranscriptMeta(idn, year)\n",
    "\n",
    "# 1. Get dummy or actual data, keeping idn:\n",
    "# Load the starter kv pairs\n",
    "demo_kvs = Meta.dummy_kv_pairs.copy()\n",
    "demo_kvs.append(('idn', idn))\n",
    "\n",
    "# Modify at will\n",
    "demo_kvs.append(('transcriber', 'sing song'))\n",
    "# change kw to get new file:\n",
    "demo_kvs[2] = (demo_kvs[2][0], 'better demo')\n",
    "\n",
    "d = Meta.get_dummy_data(new_kv_pairs=demo_kvs)\n",
    "\n",
    "# 2. update starter event_dict:\n",
    "tm.update_dict(d)\n",
    "tm.to_delete  # should not be None\n",
    "\n",
    "# 3. update_readme()\n",
    "tm.update_readme()\n",
    "\n",
    "# 4. save_transcript_md()\n",
    "tm.save_transcript_md()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2021', '06')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0. Instantiate existing event\n",
    "year = 2021\n",
    "idn = '06'  # or 6\n",
    "tm = Meta.TranscriptMeta(idn, year)\n",
    "\n",
    "\n",
    "# 1. Get dummy or actual data, keeping idn:\n",
    "\n",
    "# Load the starter kv pairs\n",
    "demo_kvs = Meta.dummy_kv_pairs.copy()\n",
    "demo_kvs.append(('idn', idn))\n",
    "# Modify at will\n",
    "demo_kvs.append(('transcriber', 'mae song'))\n",
    "# change kw to get new file:\n",
    "demo_kvs[2] = (demo_kvs[2][0], 'better demo')\n",
    "\n",
    "d = Meta.get_dummy_data(new_kv_pairs=demo_kvs)\n",
    "d['year'], d['idn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/2021/06-cat-reshama-new-demo.md')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. update starter event_dict:\n",
    "tm.update_dict(d)\n",
    "tm.to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. update_readme()\n",
    "tm.update_readme()\n",
    "\n",
    "# 4. save_transcript_md()\n",
    "tm.save_transcript_md()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# DONE: Test: Trap missing tbl delimiters in README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Audit: coverage of split_url\n",
    "RE: regex in get_id_from_YT_url not working for all md files\n",
    "## Q1: Is `parse_href` working? Yes\n",
    "## Q2: Is video_url working? Yes\n",
    "# Conclusion: Updated (fixed) regex in get_id_from_YT_url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test meetup...\n",
      "Test youtube...\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "UTL.test_split_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Audit: which xml files are not lowercase?\n",
    "- Answer by testing 1st paragraph => Modified `xml_caption_to_text` to obtain `Audit.audit_xml_captions`\n",
    "\n",
    "## Audit conclusion:\n",
    "The xml files have consistently been lowercase since event 12, hence this does not warrant implementing\n",
    "of a by-pass to text cleaning if they are not (the corrections would still need applying but they would not\n",
    "be optimal without adding special cases if text is not lowercase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUDIT ALL EVENTS\n",
      "* Replacement of xml files selected.\n",
      "* Captions case check (on 1st P with minutes_mark= 1):\n",
      "03, 2020:: Lower= False\n",
      "everyone I am doing a recording of the scraping presentation and the original recording from the webinar didn't come out well so this is just a recording in today's presentation I'm going to talk about web scraping we're going to look at the website Poshmark comm and we're going to use Python and some additional packages to gather the data so agenda I'm gonna give a quick introduction about myself and the group then we're going to talk about web scraping and high level then we'll walk through a code example I'm going to share the code files so you can walk through it on your own as well and then during the webinar there was obviously QA it's a little bit about me I'm a product manager with General Assembly I used to run operations at an online data science bootcamp and that's kind of where I picked up everything I know about Python and programming and data science and \n",
      ")\n",
      "04, 2020:: Lower= False\n",
      "all right it's 7:05 so I'll do my intra bit and we can happen to yours hey everyone my name is ty I will be doing an intro quick and true and then Ali will be giving her presentation if you have questions there's a QA function and zoom so you can ask the quench questions there if I can answer it I'll help answer if not we will run through them at the end okay so I'm just gonna give a quick introduction then I will talk and then we'll do QA this will be recorded so if something happens you have to leave or someone you know wasn't able to attend the recording will automatically be sent out by General Assembly usually I think it's within a day but it might be two days so just an intro on how a she teaches people how to code she loves Python JavaScript and talking about code and she's always been interested in programming art and education she's on the distinguished \n",
      ")\n",
      "05, 2020:: Lower= False\n",
      "hey everybody so this is an instruction video on how to contribute to source projects in particular to cycle learn I'm andreas Miller one of the core developers of recycle or project thanks for Reshma and the data umbrella for organizing this sprint so I really want to give you just a very brief overview of the technology behind contributing to open source and the steps of getting your first cohesions in so first off a great way to communicate with the developers is the getter channel you can find that at get heard I am slashed so I could learn for a spread there is this channel called sprint and there's also a cycle to learn channel that's just a general channel that you can find that Gator that I am slash scikit-learn slash I could learn \n",
      ")\n",
      "06, 2020:: Lower= False\n",
      "hello my name is Rashmi and I'm going to go through an example of submitting a pull request or a PR I participated in my first scikit-learn sprint about a year and a half ago and I'm happy to share an example once you learn this example is gonna be for the SK learn repo but once you've learned how to do it for this repository you can do it for any repository on github so the first thing that I'm going to do is I'm just gonna make sure that I have my I have some things set up and the first thing is I'm working out of my home directory guys I just want to make sure and see which which Python I am using with the system is of anaconda I am using the anaconda version which is good I also want to just confirm what version of Python I'm using and it's version 3 6 8 and I just want to check one more thing which is do I have get installed and I do that's great so the next thing that \n",
      ")\n",
      "07, 2020:: Lower= False\n",
      "okay so it's it's really great my name is Rashmi Shaikh and I am the founder of data umbrella I am also an organizer for NYC hi ladies and I am based out of New York City welcome to our welcome to our session the mission of data umbrella is to provide a welcoming and educational space for underrepresented persons in data science we welcome allies to help us in our mission and we welcome all different skill levels we are on twitter at theta umbrella so feel free to tweet and share about the event and follow us also there's a LinkedIn page for us where I usually share job posting so you're welcome to join there and there's information on the website about a discord for community I just want to go over the code of conduct that you know we're dedicated to providing harassment free experience for everyone be kind to others be professional and respectful we really want to build a friendly \n",
      ")\n",
      "08, 2020:: Lower= False\n",
      "Oh you you you you if you have a you can use the QA function so if you're on zoom and you hover over the video it should be on the bottom below the video it looks like QA and you can ask questions there and then we'll spend some time to answer them we do a break in the middle of the talk and then at the end of the talk as well you \n",
      ")\n",
      "09, 2020:: Lower= False\n",
      "there you go okay everybody welcome to data umbrella and PI ladies online event just sort of let you know how well did it how the event is gonna go I'm gonna do an introduction about the meetup group Sam Bell is going to give a talk and we'll have Q&A will also sort of watch the Q&A every 10 minutes or so and answer questions along the way just to let you know this event is being recorded about me I'm a statistician data scientist I founded data umbrella and I'm also a hi ladies organizer a New York City chapter and I am on twitter at reached is the mission of data umbrella is to provide a welcoming and educational space for underrepresented persons in the fields of data science our website has information we're on twitter if you'd like to share about this event and just know that we are a \n",
      ")\n",
      "10, 2020:: Lower= True\n",
      "hi everyone welcome to data umbrella webinar um also co-promoted with nyc pi ladies i'm going to do a brief introduction and then turn it over to emily for her presentation um and then after that what you could do is you can place any questions in the q a and um at the end uh emily will answer questions from them but just and just a reminder to reiterate this talk is being recorded about me um i'm a statistician and data scientist um i'm the founder of data brella and i'm also an nyc pie ladies organizer the mission of data umbrella is to provide a welcoming and educational space for underrepresented persons in the field of data science and machine learning we welcome allies who support our cause our home page is dataumbrella.org check it out we're also on twitter and we are a volunteer run organization \n",
      ")\n",
      "11, 2020:: Lower= False\n",
      "everybody welcome to data umbrellas webinar I'm just gonna go over a brief introduction so I'm gonna introduce the umbrella of Rebecca Kelly's going to do her talk and then you can ask questions in the Q&A tab or you can ask in the chat and I'll just move the questions over to the Q&A tab and depending on how the questions come in we can sort of I made into a home turf Rebecca if it's a good time to interrupt her but we'll get the questions answered and this webinar is being recorded about me I am the founder of data umbrella I'm a statistician by training and a data scientist and I also organize for the New York City chapter of Pi ladies and I'm on Twitter evasion is the mission of data umbrella is to provide a welcoming education inclusive space for underrepresented persons in data science and we are a volunteer run organization \n",
      ")\n",
      "12, 2020:: Lower= True\n",
      "hey everybody welcome to a data umbrella webinar um i am joining um from new york and uh i'm just gonna do a a brief introduction about data umbrella and then we'll get started on the um webinar um so it'll be it'll sort of go like i'll do the introduction um we'll do the talk and then you can post any questions in the chat or in the q a and we'll sort of answer those as they pile up over time and just to let people know this will be recorded and will be available on our youtube i posted a link to youtube in the chat if you're not able to see it just let me know and i can share it again about me i'm the founder of data umbrella i'm a statistician data scientist and i am also an organizer for the new york city chapter of \n",
      ")\n",
      "13, 2020:: Lower= True\n",
      "hey everybody welcome to data umbrella's webinar um thanks for joining us um i'm going to go over um a brief introduction about data umbrella and then liz is going to do her talk and then um for the q a for the questions you can feel free to post them in chat or in the q a tab and i'll sort of moderate the session and ask the questions as i can you know find a good place to interrupt liz as she's presenting now this webinar will be is being recorded actually okay a little bit about me i am a statistician and data scientist um i am based in new york city i am the founder of data umbrella and i'm also an organizer for the new york city chapter of pi ladies you can find me on twitter with at reishmaest and i'm also on github and linkedin with um with the same username data umbrella our mission is to provide a welcoming and educational space for \n",
      ")\n",
      "14, 2020:: Lower= True\n",
      "hi everyone welcome to data umbrella's webinar uh my name is reshma and i'm just going to go over some some housekeeping um the way that the webinar will work tonight is that i do a brief introduction megan is going to be doing um her talk and then we'll um what you can do is there's a tab on the webinar platform for q a so feel free to post any questions there and when you know when it's a good breaking point megan will answer any questions that you have this webinar is being recorded about me uh i am a statistician slash data scientist i am the founder of data umbrella and i'm also an organizer for the new york city chapter of pi ladies and you can find me on twitter at reshma s i'm also on linkedin and github with the same username \n",
      ")\n",
      "15, 2020:: Lower= True\n",
      "hi everybody welcome to data umbrella um i'm going to just go over the agenda of how the webinar is going to go i'm going to do a brief introduction emmanuelle um will do her presentation on plotly and you can ask questions on the q a tab and so we'll sort of check questions when it's a good time to stop um but not to worry your questions will get answered some of them might be at the end but we will answer all questions and this webinar is being recorded a little bit about myself i'm a statistician data scientist i'm the founder of data umbrella and i'm also an organizer for the new york city chapter of pi ladies you can find me on twitter github and linkedin as reishmas data umbrella is our mission is to provide inclusive and welcoming space for underrepresented persons in data science \n",
      ")\n",
      "16, 2020:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "17, 2020:: Lower= True\n",
      "hello everyone thank you for joining our webinar for today uh thanks for joining data umbrella i'm gonna do a quick introduction uh carol willing is going to do her talk and we'll have a q a session at the end and and this webinar is being recorded a little bit about me i'm a statistician data scientist i'm the founder of data umbrella and i am on twitter linkedin github has raised my s feel free to follow me we have a code of conduct we're dedicated to providing harassment free professional respectful experience for everyone this applies to the chat as well um thank you for helping make this a welcoming and friendly community for all of us about data umbrella we're an inclusive community for underrepresented persons in data science we welcome allies to join us and we are a volunteer run organization so how can you support data umbrella the \n",
      ")\n",
      "18, 2020:: Lower= True\n",
      "hello everyone and welcome to data umbrella's webinar i'm going to do a brief presentation um i'm going to do a quick introduction um the talk is going to be there and we are going to have a q a at the end this talk is being recorded if you have any questions there's a q a tab on this platform so if you could post questions there that's a good place to aggregate them if you happen to place them in the chat i can also easily you know transfer them over to q a but it is easier if you post them in the q a tab this webinar is being recorded a little bit about me briefly i'm a statistician data scientist i'm a founder i am the founder of data umbrella and i am on twitter linkedin and github as reishima so feel free to follow me um if you would like we have a code of conduct uh we're quite strict with our code of conduct because one of the reasons that this community was created is to provide a safe and inclusive environment for people from underrepresented groups \n",
      ")\n",
      "19, 2020:: Lower= True\n",
      "hello everybody welcome to data umbrella's webinar so um our our processes usually i do a quick introduction um maddie will do his talk and you know her q a um if you have questions you can on chat or you can post them in the q a tab i can easily move questions from the chat over to the q a so that's fine um and this webinar is being recorded and will be available on youtube um usually within a couple of days but sometimes a week depending on how much editing has to be done a quick introduction about myself i i'm the founder of data umbrella i'm a statistician data scientist by training and i am available on twitter linkedin and github at rachmas so feel free to follow me if you'd like we have a code of conduct here uh we're dedicated to providing harassment free experience for everyone this applies to \n",
      ")\n",
      "20, 2020:: Lower= True\n",
      "hello everyone and welcome to data umbrella's webinar our our agenda is going to be i'm going to do a quick introduction and then marco will be doing his talk and we have q a at the end this is actually a special webinar because what we will do after marco's presentation is we're going to go over to discord and if anybody wants to set up their environment we have somebody to help people who are viewing this reporting after today is tuesday december 15th a little bit about data umbrella we are an inclusive community for underrepresented persons in data science and we are a volunteer run organization a little bit about me i'm a statistician data scientist i have a master's in statistics and um let me just get my slides back i'm going to uh okay \n",
      ")\n",
      "21, 2020:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "22, 2020:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "23, 2020:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "24, 2020:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "01, 2021:: Lower= True\n",
      "hello everyone and welcome to our data umbrella webinar this is our first webinar of 2021. i'm going to do a quick introduction and then nick will do his talk and we'll open it up for q a you can also ask questions in the question tab throughout the presentation and we'll sort of break whenever it's a good time to break and this webinar is being recorded data umbrella is an inclusive community for underrepresented persons in data science and we are volunteer run a brief couple of things about me i'm a statistician data scientist and i have an ms in masters in statistics and an mba from nyu and i am the founder of data umbrella um if you are interested in learning more about what i'm doing you can follow me on twitter linkedin or github i have the same handle raishma s we have a code of conduct here and we're \n",
      ")\n",
      "02, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "03, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "04, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "05, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "06, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "07, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "08, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "get_all_transcripts [meta_only=False, audit_captions=True, replace_xml=True, replace_trx=False]:: done!\n"
     ]
    }
   ],
   "source": [
    "AUD.audit_all_events(audit_captions=True,\n",
    "                     replace_xml=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# DONE: Test: Change GrispecLayout (\"a regulary-spaced grid\": missed that!) to GridBox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# DONE: Incorporate modification to propercasing files in Edit page + reprocess\n",
    "**Note**: This _might_ disappear once punctuation is restored with an NLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# DONE: Added NullHandler: if not DEBUG_MODE, no Output widget created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f526e41be1477f908141d1cb7fc084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black', height='160px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55a1b87660e4a329d434d5452f70a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AppLayout(children=(Accordion(children=(VBox(children=(ToggleButtons(button_style='info', options=('Enter Info…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AC = CTR.AppControls()  # class, GUI controls instantiation\n",
    "gui = AC.app            # AppLayout method\n",
    "gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2126ff8c5eda45168e9a20a5d245ae8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'name': 'stdout', 'text': 'Update dict: OK!\\nUpdate readme: OK!\\nSave: Done!\\n', 'output_type…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gui.center.children[1].children[0].clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': '2021',\n",
       " 'presenter': 'Cat Chenal, Reshama Shaikh',\n",
       " 'title': 'Automating Audio Tanscription.',\n",
       " 'title_kw': 'audio demo',\n",
       " 'video_url': 'https://youtu.be/MHAjCcBfT_A',\n",
       " 'video_href': 'http://www.youtube.com/watch?feature=player_embedded&v=MHAjCcBfT_A',\n",
       " 'video_href_src': 'http://img.youtube.com/vi/MHAjCcBfT_A/0.jpg',\n",
       " 'video_href_alt': 'Automating Audio Tanscription.',\n",
       " 'event_url': 'N.A.',\n",
       " 'slides_url': 'N.A.',\n",
       " 'repo_url': 'N.A.',\n",
       " 'notebook_url': 'N.A.',\n",
       " 'transcriber': 'Mama Chenal',\n",
       " 'status': 'Not yet processed (editor needed)',\n",
       " 'notes': 'N.A.',\n",
       " 'extra_references': '## Other References\\n- Paper:  <Paper url or citation> \\n'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_form = AC.PC.page.children[1].children[0]\n",
    "        \n",
    "d = CTR.get_accordion_entries(input_form)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('presenter', 'Cat Chenal, Reshama Shaikh'),\n",
       "             ('title', 'Automating Audio Tanscription.'),\n",
       "             ('event_url', 'N.A.'),\n",
       "             ('yt_video_id', 'MHAjCcBfT_A'),\n",
       "             ('slides_url', 'https://www.example.com'),\n",
       "             ('repo_url', 'https://www.example.com'),\n",
       "             ('notebook_url', 'N.A.'),\n",
       "             ('transcriber', 'Bibi Chenal'),\n",
       "             ('extra_references',\n",
       "              '## Other References\\n- Binder:  url \\n- Paper:  Paper url or citation \\n- Wiki:  This is an excellent [wiki](http://en.wikipedia.org/wiki/Main_Page) \\n'),\n",
       "             ('video_href',\n",
       "              'http://www.youtube.com/watch?feature=player_embedded&v=MHAjCcBfT_A'),\n",
       "             ('video_href_src', 'http://img.youtube.com/vi/MHAjCcBfT_A/0.jpg'),\n",
       "             ('video_href_alt', 'Automating Audio Tanscription.'),\n",
       "             ('video_href_w', '25%'),\n",
       "             ('formatted_transcript',\n",
       "              \"<!-- Editing Guide: The pipe (|) position in this comment is 120:                                                       | -->\\n### Introduction\\n\\nOkay hello and welcome to Data Umbrella's webinar for October so I'm just going to go over the agenda I'm going to do a  \\nbrief introduction then there will be the workshop by Hugo and James and you can ask questions along the way in the chat  \\nor actually the best place to ask questions is the Q&A and there's an option to upvote as well so yet asking the Q&A if  \\nyou happen to post it on the chat by mistake I can also transfer it over to Q&A so that would be fine too and this  \\nwebinar is being recorded briefly about me I am a statistician and data scientist and I am the founder of Data Umbrella  \\nI am on a lot of platforms as Reshama so feel free to follow me on Twitter and LinkedIn we have a code of conduct we're  \\ndedicated to providing harassment free experience for everyone thank you for helping to make this a welcoming friendly  \\nprofessional community for all and this code of conduct applies to the chat as well so our mission is to provide an  \\ninclusive community for underrepresented persons in data science and we are an all volunteer run organization you can  \\nsupport Data Umbrella by doing the following things you can follow our code of conduct and keep our community a place  \\nwhere everybody wants to keep coming to you can donate to our open collective and that helps to pay meet-up dues and  \\nother operational costs and you can check out this link here on GitHub we have this new initiative where all the videos  \\nare being transcribed and so is to make them more accessible so we take the YouTube videos and we put the raw there and  \\nso we've had a number of volunteers help us transcribe it so feel free to check out this link and maybe if you do this  \\nvideo maybe the two speakers will follow you on Twitter I can't promise anything but it's possible Data Umbrella has a  \\njob board and it's at jobs.org and once this gets started I'll put some links in the chat the job that we are  \\nhighlighting today is is the machine learning engineer job by development seed and development seat is based in  \\nWashington DC and Lisbon Portugal and they do I'm going to go to the next slide what they do is they're doing social  \\ngood work and so they're doing for instance mapping elections from Afghanistan to the US analyzing public health and  \\neconomic data from Palestine to Illinois and leading the strategy and development behind data world bank and some other  \\norganizations and I will share a link to their job posting in the chat as well as soon as I finish this brief  \\nintroduction check out our website for resources there's a lot of resources on learning Python and r also for  \\ncontributing to open source also for guides on accessibility and responsibility and allyship we have a monthly  \\nnewsletter that goes out towards the end of the month and it has information on our upcoming events we have two great  \\nevents coming up in November and December on open source so subscribe to our newsletter to be in the know we are on all  \\nsocial media platforms as Data Umbrella Meetup is the best place to join to find out about upcoming events our website  \\nhas resources follow us on Twitter we also share a lot of information on LinkedIn and if you want to subscribe to our  \\nYouTube channel we record all of our talks and post them there within about a week of the talk so it's a good way to get  \\ninformation OK and now we are ready to get started  \\n \\n\\n#### 00:04:03,120::\\t\\t4 minutes mark -> new paragraph \\n \\nso I will hand it over to put myself on mute and I will hand it over to Hugo and James and let you take over but thank  \\nyou all for joining I just want to thank Reshama Christina and and everyone else who tied all the tireless effort that  \\nthat goes into putting these meet-ups and these online sessions together I I think one thing I want to say is actually  \\nthe the last in-person workshop I gave either at the end of February or early March was data umbrellas in inaugural  \\ntutorial and Meetup if I if I recall correctly on on bayesian bayesian thinking and hacker statistics and simulation and  \\nthat type of stuff so it's it's just wonderful to be back particularly with my colleague and friend friend James we're  \\nbuilding really cool distributed data science products at coiled we'll say a bit about that but we'll do some  \\nintroductions in in a bit I just wanted to get you all accustomed to it was February thank you Reshama we're working  \\nwith Jupyter notebooks in a GitHub repository the repository is pinned to the top of the chat this is what it looks like  \\nthese are all the files this is the file system now we use something called Binder which is a project out of and related  \\nto project project Jupyter which provides infrastructure to run notebooks without any local installs so there are two  \\nways you can you can code along on this tutorial the first is and I won't get you to do this yet is to launch Binder the  \\nreason I won't get you to do that yet is because once you launch it we have 10 minutes to start coding or the Binder  \\nsession times out I've been burnt by that before actually several times I'm surprised I even remembered it this time the  \\nother thing you can do is install everything locally by cloning the repository downloading anaconda creating a conda  \\nenvironment if you haven't done that I suggest you do not do that now and you launch launch the Binder James is going to  \\nstart by telling us a few a few things about about GAs and distributed compute in general my question for you James is  \\nif we get people to launch this now will we get to execute a cell code cell in 10 minutes I would let's hold off for now  \\nmaybe yep maybe I'll indicate when we should launch Binder OK fantastic cool and just what I'm looking at right now is  \\nthe GitHub repository on your browser OK exactly so I will not launch Binder now I will not get you to now I've I'm  \\ndoing this locally and we see that I'm in notebook zero and if you want to actually have a look at this notebook before  \\nlaunching Binder it's in the notebooks Data Umbrella subdirectory and its notebook zero and we're going to hopefully  \\nmake it through the overview then chatting about Dask Dask delayed and and data framing and machine learning great so we  \\nhave Hashim has said you could open in VSCode as well you could I mean that would require all your local installs and  \\nthat that type of stuff as well but we're to introduce me and James we we work at coiled where we build products for  \\ndistributed compute in infrastructure as we'll see one of the big problems with like bursting to the cloud is all the  \\nlike Kubernetes AWS docker stuff so we build a one-click host of deployments for das but for data science and machine  \\nlearning in general James maintains task along with Matt Matt Rocklin who created Dask with a team people who was  \\nworking with Continuum Anaconda at the time and James is a software engineer at called and I run data science evangelism  \\nmarketing work on a bunch of product product stuff as well wear a bunch of different different hats  \\n \\n\\n#### 00:08:01,680::\\t\\t4 minutes mark -> new paragraph \\n \\noccasionally there are many ways to think about distributed compute and how to do it in in Python we're going to present  \\nhey James you're muted I'm taking it I went away based on what I see in the chat you did you did but now we're back I've  \\nintroduced you I've introduced me I've mentioned that there are many ways to do distributed compute in the Python  \\necosystem and we'll be chatting about one called Dask and maybe I'll pass you in a second but I'll say one thing that I  \\nreally like about my background isn't in distributed compute my background's in pythonic data science when thinking  \\nabout bursting to larger data sets and larger models there are a variety of options the thing that took me attracted me  \\nto desk originally I saw Cameron's note the ghost in the machine aren't playing nice tonight I think that ain't that the  \\ntruth is that dark plays so nicely with the entire PyData ecosystem so as we'll see if you want to write dash code for  \\ndata frames dash data frames it really mimics your Pandas code same with numpy same with scikit-learn OK and the other  \\nthing is dark essentially runs the Python code under the hood so your mental model of what's happening is actually  \\ncorresponds to the code being being executed OK now I'd like to pass over to James but it looks like he's disappeared  \\nagain I'm still here if you can hear me I've just turned my camera off oh yeah OK great I'm gonna turn my camera  \\nhopefully that will help yeah and I might do do the same for bandwidth bandwidth issues so if if you want to jump in and  \\nand talk about dark at a high level I'm sharing my screen and we can scroll through yeah that sounds great so that's  \\nsort of a nutshell you can think of it as being composed of two main well components the first we call collections these  \\nare the user interfaces that you use to actually construct a computation you would like to compute in parallel or on  \\ndistributed hardware there are a few different interfaces that desk implements for instance there's Dask array for doing  \\nnd array computations there's das data frame for working with tabular data you can think of those as like GAsk array as  \\na parallel version of numpy das data frame has a parallel version of Pandas and so on there are also a couple other  \\ninterfaces that we'll be talking about das delayed for instance we'll talk about that today we'll also talk about the  \\nfutures API those are sort of for lower level custom algorithms in sort of paralyzing existing existing code the main  \\ntakeaway is that there are several sort of familiar APIs that desk implements and that will use today to actually  \\nconstruct your computation so that's the first part of desk it is these dash collections you then take these collections  \\nset up your steps for your computation and then pass them off to the second component which are desk schedulers and  \\nthese will actually go through and execute your computation potentially in parallel there are two flavors of schedulers  \\nthat desk offers the first is a are called single machine schedulers and these just take advantage of your local  \\nhardware they will spin up a a local thread or process pool and start submitting tasks in your computation to to be  \\nexecuted in parallel either on multiple threads or multiple processes there's also a distributed scheduler or maybe a  \\nbetter term for would actually be called the advanced scheduler because it works well on a single machine but it also  \\nscales out to multiple machines so for instance as you'll see later we will actually spin up a distributed scheduler  \\nthat has workers on remote  \\n \\n\\n#### 00:12:00,160::\\t\\t4 minutes mark -> new paragraph \\n \\nmachines on AWS so you can actually scale out beyond your local resources like say what's on your laptop kind of  \\nscrolling down then to the image of the cluster we can see the main components of the distributed scheduler and James I  \\nmight get people to spin up the Binder now because we're going to execute codes now is a good point yep so just here's a  \\nquick break point before you know a teaser for schedulers and what's happening there I'll ask you to in the repository  \\nthere's also the link to the Binder click on launch Binder I'm going to open it in a new tab and what this will create  \\nis an environment in which you can just execute the code in in the notebooks OK so hopefully by the time we've gotten  \\ngone through this section this will be ready to start executing code so if everyone wants to do that to code along  \\notherwise just watch or if you're running things locally also cool thanks James yeah yeah no problem thank you so so  \\nyeah looking at the image for the distributed scheduler we're not gonna have time to go into the a lot of detail about  \\nthe distributed scheduler in this workshop so but we do want to provide at least a high level overview of the the  \\ndifferent parts and components of the distributed scheduler so the first part I want to talk about is in the diagram  \\nwhat's labeled as a client so this is the user facing entry point to a cluster so wherever you are running your Python  \\nsession that could be in a Jupyter lab session like we are here that could be in a Python script somewhere you will  \\ncreate and instantiate a client object that connects to the second component which is the das scheduler so each desk  \\ncluster has a single scheduler in it that sort of keeps track of all of the state for all of the the state of your  \\ncluster and all the tasks you'd like to compute so from your client you might start submitting tasks to the cluster the  \\nschedule will receive those tasks and compute things like all the dependencies needed for that task like say you're  \\nimplementing you say you want to compute task c but that actually requires first you have to compute task b and task a  \\nlike there are some dependency structures there it'll compute those dependencies as well as keep track of them it'll  \\nalso communicate with all the workers to understand what worker is working on which task and as space frees up on the  \\nworkers it will start farming out new tasks to compute to the workers so in this particular diagram there are three das  \\ndistributed workers here however you can have as you can have thousands of workers if you'd like so the workers are the  \\nthings that actually compute the tasks they also store the results of your tasks and then serve them back to you and the  \\nclient the scheduler basically manages all the state needed to perform the computations and you submit tasks from the  \\nclient so that's sort of a quick whirlwind tour of the different components for the distributed scheduler and at this  \\npoint I think it'd be great to actually see see some of this in action Hugo would like to take over absolutely thank you  \\nfor that wonderful introduction to darsk and and the schedulers in particular and we are going to see that with dark in  \\naction I'll just note that this tab in which I launched the Binder is up and running if you're going to execute code  \\nhere click on notebooks click on Data Umbrella oop and then go to the overview notebook and you can drag around we'll  \\nsee the utility of these these dashboards in a second but you can you know drag your stuff around to to make you know  \\nhowever you want to want to structure it and then you can execute code in here I'm not going to do that I'm going to do  \\nthis locally at the moment but just to see dust in action to begin with I'm going to I'm actually going to  \\n \\n\\n#### 00:16:02,720::\\t\\t4 minutes mark -> new paragraph \\n \\nrestart kernel and clear my outputs so I'm going to import from dash distributed the client the sorry the other thing I  \\nwanted to mention is we made a decision around content for this we do have a notebook that we we love to teach on  \\nschedulers but we decided to switch it out for machine learning for this workshop in particular we are teaching a  \\nsimilar although distinct workshop at PyData global so we may see some of you there in which we'll be going more in  \\ndepth into schedulers as well so if you want to check that out definitely do so we instantiate the client which as James  \\nmentioned is kind of what we work with as the user to submit our code so that will take take a few seconds OK it's got a  \\nport in you so it's going going elsewhere what I'll just first get you to notice is that it tells us where our dashboard  \\nis and we'll see those tools in a second tells us about our cluster that we have four workers eight cores between eight  \\nand nine gigs of of ram OK now this is something I really love about Dask all the diagnostic tools if I click on the  \\nlittle desk thing here and we've modified the Binder so that that exists there as well we can see I'll hit search and it  \\nshould that now corresponds to the the scheduler now I want to look at the task stream which will tell us in real time  \\nwhat's happening I also want to look at the cluster map so we see here this is already really cool we've got all of our  \\nworkers around here and our scheduler scheduler there and when we start doing some compute we'll actually see  \\ninformation flowing between these and the other thing maybe I'll yeah I'll include a little progress and that can be an  \\nalternate tab to ask I'm wondering perhaps I also want to include something about the workers yeah OK great so we've got  \\na bunch of stuff that's that's pretty interesting there and so the next thing I'm going to do we've got a little utility  \\nfile which downloads some of the data and this is what it does is if you're in Binder it downloads a subset of the data  \\nif you're anywhere else it loads a larger set for this particular example we're dealing with a small data set you see  \\nthe utility of dark and distributed compute when it generalizes to larger data sets but for pedagogical purposes we're  \\ngoing to sit with a smaller data set so that we can actually run run the code there's a trade-off there so actually that  \\nwas already downloaded it seems but you should all see it download I'm actually going to run that in the Binder just to  \\nyou should start seeing downloading nyc flights data set done extracting creating json data etc OK now what we're going  \\nto do is we're going to read in this data as a Dask data frame and what I want you to notice is that it really the das  \\ncode mimics Pandas code so instead of pd read csv we've got dd read csv we've got you know this is the file path the  \\nfirst argument we're doing some parse date setting some data types OK we've got a little wild card regular expression  \\nthere to to join to do a bunch of them and then we're performing a group by OK so we're grouping by the origin of these  \\nflight flight data we're looking at the the mean departure delay group by origin the the one difference I want to make  \\nclear is that in das we need a compute method that's because das performs lazy computation it won't actually do anything  \\nbecause you don't want it to do anything on really large data sets until you explicitly tell it tell it to compute so  \\nI'm going to execute this now and we should see some information  \\n \\n\\n#### 00:20:01,520::\\t\\t4 minutes mark -> new paragraph \\n \\ntransfer between the scheduler and the workers and we should see tasks starting starting to be done OK so moment of  \\ntruth fantastic so we call this a pew pew plot because we see pew pew pew we saw a bunch of data transfer happening  \\nbetween them these are all our cause and we can see tasks happening it tells us what tasks there are we can see that  \\nmost of the time was spent reading reading csvs then we have some group bias on chunks and and that type of stuff so  \\nthat's a really nice diagnostic tool to see what most of your work is is actually doing under dark work as you can see  \\nmemory used CPU use more fine-grained examples there so I I'd love to know if in the Q&A I'm going to ask were you able  \\nto execute this code and if you were in Binder just a thumb up a vote would be no would be fantastic much appreciated so  \\nas we've mentioned I just wanted to say a few things about tutorial goals the goal is to cover the basics of dark and  \\ndistributed compute we'd love for you to walk away with an understanding of when to use it when to not what it has to  \\noffer we're going to be covering the basics of Dask delayed which although not immediately applicable to data science  \\nprovides a wonderful framework for thinking about Dask how dark works and understanding how it works under the hood then  \\nwe're going to go into dark data frames and then machine learning hopefully due to the technical considerations with  \\nwe've got less time than than we thought we would but we'll definitely do the best we can we may have less time to do  \\nexercises so we've had two people who are able to execute this code if you if you tried to execute it in Binder and were  \\nnot able to perhaps post that in the Q&A but we also have several exercises and I'd like you to take a minute just to do  \\nthis exercise the I I'm not asking you to do this because I want to know if you're able to print hello world I'm  \\nessentially asking you to do it so you get a sense of how these exercises work so if you can take 30 seconds to print  \\nhello world then we'll we'll move on after that so just take 30 seconds now and it seems like we have a few more people  \\nwho are able to execute code which which was great OK fantastic so you will put your solution there for some reason I  \\nhave an extra cell here so I'm just going to clip that and to see a solution I'll just get you to execute this cell and  \\nit provides the solution and then we can execute it and compare it to the the output of what you had OK hello world so  \\nas as we saw I've done all this locally you may have done it on Binder there is an option to work directly from the  \\ncloud and I'll I'll take you through this there are many ways to do this as I mentioned we're working on one way with  \\ncoil and I'll explain the rationale behind that in in a second but I'll show you how easy it is to get a cluster up and  \\nrunning on on AWS without even interacting with AWS for free for example you can follow along by signing into coiled  \\ncloud to be clear this is not a necessity and it does involve you signing up to our product so I just wanted to be  \\nabsolutely transparent about that it does not involve any credit card information or anything  \\n \\n\\n#### 00:24:01,520::\\t\\t4 minutes mark -> new paragraph \\n \\nalong those lines and in my opinion it does give a really nice example of how to run stuff on the cloud to do so you can  \\nsign in at cloud dot coiled dot io you can also pip install coiled and then do authentication you can also spin up this  \\nthis hosted coiled notebook so I'm going to spin that up now and I'm going to post that here actually yep I'm gonna post  \\nthat in the ch chat if you let me get this right if you've if you've never logged in to code before it'll ask you to  \\nsign up using gmail or GitHub so feel free to do that if you'd like if not that's also also cool but I just wanted to be  \\nexplicit about that the reason I want to do this is to show how dars can be leveraged to do work on really large data  \\nsets so you will recall that I had between eight and nine gigs of ram on my local system oh wow Anthony says on ipad  \\nunable to execute on Binder incredible I don't have a strong sense of how Binder works on ipad I do know that I was able  \\nto to check to use a Binder on my iphone several years ago on my way to scipy doing code review for someone for Eric  \\nMaher I think for what that that's worth but back to this we have this nyc taxi data set which is over 10 gigs it won't  \\neven I can't even store that in local memory I don't have enough ram to store that so we do need either to do it locally  \\nin an out of core mode of some sort or we can we can burst to the cloud and we're actually going to burst to the cloud  \\nusing using coiled so the notebook is running here for me and but I'm actually gonna do it from my local local notebook  \\nbut you'll see and once again feel free to code along here it's spinning up a notebook and James who is is my co-  \\ninstructor here is to be I'm I'm so grateful all the work is done on our notebooks in coiled you can launch the cluster  \\nhere and then analyze the entire over 10 gigs of data there I'm going to do it here so to do that I import coiled and  \\nthen I import the dash distributed stuff and then I can create my own software environment cluster configuration I'm not  \\ngoing to do that because the standard coiled cluster configuration software environment works now I'm going to spin up a  \\ncluster and instantiate a client now because we're spinning up a cluster in in the cloud it'll take it'll take a minute  \\na minute or two enough time to make a cup of coffee but it's also enough time for me to just talk a bit about why this  \\nis important and there are a lot of a lot of good good people working on on similar things but part of the motivation  \\nhere is that if you want to you don't always want to do distributed data science OK first I'd ask you to look at instead  \\nof using dark if you can optimize your Pandas code right second I'd ask if you've got big data sets it's a good question  \\ndo you actually need all the data so I would if you're doing machine learning plot your learning curve see how accurate  \\nsee how your accuracy or whatever your metric of interest is improves as you increase the amount of data right and if it  \\nplateaus before you get to a large data size then you may as well most of the time use your small data see if sub  \\nsampling can actually give you the results you need so you can get a bigger bigger access to a bigger machine so you  \\ndon't have to burst to the cloud but after all these things if you do need to boast burst to the cloud until recently  \\nyou've had to get an AWS account you've had to you know set up containers with docker and or Kubernetes and do all of  \\nthese kind of  \\n \\n\\n#### 00:28:00,640::\\t\\t4 minutes mark -> new paragraph \\n \\nI suppose devopsy software engineering foo stuff which which if you're into that I I absolutely encourage you encourage  \\nyou to do that but a lot of working data scientists aren't paid to do that and I don't necessarily want to so that's  \\nsomething we're working on is thinking about these kind of one-click hosted deployments so you don't have to do all of  \\nthat having said that I very much encourage you to try doing that stuff if if you're interested we'll see that the the  \\ncluster has just been created and what I'm going to do we see that oh I'm sorry I've done something funny here I'm I'm  \\nreferencing the previous client anna James yeah it looks like you should go ahead and connect a new client to the coil  \\ncluster and making sure not to re-execute the cluster creation exactly so would that be how would I what's the call here  \\nI would just open up a new cell and say client equals capital client and then pass in the cluster like open parentheses  \\ncluster yeah great OK fantastic and what we're seeing is a slight version this we don't need to worry about this this is  \\nessentially saying that the environment on the cloud mis is there's a slight mismatch with my with my local environment  \\nwe're fine with that I'm going to look here for a certain reason the the dashboard isn't quite working here at the  \\nmoment James would you suggest I just click on this and open a new yeah click on the ecs dashboard link oh yes fantastic  \\nso yep there's some bug with the local dashboards that we're we're currently currently working on but what we'll see now  \\njust a SEC I'm going to remove all of this we'll see now that I have access to 10 workers I have access to 40 cores and  \\nI have access to over 170 gigs of memory OK so now I'm actually going to import this data set and it's the entire year  \\nof data from 2019 and we'll start seeing on on the diagnostics all the all the processing happening OK so oh actually  \\nnot yet because we haven't called compute OK so it's done this lazily we've imported it it shows kind of like Pandas  \\nwhen you show a data frame the column names and data types but it doesn't show the data because we haven't loaded it yet  \\nit does tell you how many partitions it is so essentially and we'll see this soon das data frames correspond to  \\ncollections of Pandas data frames so they're really 127 Pandas data frames underlying this task data frame so now I'm  \\ngoing to do the compute well I'm going to set myself up for the computation to do a group by passenger gown and look at  \\nthe main tip now that took a very small amount of time we see the IPython magic timing there because we haven't computed  \\nit now we're actually going to compute and James if you'll see in the chat Eliana said her coil coiled authentication  \\nfailed I don't know if you're able to to help with that but if you are that would be great and it may be difficult to  \\ndebug in but look as we see we have the task stream now and we see how many you know we've got 40 cores working together  \\nwe saw the processing we saw the bytes stored it's over 10 gigs as I said and we see we were able  \\n \\n\\n#### 00:32:01,519::\\t\\t4 minutes mark -> new paragraph \\n \\nto do our basic analytics we were able to do it on a 10 plus gig data set in in 21.3 seconds which is pretty pretty  \\nexceptional if any any code based issues come up and they're correlated in particular so if you have questions about the  \\ncode execution please ask in the Q&A not in the chat because others cannot vote it and I will definitively prioritize  \\nquestions on technical stuff particularly ones that up that are upvoted but yeah I totally agree thanks thanks very much  \\nso yeah let's jump into into data frames so of course we write here that in the last exercise we used ask delayed to  \\nparallelize loading multiple csv files into a Pandas data frame we're not we we haven't done that but you can definitely  \\ngo through and have a look at that but I think perhaps even more immediately relevant for a data science crowd and an  \\nanalytics crowd is which is what I see here from the reasons people people have joined is jumping into Dask data frames  \\nand as I said before adas data frame really feels like a Pandas data frame but internally it's composed of many  \\ndifferent different data frames this is one one way to think about it that we have all these Pandas data frames and the  \\ncollection of them is a dark data frame and as we saw before they're partitioned we saw when we loaded the taxi data set  \\nin the dash data frame was 127 partitions right where each partition was a normal panda Pandas data frame and they can  \\nlive on disk as they did early in the first example dark in action or they can live on other machines as when I spun up  \\na coiled cluster and and did it on on AWS something I love about darth's data frames I mean I ran about this all the  \\ntime it's how it's the Pandas API and and Matt Matt Rocklin actually has a post on on the blog called a brief history of  \\nDask in which he talks about the technical goals of us but also talks about a social goal of task which in Matt's words  \\nis to invent nothing he wanted and the team wanted the Dask API to be as comfortable and familiar for users as possible  \\nand that's something I really appreciate about it so we see we have element element wires on operations we have the our  \\nfavorite row eyes selections we have loc we have the common aggregations we saw group buyers before we have is-ins we  \\nhave date time string accessors oh James we forgot to I forgot to edit this and I it should be grouped by I don't know  \\nwhat what a fruit buy is but that's something we'll make sure the next iteration to to get right at least we've got it  \\nright there and in the code but have a look at the dash data frame API docs to check out what's happening and a lot of  \\nthe time dash data frames can serve as drop in replacements for Pandas data frames the one thing that I just want to  \\nmake clear as I did before is that you need to call compute because of the lazy laser compute property of das so this is  \\nwonderful to talk about when to use data frames so if your data fits in memory use Pandas if your data fits in memory  \\nand your code doesn't run super quickly I wouldn't go to Dask I'd try to I'd do my best to optimize my Pandas code  \\nbefore trying to get gains gains and efficiency but dark itself becomes useful when the data set you want to analyze is  \\nlarger than your machine's ram where you normally run into memory errors and that's what we saw  \\n \\n\\n#### 00:36:01,520::\\t\\t4 minutes mark -> new paragraph \\n \\nwith the taxicab example the other example that we'll see when we get to [music] machine learning is you can do machine  \\nlearning on a small data set that fits in memory but if you're building big models or training over like a lot of  \\ndifferent hyper parameters or different types of models you can you can parallelize that using using dark so there is  \\nyou know you want to use dash perhaps in the big data or medium to big data limit as we see here or in the medium to big  \\nmodel limit where training for example takes and takes a lot of time OK so without further ado let's get started with  \\ndas data frames you likely ran this preparation file to get the data in the previous notebook but if you didn't execute  \\nthat now we're going to get our file names by doing doing a few joins and we see our file is a string data nyc flights a  \\nwildcard to access all of them dot dot csv and we're going to import our Dask dust.dataframe and read in our dataframe  \\nparsing some dates setting some sending some data types OK I'll execute that we'll see we have 10 partitions as we noted  \\nbefore if this was a Pandas data frame we'd see a bunch of entries here we don't we see only the column names and the  \\ndata types of the columns and the reason is as we've said it explicitly here is the representation of the data frame  \\nobject contains no data it's done Dask has done enough work to read the start of the file so that we know a bit about it  \\nsome of the important stuff and then further column types and column names and data types OK but we don't once again we  \\ndon't let's say we've got 100 gigs of data we don't want to like do this call and suddenly it's reading all that stuff  \\nin and doing a whole bunch of compute until we explicitly tell it to OK now this is really cool if you know a bit of  \\nPandas you'll know that you can there's an attribute columns which prints out it's well it's actually the columns form  \\nan index right the Pandas index object and we get the we get the column names there cool Pandas in dark form we can  \\ncheck out the data types as well as we would in Pandas we see we've got some ins for the day of the week we've got some  \\nfloats for departure time maybe we'd actually prefer that to be you know a date time at some point we've got some  \\nobjects which generally are the most general on objects so generally strings so that's all pandasey type stuff in  \\naddition das data frames have an attribute n partitions which tells us the number of partitions and we saw before that  \\nthat's 10 so I'd expect to see 10 here hey look at that now this is something that we talk about a lot in the delayed  \\nnotebook is really the task graph and I don't want to say too much about that but really it's a visual schematic of of  \\nthe order in which different types of compute happen OK and so the task graph for read csv tells us what happens when we  \\ncall compute and essentially it reads csv 10 ten times zero indexed of course because Python it reads csv ten different  \\ntimes into these ten different Pandas Pandas data frames and if there were group buys or stuff after that we'd see them  \\nhappen in in the in the graph there and we may see an example of this in a second so once again as with Pandas we're  \\ngoing to view the the head of the data frame great and we see a bunch of stuff you know we we see the first first five  \\nrows I'm actually also gonna gonna have a look at the  \\n \\n\\n#### 00:40:02,240::\\t\\t4 minutes mark -> new paragraph \\n \\nthe tail the final five rows that may take longer because it's accessing the the final I I there's a joke and it may not  \\neven be a joke how much data analytics is actually biased by people looking at the first five rows before actually you  \\nknow interrogating the data more more seriously so how would all of our results look different if if our files were  \\nordered in in a different way that's another conversation for a more philosophical conversation for another time so now  \\nI want to show you some computations with dark data frames OK so since dash data frames implement a Pandas like API we  \\ncan just write our familiar Pandas codes so I want to look at the column departure delay and look at the maximum of that  \\ncolumn I'm going to call that max delay so you can see we're selecting the column and then applying the max method as we  \\nwould with Pandas oh what happened there gives us some da scala series and what's happened is we haven't called compute  \\nright so it hasn't actually done the compute yet we're going to do compute but first we're going to visualize the task  \\ngraph like we did here and let's try to reason what the task graph would look like right so the task graph first it's  \\ngoing to read in all of these things and then it'll probably perform this selector on each of these different Pandas  \\ndata frames comprising the dash data frame and then it will compute the max of each of those and then do a max on all  \\nthose maxes I think that's what I would assume is happening here great so that's what we're what we're doing we're  \\nreading this so we read the first perform the first read csv get this das data frame get item I think is that selection  \\nthen we're taking the max we're doing the same for all of them then we take all of these max's and aggregate them and  \\nthen take the max of that OK so that that's essentially what's happening when I call compute which I'm going to do now  \\nmoment of truth OK so that took around eight seconds and it tells us the max and I I'm sorry let's let's just get out  \\nsome of our dashboards up as well huh I think in this notebook we are using the single machine scheduler Hugo so I don't  \\nthink there is a dashboard to be seen exactly yeah thank you for that that that catch James great is even better James  \\nwe have a question around using dark for reinforcement learning can you can you speak to that yeah so it depends on this  \\nI mean yeah short answer yes you can use GAs to train reinforcement learning models so there's a package that Hugo will  \\ntalk about called desk ML that we'll see in the next notebook for distributing machine learning that paralyzes and and  \\ndistributes some existing models using desks so for instance things like random forces forest inside kit learn so so yes  \\nyou can use das to do distributed training for models I'm not actually sure if GAskml implements any reinforcement  \\nlearning models in particular but that is certainly something that that can be done yeah and I'll I'll build on that by  \\nsaying we are about to jump into machine  \\n \\n\\n#### 00:44:00,000::\\t\\t4 minutes mark -> new paragraph \\n \\nlearning I don't think as James said I don't think there's reinforcement learning explicitly that that one can do but  \\nyou of course can use the das scheduler yourself to you know to distribute any reinforcement learning stuff you you have  \\nas well and that's actually another another point to make that maybe James can speak to a bit more is that the dark team  \\nof course built all of these high-level collections and task arrays and dust data frames and were pleasantly surprised  \\nwhen you know maybe even up to half the people using dust came in all like we love all that but we're going to use the  \\nscheduler for our own bespoke use cases right yeah exactly yeah the original intention was to like make basically a num  \\nlike a parallel numpy so that was like the desk array stuff like run run numpy and parallel on your laptop and and yeah  \\nso in order to do that we ended up building a distributed scheduler which sort of does arbitrary task computations so  \\nnot just things like you know parallel numpy but really whatever you'd like to throw at it and it turns out that ended  \\nup being really useful for people and so yeah now people use that sort of on their own just using the distributed  \\nscheduler to do totally custom algorithms in parallel in addition to these like nice collections like you saw Hugo  \\npresents the dash data frame API is you know the same as the panda's API so there is this like familiar space you can  \\nuse things like the high-level collections but you can also run whatever custom like Hugo said bespoke computations you  \\nmight have exactly and it's it's been wonderful to see so many people so many people do that and the first thing as  \\nwe'll see here the first thing to think about is if if you're doing lifestyle compute if there's anything you can you  \\nknow parallelize embarrassingly as they say right so just if you're doing a hyper parameter search you just run some on  \\none worker and some on the other and there there's no interaction effect so you don't need to worry about that as  \\nopposed to if you're trying to do you know train on streaming data where you may require it all to happen on on on the  \\nsame worker OK yeah so even think about trying to compute the standard deviation of a of a a univariate data set right  \\nin in that case you can't just send you can't just compute the standard deviation on two workers and then combine the  \\nresult in some some way you need to do something slightly slightly more nuanced and slightly slightly clever more clever  \\nI mean you still can actually in in that case but you can't just do it as naively as that but so now we're talking about  \\nparallel and distributed machine learning we have 20 minutes left so this is kind of going to be a whirlwind tour but  \\nyou know whirlwinds when safe exciting and informative I just want to make clear the material in this notebook is based  \\non the open source content from darsk's tutorial repository as there's a bunch of stuff we've shown you today the reason  \\nwe've done that is because they did it so well so I just want to give a shout out to all the das contributors OK so what  \\nwe're going to do now is just break down machine learning scaling problems into two categories just review a bit of  \\npsychic learn in passing solve a machine learning problem with single Michelle single Michelle I don't know who she is  \\nbut single Michelle wow single machine and parallelism with psychic learning job lib then solve an l problem with an ML  \\nproblem with multiple machines and parallelism using dark as well and we won't have time to burst for the cloud I don't  \\nthink but you can also play play around with that OK so as I mentioned before when thinking about distributed compute a  \\nlot of people do it when they have large data they don't necessarily think about the large model limit and this  \\nschematic kind of speaks to that if you've got a small model that fits in ram you don't need to think about  \\n \\n\\n#### 00:48:00,480::\\t\\t4 minutes mark -> new paragraph \\n \\ndistributed compute if your data size if your data is larger than your ram so your computer's ram bound then you want to  \\nstart going to a distributed setting or if your model is big and CPU bound such as like large-scale hyper-parameter  \\nsearches or like ensembl blended models of like machine learning algorithms whatever it is and then of course we have  \\nthe you know big data big model limit where distributed computer desk is incredibly handy as I'm sure you could imagine  \\nOK and that's really what I've what I've gone through here a bird's-eye view of the strategies we think about if it's in  \\nmemory in the bottom left quadrant just use scikit-learn or your favorite ML library otherwise known as psychic learn  \\nfor me anyway I was going to make a note about xg boost but I but I won't for large models you can use joblib and your  \\nfavorite circuit learn estimator for large data sets use our dark ML estimators so we're gonna do a whirlwind tour of  \\npsychic learn in in five minutes we're going to load in some data so we'll actually generate it we'll import scikit-  \\nlearn for our ML algorithm create an estimator and then check the accuracy of the model OK so once again I'm actually  \\ngoing to clear all outputs after restarting the kernel OK so this is a utility function of psychic learn to create some  \\ndata sets so I'm going to make a classification data set with four features and 10 000 samples and just have a quick  \\nview of some of it so just a reminder on ML x is the samples matrix the size of x is the number of samples in terms of  \\nrows number of features as columns and then a feature or an attribute is what we're trying to predict essentially OK so  \\nwhy is the predictor variable which we're where which we're or the target variable which we're trying to predict so  \\nlet's have a quick view of why it's zeros and ones in in this case OK so yep that's what I've said here why are the  \\ntargets which are real numbers for regression tasks or integers for classification or any other discrete sets of values  \\nno words about unsupervised learning at the moment we're just going to support we're going to fit a support vector  \\nclassifier for this example so let's just load the appropriate scikit-learn module we don't really need to discuss what  \\nsupport vector classifiers are at the moment now this is one of the very beautiful things about the scikit-learn API in  \\nterms of fitting the the model we instantiate a classifier and we want to fit it to the features with respect to the  \\ntarget OK so the first argument is the features second argument is the target variable so we've done that now I'm not  \\ngoing to worry about inspecting the learn features I just want to see how accurate it was OK and once we see how  \\naccurate it was I'm not gonna do this but then we can make a prediction right using estimator dot predict on a new a new  \\ndata set so this estimator will tell us so this score will tell us the accuracy and essentially that's the proportion or  \\npercentage a fraction of the results that were that the estimator got correct and we're doing this on the training data  \\nset we've just trained the model on this so this is telling us the accuracy on the on the training data set OK so it's  \\n90  \\n \\n\\n#### 00:52:01,760::\\t\\t4 minutes mark -> new paragraph \\n \\naccurate on the training data set if you dive into this a bit more you'll recognize that if we we really want to know  \\nthe accuracy on a holdout set or a test set and it should be probably a bit lower because this is what we use to fit it  \\nOK but all that having been said I expect you know if if this is all resonating with you it means we can really move on  \\nto the distributed stuff in in a second but the other thing that that's important to note is that we've trained it but a  \\nlot of model a lot of estimators and models have hyper parameters that affect the fit but you that we need to specify up  \\nfront instead of being learned during training so you know there's a c parameter here there's a are we using shrinking  \\nor not so we specify those we didn't need to specify them because there are default values but here we specify them OK  \\nand then we're going to look at the score now OK this is amazing we've got 50 accuracy which is the worst score possible  \\njust think about this if if you've got binary classification task and you've got 40 accuracy then you just flip the  \\nlabels and that changes to 60 accuracy so it's amazing that we've actually hit 50 accuracy we're to be congratulated on  \\nthat and what I want to note here is that we have two sets of hyper parameters we've used one's created 90 actual model  \\nwith 90 accuracy another one one with 50 accuracy so we want to find the best hyper parameters essentially and that's  \\nwhy hyper parameter optimization is is so important there are several ways to do hyper parameter optimization one is  \\ncalled grid search cross validation I won't talk about cross validation it's essentially a more robust analogue of train  \\ntest split where you train on a subset of your data and compute the accuracy on a test on a holdout set or a test set  \\ncross validation is a as I said a slightly more robust analog of this it's called grid search because we have a grid of  \\nhyper parameters so we have you know in this case we have a hyper parameter c we have a hyper parameter kernel and we  \\ncan imagine them in a in a grid and we're performing we're checking out the score over all this gr over this entire grid  \\nof hyper parameters OK so to do that I import grid search csv now I'm going to compute the estimator over over these  \\ntrain the estimator over over this grid and as you see this is taking time now OK and what I wanted to make clear and I  \\nthink should be becoming clearer now is that if we have a large hyper parameter sweep we want to do on a small data set  \\ndas can be useful for that OK because we can send some of the parameters to one worker some to another and they can  \\nperform them in parallel so that's embarrassingly parallel because you're you're doing the same work as you would  \\notherwise but sending it to a bunch of different workers we saw that took 30 seconds which is in my realm of comfort as  \\na data scientist I'm happy to wait 30 seconds if I had to wait much longer if this grid was bigger I'd start to get  \\nprobably a bit frustrated but we see that it computed it for c is equal to all combinations of these essentially OK so  \\nthat's really all I wanted to say there and then we can see the best parameters and the best score so the best score was  \\n0.098 and it was c10 and the kernel rbf a radial basis function it doesn't even matter what that is though for the  \\npurposes of this so we've got 10 minutes left we're going to we're going to make it I can feel it I have a good I have a  \\ngood sense  \\n \\n\\n#### 00:56:00,400::\\t\\t4 minutes mark -> new paragraph \\n \\n a good after the I mean this demo is actually going incredibly well given the initial technical hurdles so touchwood  \\nHugo OK so what we've done is we've really segmented ML scaling problems into two categories CPU bound and ram bound and  \\nI I really I can't emphasize that enough because I see so many people like jumping in to use new cool technologies  \\nwithout perhaps taking it being a bit mindful and intentional about it and reasoning about when things are useful and  \\nand when not I suppose the one point there is that sure data science is a technical discipline but there are a lot of  \\nother aspects to it involving this type of reasoning as well so we then carried out a typical sklearn workflow for ML  \\nproblems with small models and small data and we reviewed hyper parameters and hyper parameter optimization so in this  \\nsection we'll see how job lib which is a set of tools to provide lightweight pipelining in Python gives us parallelism  \\non our laptop and then we'll see how dark ML can give us awesome parallelism on on clusters OK so essentially what I'm  \\ndoing here is I'm doing exactly the same as above with a grid search but I'm using the quark the keyword argument n jobs  \\nwhich tells you how many tasks to run in parallel using the cause available on your local workstation and specifying  \\nminus one jobs means the it just runs them the maximum possible OK so I'm going to execute that great so we should be  \\ndone in a second feel free to ask any questions in the chat oh alex has a great question in the Q&A does das have see a  \\nsequel and query optimizer I'm actually so excited that [music] and James maybe you can provide a couple of links to  \\nthis we're really excited to have seen dark dust SQL developments there recently so that's dark hyphen hyphen SQL and  \\nwe're actually we're working on some some content and a blog post and maybe a live live coding session about that in in  \\nthe near future so if anyone if you want updates from from coyle feel free to go to our website and sign up for our  \\nmailing list and we'll let you know about all of this type of stuff but the short answer is yes alex and it's getting  \\nbetter and if James is able to post post a link there that would be that would be fantastic so we've done link in the  \\nchat fantastic [music] and so we've we've seen how we have [music] single machine parallelism here using the using the  \\nend jobs quark and in the final minutes let's see multiple multi-machine parallelism with Dask OK so what I'm going to  \\ndo is I'm going to do my imports and create my client incentive my client and check it out OK so once again I'm working  \\nlocally I hit search and that'll task is pretty smart in terms of like knowing which which client I want to check out do  \\nthe tasks stream because it's my favorite I'll do the cluster map otherwise known as the pew pew map and then I want  \\nsome progress we all we all crave progress don't we and maybe my workers tab OK great so we've got that up and running  \\nnow I'm going to do a slightly larger hyper parameter search OK so remember we had just a couple for c a couple for  \\nkernel we're going to do more we have some for shrinking now I'm actually going to comment that out because I don't know  \\nhow long that's going to take if you're coding them on Binder now this May actually take far far too long for you but  \\nwe'll we'll see so I'll execute this code and we should see just sick no we shouldn't see any work happening yet but  \\nwhat I'm doing here is oh looks like OK my clusters back up great we're doing our grid search but we're going to use  \\nDask as as the back end right and this is a context manager where we're asserting that and and we can just discuss the  \\nthe syntax there but it's not particularly important currently I'm going to execute this now and let's see fantastic  \\nwe'll see all this data transfer happening here we'll see our tasks happening here we can see these big batches of fit  \\nand score fit so fitting fitting the models then finding how well they perform via this k-fold cross validation which is  \\nreally cool and let's just yep we can see what's happening here we can see we currently have 12 processing we've got  \\nseven in memory and we have several more that we need to do our desk workers we can see us oh we can see our CPU usage  \\nwe can see how we can see CPU usage across all the workers which is which is pretty cool seeing that distribution is is  \\nreally nice whenever some form of b swarm plot if you have enough would would be useful there or even some form of  \\ncumulative distribution function or something like that not a histogram people OK you can go to my bayesian tutorial  \\nthat I've taught here before to hear me rave about the the horrors of histograms so we saw that talk a minute which is  \\ngreat and we split it across you know eight cores or whatever it is and now we'll have a look once again we get the same  \\nbest performer which is which is a sanity check and that's pretty cool I think we have a we actually have a few minutes  \\nleft so I am gonna just see if I can oh let me think yeah I will see if I can burst burst to the cloud and and and do  \\nthis that will take a minute a minute or two to create the cluster again but while we're while we're doing that I'm  \\nwondering if we have any any questions or if anyone has any feedback on on this workshop I very much welcome welcome  \\nthat perhaps if there are any final messages you'd you'd like to say James while we're spinning this up you can you can  \\nlet me know yeah sure I just also first off wanted to say thanks everyone for attending and like bearing  \\n \\n\\n#### 01:04:01,119::\\t\\t4 minutes mark -> new paragraph \\n \\nbearing with us with the technical difficulties really appreciate that real quick I'm just yeah so if you have if you  \\nhave questions please post in the Q&A section while the cold cluster's spinning up theodore posted in the last largest  \\nexample of grid search how much performance gain did we get from using das and not just in jobs hmm that's a great  \\nquestion and we actually didn't see let's see so it took 80 seconds ah let me get this they're actually not comparable  \\nbecause I did the grid search over a different set of hyper parameters I did it over a larger set of hyper parameters  \\nright so when I did end jobs I did it there were only it was a two by two grid of hyper parameters whereas when I did it  \\nwith with Dask it was a one two three four five six six by three so let's just reason about that this one was eighteen  \\nsix by three is eighteen which took eighty seconds and this one was two by two so it was four and it took 26 seconds so  \\na minor gain I think with this hyper parameter search if you multiply that by by four you'll well 4.2 4.5 you'll need  \\nthat would have taken maybe two minutes or something something like that so we saw some increase in efficiency not a  \\ngreat deal but James maybe you can say more to this part of the reason for that is that we're doing it on kind of a very  \\nsmall example so we won't necessarily see the gains in efficiency with a data set this size and with a small hyper  \\nparameter suite like this is that right yeah yeah and yeah exactly and I guess also this is more of an kind of an  \\nillustrative point here I guess so you're just using directly using in jobs with something like job lib by default we'll  \\nuse local threads and processes on like whatever machine you happen to be running on so like in this case on Hugo's  \\nlaptop one of the real advantages of using job lib with the das back in will actually dispatch back to to run tasks on a  \\ndash cluster is that your cluster can expand beyond what local resources you have so you can run you know you can  \\nbasically scale out like for instance using the coil cluster to have many many CPUs and a large amount of ram that you  \\nwouldn't have on your locally table to run and there you'll see both large performance gains as well as you'll be able  \\nto expand your the set of possible problems you can solve to larger than ram scenarios so you're out of out of core  \\ntraining exactly and thank you Jack this was absolutely unplanned and we didn't plan that question but that's a  \\nwonderful segue into me now performing exactly the same compute with the same code using the dasc as the parallel back  \\nend on a on a coiled cluster which is an AWS cluster right so we can I'm more currently anyway so I will execute this  \\ncode and it's exactly the same as we did whoa OK great so we see our tasks task stream here you see once again we see  \\nthe majority is being batch fit and and getting the scores out similarly we see the same result being the best I'll just  \\nnotice that for this for this small task doing it on the cloud took 20 seconds doing it locally for me took 80 seconds  \\nso that's a four-fold increase in performance on a very small task so imagine what that does if you can take the same  \\ncode as you've written  \\n \\n\\n#### 01:08:00,240::\\t\\t4 minutes mark -> new paragraph \\n \\nhere and burst to the cloud with with one click or however however you do it I think that that's incredibly powerful and  \\nthat the fact that your code and what's happening in the back end with Dask generalizes immediately to the new setting  \\nof working on a cluster I personally find very exciting and if you work with larger data sets or building larger models  \\nor big hyper parameter sweeps I'm pretty sure it's an exciting option for all of you also so on that note I'd like to  \\nreiterate James what James said and thanking you all so much for joining us for asking great questions and for bearing  \\nwith us through some some technical technical hurdles but it made it even even funner when when we got up and running  \\nonce again I'd love to thank Mark Christina and and the rest of the organizers for doing such a wonderful job and doing  \\nsuch a great service to the data science and machine learning community and ecosystem worldwide so thank you once again  \\nfor having us thank you Hugo and James I have to say like with all the technical difficulties I was actually giggling  \\nbecause it was kind of funny yeah but we're very sorry and we thank you for your patience and sticking through it and I  \\nwill be editing this video to you know make it as efficient as possible and have that available Tim supercard thank you  \\ngreat and I'll just ask you if you are interested in checking out coiled go to our website if you want to check out our  \\nproduct go to cloud.coil.io we started building this company in February we're really excited about building a new  \\nproduct so if you're interested reach out we'd love to chat with you about what we're doing and what we're up to and  \\nit's wonderful to be in the same community as you all so thanks  \\n \\n\"),\n",
       "             ('year', '2021'),\n",
       "             ('idn', '08'),\n",
       "             ('video_url', 'https://youtu.be/MHAjCcBfT_A'),\n",
       "             ('title_kw', 'audio-demo'),\n",
       "             ('transcript_md', '08-cat-reshama-audio-demo.md'),\n",
       "             ('audio_track',\n",
       "              WindowsPath('C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/resources/EventManagement/data/meta/2021_08_MHAjCcBfT_A.mp4')),\n",
       "             ('audio_text',\n",
       "              WindowsPath('C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/resources/EventManagement/data/meta/2021_08_MHAjCcBfT_A.txt')),\n",
       "             ('has_transcript', True),\n",
       "             ('trans_idx', 684),\n",
       "             ('status', 'Partial (new editor requested)'),\n",
       "             ('notes', 'N.A.'),\n",
       "             ('video_embed',\n",
       "              '\\n<iframe width=\"560\" height=\"315\" \\n        src=\"https://www.youtube-nocookie.com/embed/MHAjCcBfT_A?cc_load_policy=1&autoplay=0\" \\n        frameborder=\"0\">\\n</iframe>\\n')])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gui.data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_idx = AC.app.left_sidebar.selected_index\n",
    "AC.app.left_sidebar.children[menu_idx].children[0].index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_string.Textarea"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "60133"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AC.PC.page.children[1].children[1].value\n",
    "txa = AC.PC.page.children[1].children[1]\n",
    "type(txa)\n",
    "len(txa.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = \"\"\"\n",
    "'<!-- Editing Guide: The pipe (|) position in this comment is 120:                                                       | -->\\n### Introduction\\n\\nOkay hello and welcome to Data Umbrella\\'s webinar for October so I\\'m just going to go over the agenda I\\'m going to do a  \\nbrief introduction then there will be the workshop by Hugo and James and you can ask questions along the way in the chat  \\nor actually the best place to ask questions is the Q&A and there\\'s an option to upvote as well so yet asking the Q&A if  \\nyou happen to post it on the chat by mistake I can also transfer it over to Q&A so that would be fine too and this  \\nwebinar is being recorded. Briefly about me. I am a statistician and data scientist and I am the founder of Data Umbrella.  \\nI am on a lot of platforms as Reshama so feel free to follow me on Twitter and LinkedIn. We have a code of conduct we\\'re  \\ndedicated to providing a harassment-free experience for everyone. Thank you for helping to make this a welcoming friendly  \\nprofessional community for all and this code of conduct applies to the chat as well. So our mission is to provide an  \\ninclusive community for underrepresented persons in data science and we are an all volunteer-run organization you can  \\nsupport Data Umbrella by doing the following things: You can follow our code of conduct and keep our community a place  \\nwhere everybody wants to keep coming to; You can donate to our open collective and that helps to pay meet-up dues and  \\nother operational costs and you can check out this link here on GitHub we have this new initiative where all the videos  \\nare being transcribed and so is to make them more accessible. So we take the YouTube videos and we put the raw there and  \\nso we\\'ve had a number of volunteers help us transcribe it so feel free to check out this link and maybe if you do this  \\nvideo maybe the two speakers will follow you on Twitter. I can\\'t promise anything, but it\\'s possible Data Umbrella has a  \\njob board and it\\'s at jobs.org and once this gets started I\\'ll put some links in the chat. The job that we are  \\nhighlighting today is the machine learning engineer job by Development Seed. Development Seed is based in  \\nWashington DC and Lisbon Portugal and they do I\\'m going to go to the next slide what they do is they\\'re doing social  \\ngood work and so they\\'re doing for instance mapping elections from Afghanistan to the US analyzing public health and  \\neconomic data from Palestine to Illinois and leading the strategy and development behind data world bank and some other  \\norganizations. I will share a link to their job posting in the chat as well as soon as I finish this brief  \\nintroduction. Check out our website for resources there\\'s a lot of resources on learning Python and R also for  \\ncontributing to open source also for guides on accessibility and responsibility and allyship. We have a monthly  \\nnewsletter that goes out towards the end of the month and it has information on our upcoming events. We have two great  \\nevents coming up in November and December on open source so subscribe to our newsletter to be in the know. We are on all  \\nsocial media platforms as Data Umbrella Meetup is the best place to join to find out about upcoming events our website  \\nhas resources follow us on Twitter we also share a lot of information on LinkedIn and if you want to subscribe to our  \\nYouTube channel we record all of our talks and post them there within about a week of the talk so it\\'s a good way to get  \\ninformation. OK and now we are ready to get started so I will put myself on mute and I will hand it over to Hugo and James  \\nand let you take over but thank you all for joining!   \\n \\n\\n#### 00:04:03,120::\\t\\t4 minutes Mark -> new paragraph \\n \\nI just want to thank Reshama Christina and everyone else who tied all the tireless effort that  \\nthat goes into putting these meet-ups and these online sessions together I think one thing I want to say is actually  \\nthe last in-person workshop I gave either at the end of February or early March was Data Umbrella\\'s inaugural  \\ntutorial and Meetup if I recall correctly on Bayesian thinking and hacker statistics and simulation and  \\nthat type of stuff so it\\'s just wonderful to be back particularly with my colleague and friend James we\\'re  \\nbuilding really cool distributed data science products at coiled we\\'ll say a bit about that but we\\'ll do some  \\nintroductions in a bit I just wanted to get you all accustomed to it was February thank you Reshama we\\'re working  \\nwith Jupyter notebooks in a GitHub repository the repository is pinned to the top of the chat this is what it looks like  \\nthese are all the files this is the file system now we use something called Binder which is a project out of and related  \\nto project Jupyter which provides infrastructure to run notebooks without any local installs so there are two  \\nways you can code along on this tutorial the first is and I won\\'t get you to do this yet is to launch Binder the  \\nreason I won\\'t get you to do that yet is because once you launch it we have 10 minutes to start coding or the Binder  \\nsession Times out I\\'ve been burnt by that before actually several Times I\\'m surprised I even remembered it this time the  \\nother thing you can do is install everything locally by cloning the repository downloading anaconda creating a conda  \\nenvironment if you haven\\'t done that I suggest you do not do that now and you launch the Binder James is going to  \\nstart by telling us a few things about GAs and distributed computing in general my question for you James is  \\nif we get people to launch this now will we get to execute a cell code cell in 10 minutes I would let\\'s hold off for now  \\nmaybe yep maybe I\\'ll indicate when we should launch Binder OK fantastic cool and just what I\\'m looking at right now is  \\nthe GitHub repository on your browser OK exactly so I will not launch Binder now I will not get you to now I\\'ve I\\'m  \\ndoing this locally and we see that I\\'m in notebook zero and if you want to actually have a look at this notebook before  \\nlaunching Binder, it\\'s in the notebooks Data Umbrella subdirectory and it\\'s notebook zero and we\\'re going to hopefully  \\nmake it through the overview then chatting about Dask, Dask delayed and data framing and machine learning great so we  \\nhave Hashim has said you could open in VSCode as well. You could. I mean that would require all your local installs and  \\nthat that type of stuff as well but we\\'re to introduce me and James we work at coiled where we build products for  \\ndistributed computing in infrastructure as we\\'ll see one of the big problems with like bursting to the cloud is all the  \\nlike Kubernetes AWS docker stuff so we build a one-click host of deployments for das but for data science and machine  \\nlearning in general James maintains task along with Matt Matt Rocklin who created Dask with a team of people and was  \\nworking with Continuum Anaconda at the Time and James is a software engineer at coiled and I run data science evangelism  \\nMarketing work on a bunch of product stuff as well wear a bunch of different hats occasionally.  \\n \\n\\n#### 00:08:01,680::\\t\\t4 minutes Mark -> new paragraph \\n \\nThere are many ways to think about distributed compute and how to do it in in Python we\\'re going to present  \\nhey James you\\'re muted I\\'m taking it I went away based on what I see in the chat you did you did but now we\\'re back I\\'ve  \\nintroduced you I\\'ve introduced me I\\'ve mentioned that there are many ways to do distributed compute in the Python  \\necosystem and we\\'ll be chatting about one called Dask and maybe I\\'ll pass you in a second but I\\'ll say one thing that I  \\nreally like about my background isn\\'t in distributed compute my background\\'s in Pythonic data science when thinking  \\nabout bursting to larger data sets and larger models there are a variety of options the thing that took me attracted me  \\nto desk originally I saw Cameron\\'s note the ghost in the machine aren\\'t playing nice tonight I think that ain\\'t that the  \\ntruth is that dark plays so nicely with the entire PyData ecosystem so as we\\'ll see if you want to write dash code for  \\ndata frames dash data frames it really mimics your Pandas code same with numpy same with scikit-learn OK and the other  \\nthing is dark essentially runs the Python code under the hood so your mental model of what\\'s happening is actually  \\ncorresponds to the code being being executed OK now I\\'d like to pass over to James but it looks like he\\'s disappeared  \\nagain I\\'m still here if you can hear me I\\'ve just turned my camera off oh yeah OK great I\\'m gonna turn my camera  \\nhopefully that will help yeah and I might do do the same for bandwidth bandwidth issues so if if you want to jump in and  \\nand talk about dark at a high level I\\'m sharing my screen and we can scroll through yeah that sounds great so that\\'s  \\nsort of a nutshell you can think of it as being composed of two main well components the first we call collections these  \\nare the user interfaces that you use to actually construct a computation you would like to compute in parallel or on  \\ndistributed hardware there are a few different interfaces that Dask implements. For instance, there\\'s Dask array for doing  \\nnd array computations there\\'s Dask dataframe for working with tabular data you can think of those as like Dask array as  \\na parallel version of numpy. Dask dataframe has a parallel version of Pandas and so on there are also a couple other  \\ninterfaces that we\\'ll be talking about das delayed for instance we\\'ll talk about that today we\\'ll also talk about the  \\nfutures API those are sort of for lower level custom algorithms in sort of paralyzing existing existing code the main  \\ntakeaway is that there are several sort of familiar APIs that desk implements and that will use today to actually  \\nconstruct your computation so that\\'s the first part of desk it is these dash collections you then take these collections  \\nset up your steps for your computation and then pass them off to the second component which are desk schedulers and  \\nthese will actually go through and execute your computation potentially in parallel there are two flavors of schedulers  \\nthat desk offers the first is a are called single machine schedulers and these just take advantage of your local  \\nhardware they will spin up a a local thread or process pool and start submitting tasks in your computation to to be  \\nexecuted in parallel either on multiple threads or multiple processes there\\'s also a distributed scheduler or maybe a  \\nbetter term for would actually be called the advanced scheduler because it works well on a single machine but it also  \\nscales out to multiple machines so for instance as you\\'ll see later we will actually spin up a distributed scheduler  \\nthat has workers on remote  \\n \\n\\n#### 00:12:00,160::\\t\\t4 minutes Mark -> new paragraph \\n \\nmachines on AWS so you can actually scale out beyond your local resources like say what\\'s on your laptop kind of  \\nscrolling down then to the image of the cluster we can see the main components of the distributed scheduler and James I  \\nmight get people to spin up the Binder now because we\\'re going to execute codes now is a good point yep so just here\\'s a  \\nquick break point before you know a teaser for schedulers and what\\'s happening there I\\'ll ask you to in the repository  \\nthere\\'s also the link to the Binder click on launch Binder I\\'m going to open it in a new tab and what this will create  \\nis an environment in which you can just execute the code in in the notebooks OK so hopefully by the Time we\\'ve gotten  \\ngone through this section this will be ready to start executing code so if everyone wants to do that to code along  \\notherwise just watch or if you\\'re running things locally also cool thanks James yeah yeah no problem thank you so so  \\nyeah looking at the image for the distributed scheduler we\\'re not gonna have Time to go into the a lot of detail about  \\nthe distributed scheduler in this workshop so but we do want to provide at least a high level overview of the the  \\ndifferent parts and components of the distributed scheduler so the first part I want to talk about is in the diagram  \\nwhat\\'s labeled as a client so this is the user facing entry point to a cluster so wherever you are running your Python  \\nsession that could be in a Jupyter lab session like we are here that could be in a Python script somewhere you will  \\ncreate and instantiate a client object that connects to the second component which is the das scheduler so each desk  \\ncluster has a single scheduler in it that sort of keeps track of all of the state for all of the the state of your  \\ncluster and all the tasks you\\'d like to compute so from your client you might start submitting tasks to the cluster the  \\nschedule will receive those tasks and compute things like all the dependencies needed for that task like say you\\'re  \\nimplementing you say you want to compute task c but that actually requires first you have to compute task b and task a  \\nlike there are some dependency structures there it\\'ll compute those dependencies as well as keep track of them it\\'ll  \\nalso communicate with all the workers to understand what worker is working on which task and as space frees up on the  \\nworkers it will start farming out new tasks to compute to the workers so in this particular diagram there are three das  \\ndistributed workers here however you can have as you can have thousands of workers if you\\'d like so the workers are the  \\nthings that actually compute the tasks they also store the results of your tasks and then serve them back to you and the  \\nclient the scheduler basically manages all the state needed to perform the computations and you submit tasks from the  \\nclient so that\\'s sort of a quick whirlwind tour of the different components for the distributed scheduler and at this  \\npoint I think it\\'d be great to actually see see some of this in action Hugo would like to take over absolutely thank you  \\nfor that wonderful introduction to Dask and and the schedulers in particular and we are going to see that with dark in  \\naction I\\'ll just note that this tab in which I launched the Binder is up and running if you\\'re going to execute code  \\nhere click on notebooks click on Data Umbrella oop and then go to the overview notebook and you can drag around we\\'ll  \\nsee the utility of these these dashboards in a second but you can you know drag your stuff around to to make you know  \\nhowever you want to want to structure it and then you can execute code in here I\\'m not going to do that I\\'m going to do  \\nthis locally at the moment but just to see dust in action to begin with I\\'m going to I\\'m actually going to  \\n \\n\\n#### 00:16:02,720::\\t\\t4 minutes Mark -> new paragraph \\n \\nrestart kernel and clear my outputs so I\\'m going to import from dash distributed the client the sorry the other thing I  \\nwanted to mention is we made a decision around content for this we do have a notebook that we we love to teach on  \\nschedulers but we decided to switch it out for machine learning for this workshop in particular we are teaching a  \\nsimilar although distinct workshop at PyData global so we may see some of you there in which we\\'ll be going more in  \\ndepth into schedulers as well so if you want to check that out definitely do so we instantiate the client which as James  \\nmentioned is kind of what we work with as the user to submit our code so that will take take a few seconds OK it\\'s got a  \\nport in you so it\\'s going going elsewhere what I\\'ll just first get you to notice is that it tells us where our dashboard  \\nis and we\\'ll see those tools in a second tells us about our cluster that we have four workers eight cores between eight  \\nand nine gigs of of ram OK now this is something I really love about Dask all the diagnostic tools if I click on the  \\nlittle desk thing here and we\\'ve modified the Binder so that that exists there as well we can see I\\'ll hit search and it  \\nshould that now corresponds to the the scheduler now I want to look at the task stream which will tell us in real Time  \\nwhat\\'s happening I also want to look at the cluster map so we see here this is already really cool we\\'ve got all of our  \\nworkers around here and our scheduler scheduler there and when we start doing some compute we\\'ll actually see  \\ninformation flowing between these and the other thing maybe I\\'ll yeah I\\'ll include a little progress and that can be an  \\nalternate tab to ask I\\'m wondering perhaps I also want to include something about the workers yeah OK great so we\\'ve got  \\na bunch of stuff that\\'s that\\'s pretty interesting there and so the next thing I\\'m going to do we\\'ve got a little utility  \\nfile which downloads some of the data and this is what it does is if you\\'re in Binder it downloads a subset of the data  \\nif you\\'re anywhere else it loads a larger set for this particular example we\\'re dealing with a small data set you see  \\nthe utility of dark and distributed compute when it generalizes to larger data sets but for pedagogical purposes we\\'re  \\ngoing to sit with a smaller data set so that we can actually run run the code there\\'s a trade-off there so actually that  \\nwas already downloaded it seems but you should all see it download I\\'m actually going to run that in the Binder just to  \\nyou should start seeing downloading NYC flights data set done extracting creating json data etc OK now what we\\'re going  \\nto do is we\\'re going to read in this data as a Dask data frame and what I want you to notice is that it really the das  \\ncode mimics Pandas code so instead of pd read csv we\\'ve got dd read csv we\\'ve got you know this is the file path the  \\nfirst argument we\\'re doing some parse date setting some data types OK we\\'ve got a little wild card regular expression  \\nthere to to join to do a bunch of them and then we\\'re performing a group by OK so we\\'re grouping by the origin of these  \\nflight flight data we\\'re looking at the the mean departure delay group by origin the the one difference I want to make  \\nclear is that in das we need a compute method that\\'s because das performs lazy computation it won\\'t actually do anything  \\nbecause you don\\'t want it to do anything on really large data sets until you explicitly tell it tell it to compute so  \\nI\\'m going to execute this now and we should see some information  \\n \\n\\n#### 00:20:01,520::\\t\\t4 minutes Mark -> new paragraph \\n \\ntransfer between the scheduler and the workers and we should see tasks starting starting to be done OK so moment of  \\ntruth fantastic so we call this a pew pew plot because we see pew pew pew we saw a bunch of data transfer happening  \\nbetween them these are all our cause and we can see tasks happening it tells us what tasks there are we can see that  \\nmost of the Time was spent reading csvs then we have some group bias on chunks and and that type of stuff so  \\nthat\\'s a really nice diagnostic tool to see what most of your work is is actually doing under dark work as you can see  \\nmemory used CPU use more fine-grained examples there so I I\\'d love to know if in the Q&A I\\'m going to ask were you able  \\nto execute this code and if you were in Binder just a thumb up a vote would be no would be fantastic much appreciated so  \\nas we\\'ve mentioned I just wanted to say a few things about tutorial goals the goal is to cover the basics of dark and  \\ndistributed compute we\\'d love for you to walk away with an understanding of when to use it when to not what it has to  \\noffer we\\'re going to be covering the basics of Dask delayed which although not immediately applicable to data science  \\nprovides a wonderful framework for thinking about Dask how dark works and understanding how it works under the hood then  \\nwe\\'re going to go into dark data frames and then machine learning hopefully due to the technical considerations with  \\nwe\\'ve got less Time than than we thought we would but we\\'ll definitely do the best we can we may have less Time to do  \\nexercises so we\\'ve had two people who are able to execute this code if you if you tried to execute it in Binder and were  \\nnot able to perhaps post that in the Q&A but we also have several exercises and I\\'d like you to take a minute just to do  \\nthis exercise the I I\\'m not asking you to do this because I want to know if you\\'re able to print hello world I\\'m  \\nessentially asking you to do it so you get a sense of how these exercises work so if you can take 30 seconds to print  \\nhello world then we\\'ll we\\'ll move on after that so just take 30 seconds now and it seems like we have a few more people  \\nwho are able to execute code which which was great OK fantastic so you will put your solution there for some reason I  \\nhave an extra cell here so I\\'m just going to clip that and to see a solution I\\'ll just get you to execute this cell and  \\nit provides the solution and then we can execute it and compare it to the the output of what you had OK hello world so  \\nas as we saw I\\'ve done all this locally you may have done it on Binder there is an option to work directly from the  \\ncloud and I\\'ll I\\'ll take you through this there are many ways to do this as I mentioned we\\'re working on one way with  \\ncoil and I\\'ll explain the rationale behind that in in a second but I\\'ll show you how easy it is to get a cluster up and  \\nrunning on on AWS without even interacting with AWS for free for example you can follow along by signing into coiled  \\ncloud to be clear this is not a necessity and it does involve you signing up to our product so I just wanted to be  \\nabsolutely transparent about that it does not involve any credit card information or anything  \\n \\n\\n#### 00:24:01,520::\\t\\t4 minutes Mark -> new paragraph \\n \\nalong those lines and in my opinion it does give a really nice example of how to run stuff on the cloud to do so you can  \\nsign in at cloud.coiled.io you can also pip install coiled and then do authentication you can also spin up this  \\nthis hosted coiled notebook so I\\'m going to spin that up now and I\\'m going to post that here actually yep I\\'m gonna post  \\nthat in the ch chat if you let me get this right if you\\'ve if you\\'ve never logged in to code before it\\'ll ask you to  \\nsign up using gmail or GitHub so feel free to do that if you\\'d like if not that\\'s also also cool but I just wanted to be  \\nexplicit about that the reason I want to do this is to show how Dask can be leveraged to do work on really large datasets  \\nso you will recall that I had between eight and nine gigs of ram on my local system. Oh wow! Anthony says on iPad  \\n\"unable to execute\" on Binder, incredible! I don\\'t have a strong sense of how Binder works on iPad. I do know that I was able  \\nto to check to use a Binder on my iPhone several years ago on my way to scipy doing code review for someone for Eric  \\nMaher I think for what that that\\'s worth but back to this we have this NYC taxi data set which is over 10 gigs it won\\'t  \\neven I can\\'t even store that in local memory I don\\'t have enough ram to store that so we do need either to do it locally  \\nin an out of core mode of some sort or we can we can burst to the cloud and we\\'re actually going to burst to the cloud  \\nusing using coiled so the notebook is running here for me and but I\\'m actually gonna do it from my local local notebook  \\nbut you\\'ll see and once again feel free to code along here it\\'s spinning up a notebook and James who is is my co-  \\ninstructor here is to be I\\'m I\\'m so grateful all the work is done on our notebooks in coiled you can launch the cluster  \\nhere and then analyze the entire over 10 gigs of data there I\\'m going to do it here so to do that I import coiled and  \\nthen I import the dash distributed stuff and then I can create my own software environment cluster configuration I\\'m not  \\ngoing to do that because the standard coiled cluster configuration software environment works now I\\'m going to spin up a  \\ncluster and instantiate a client now because we\\'re spinning up a cluster in in the cloud it\\'ll take it\\'ll take a minute  \\na minute or two enough Time to make a cup of coffee but it\\'s also enough Time for me to just talk a bit about why this  \\nis important and there are a lot of a lot of good good people working on on similar things but part of the motivation  \\nhere is that if you want to you don\\'t always want to do distributed data science OK first I\\'d ask you to look at instead  \\nof using dark if you can optimize your Pandas code right second I\\'d ask if you\\'ve got big data sets it\\'s a good question  \\ndo you actually need all the data so I would if you\\'re doing machine learning plot your learning curve see how accurate  \\nsee how your accuracy or whatever your metric of interest is improves as you increase the amount of data right and if it  \\nplateaus before you get to a large data size then you may as well most of the Time use your small data see if sub  \\nsampling can actually give you the results you need so you can get a bigger bigger access to a bigger machine so you  \\ndon\\'t have to burst to the cloud but after all these things if you do need to boast burst to the cloud until recently  \\nyou\\'ve had to get an AWS account you\\'ve had to you know set up containers with docker and or Kubernetes and do all of  \\nthese kind of  \\n \\n\\n#### 00:28:00,640::\\t\\t4 minutes Mark -> new paragraph \\n \\nI suppose devopsy software engineering foo stuff which which if you\\'re into that I I absolutely encourage you encourage  \\nyou to do that but a lot of working data scientists aren\\'t paid to do that and I don\\'t necessarily want to so that\\'s  \\nsomething we\\'re working on is thinking about these kind of one-click hosted deployments so you don\\'t have to do all of  \\nthat having said that I very much encourage you to try doing that stuff if if you\\'re interested we\\'ll see that the the  \\ncluster has just been created and what I\\'m going to do we see that oh I\\'m sorry I\\'ve done something funny here I\\'m I\\'m  \\nreferencing the previous client anna James yeah it looks like you should go ahead and connect a new client to the coil  \\ncluster and making sure not to re-execute the cluster creation exactly so would that be how would I what\\'s the call here  \\nI would just open up a new cell and say client equals capital client and then pass in the cluster like open parentheses  \\ncluster yeah great OK fantastic and what we\\'re seeing is a slight version this we don\\'t need to worry about this this is  \\nessentially saying that the environment on the cloud mis is there\\'s a slight mismatch with my with my local environment  \\nwe\\'re fine with that I\\'m going to look here for a certain reason the the dashboard isn\\'t quite working here at the  \\nmoment James would you suggest I just click on this and open a new yeah click on the ecs dashboard link oh yes fantastic  \\nso yep there\\'s some bug with the local dashboards that we\\'re we\\'re currently currently working on but what we\\'ll see now  \\njust a SEC I\\'m going to remove all of this we\\'ll see now that I have access to 10 workers I have access to 40 cores and  \\nI have access to over 170 gigs of memory OK so now I\\'m actually going to import this data set and it\\'s the entire year  \\nof data from 2019 and we\\'ll start seeing on on the diagnostics all the all the processing happening OK so oh actually  \\nnot yet because we haven\\'t called compute OK so it\\'s done this lazily we\\'ve imported it it shows kind of like Pandas  \\nwhen you show a data frame the column names and data types but it doesn\\'t show the data because we haven\\'t loaded it yet  \\nit does tell you how many partitions it is so essentially and we\\'ll see this soon das data frames correspond to  \\ncollections of Pandas data frames so they\\'re really 127 Pandas data frames underlying this task data frame so now I\\'m  \\ngoing to do the compute well I\\'m going to set myself up for the computation to do a group by passenger gown and look at  \\nthe main tip now that took a very small amount of Time we see the IPython magic Timing there because we haven\\'t computed  \\nit now we\\'re actually going to compute and James if you\\'ll see in the chat Eliana said her coil coiled authentication  \\nfailed I don\\'t know if you\\'re able to to help with that but if you are that would be great and it may be difficult to  \\ndebug in but look as we see we have the task stream now and we see how many you know we\\'ve got 40 cores working together  \\nwe saw the processing we saw the bytes stored it\\'s over 10 gigs as I said and we see we were able  \\n \\n\\n#### 00:32:01,519::\\t\\t4 minutes Mark -> new paragraph \\n \\nto do our basic analytics we were able to do it on a 10 plus gig data set in in 21.3 seconds which is pretty pretty  \\nexceptional if any any code based issues come up and they\\'re correlated in particular so if you have questions about the  \\ncode execution please ask in the Q&A not in the chat because others cannot vote it and I will definitively prioritize  \\nquestions on technical stuff particularly ones that up that are upvoted but yeah I totally agree thanks thanks very much  \\nso yeah let\\'s jump into into data frames so of course we write here that in the last exercise we used ask delayed to  \\nparallelize loading multiple csv files into a Pandas DataFrame we\\'re not we we haven\\'t done that but you can definitely  \\ngo through and have a look at that but I think perhaps even more immediately relevant for a data science crowd and an  \\nanalytics crowd is which is what I see here from the reasons people people have joined is jumping into Dask dataframes  \\nand as I said before, a Dask dataframe really feels like a Pandas data frame but internally it\\'s composed of many  \\ndifferent data frames this is one one way to think about it that we have all these Pandas data frames and the  \\ncollection of them is a dark data frame and as we saw before they\\'re partitioned we saw when we loaded the taxi data set  \\nin the dash data frame was 127 partitions right where each partition was a normal panda Pandas data frame and they can  \\nlive on disk as they did early in the first example dark in action or they can live on other machines as when I spun up  \\na coiled cluster and and did it on on AWS something I love about Dask data frames I mean I ran about this all the  \\ntime it\\'s how it\\'s the Pandas API and and Matt Matt Rocklin actually has a post on on the blog called a brief history of  \\nDask in which he talks about the technical goals of us but also talks about a social goal of task which in Matt\\'s words  \\nis to invent nothing he wanted and the team wanted the Dask API to be as comfortable and familiar for users as possible  \\nand that\\'s something I really appreciate about it so we see we have element element wires on operations we have the our  \\nfavorite row eyes selections we have loc we have the common aggregations we saw group buyers before we have is-ins we  \\nhave date Time string accessors oh James we forgot to I forgot to edit this and I it should be grouped by I don\\'t know  \\nwhat what a fruit buy is but that\\'s something we\\'ll make sure the next iteration to to get right at least we\\'ve got it  \\nright there and in the code but have a look at the dash data frame API docs to check out what\\'s happening and a lot of  \\nthe Time dash data frames can serve as drop in replacements for Pandas data frames the one thing that I just want to  \\nmake clear as I did before is that you need to call compute because of the lazy laser compute property of das so this is  \\nwonderful to talk about when to use data frames so if your data fits in memory use Pandas if your data fits in memory  \\nand your code doesn\\'t run super quickly I wouldn\\'t go to Dask I\\'d try to I\\'d do my best to optimize my Pandas code  \\nbefore trying to get gains gains and efficiency but dark itself becomes useful when the data set you want to analyze is  \\nlarger than your machine\\'s ram where you normally run into memory errors and that\\'s what we saw  \\n \\n\\n#### 00:36:01,520::\\t\\t4 minutes Mark -> new paragraph \\n \\nwith the taxicab example the other example that we\\'ll see when we get to [music] machine learning is you can do machine  \\nlearning on a small data set that fits in memory but if you\\'re building big models or training over like a lot of  \\ndifferent hyper parameters or different types of models you can you can parallelize that using using dark so there is  \\nyou know you want to use dash perhaps in the big data or medium to big data limit as we see here or in the medium to big  \\nmodel limit where training for example takes and takes a lot of Time OK so without further ado let\\'s get started with  \\ndas data frames you likely ran this preparation file to get the data in the previous notebook but if you didn\\'t execute  \\nthat now we\\'re going to get our file names by doing doing a few joins and we see our file is a string data NYC flights a  \\nwildcard to access all of them dot dot csv and we\\'re going to import our Dask.dataframe and read in our dataframe  \\nparsing some dates setting some sending some data types OK I\\'ll execute that we\\'ll see we have 10 partitions as we noted  \\nbefore if this was a Pandas data frame we\\'d see a bunch of entries here we don\\'t we see only the column names and the  \\ndata types of the columns and the reason is as we\\'ve said it explicitly here is the representation of the data frame  \\nobject contains no data it\\'s done Dask has done enough work to read the start of the file so that we know a bit about it  \\nsome of the important stuff and then further column types and column names and data types OK but we don\\'t once again we  \\ndon\\'t let\\'s say we\\'ve got 100 gigs of data we don\\'t want to like do this call and suddenly it\\'s reading all that stuff  \\nin and doing a whole bunch of compute until we explicitly tell it to OK now this is really cool if you know a bit of  \\nPandas you\\'ll know that you can there\\'s an attribute columns which prints out it\\'s well it\\'s actually the columns form  \\nan index right the Pandas index object and we get the we get the column names there cool Pandas in dark form we can  \\ncheck out the data types as well as we would in Pandas we see we\\'ve got some ins for the day of the week we\\'ve got some  \\nfloats for departure Time maybe we\\'d actually prefer that to be you know a date Time at some point we\\'ve got some  \\nobjects which generally are the most general on objects so generally strings so that\\'s all Pandasey type stuff in  \\naddition das data frames have an attribute n partitions which tells us the number of partitions and we saw before that  \\nthat\\'s 10 so I\\'d expect to see 10 here hey look at that now this is something that we talk about a lot in the delayed  \\nnotebook is really the task graph and I don\\'t want to say too much about that but really it\\'s a visual schematic of of  \\nthe order in which different types of compute happen OK and so the task graph for read csv tells us what happens when we  \\ncall compute and essentially it reads csv 10 ten Times zero indexed of course because Python it reads csv ten different  \\nTimes into these ten different Pandas Pandas data frames and if there were group buys or stuff after that we\\'d see them  \\nhappen in in the in the graph there and we may see an example of this in a second so once again as with Pandas we\\'re  \\ngoing to view the the head of the data frame great and we see a bunch of stuff you know we we see the first first five  \\nrows I\\'m actually also gonna gonna have a look at the  \\n \\n\\n#### 00:40:02,240::\\t\\t4 minutes Mark -> new paragraph \\n \\nthe tail the final five rows that may take longer because it\\'s accessing the the final I I there\\'s a joke and it may not  \\neven be a joke how much data analytics is actually biased by people looking at the first five rows before actually you  \\nknow interrogating the data more more seriously so how would all of our results look different if if our files were  \\nordered in in a different way that\\'s another conversation for a more philosophical conversation for another Time so now  \\nI want to show you some computations with dark data frames OK so since dash data frames implement a Pandas like API we  \\ncan just write our familiar Pandas codes so I want to look at the column departure delay and look at the maximum of that  \\ncolumn I\\'m going to call that max delay so you can see we\\'re selecting the column and then applying the max method as we  \\nwould with Pandas. Oh what happened there gives us some Dask scalar series and what\\'s happened is we haven\\'t called compute  \\nright so it hasn\\'t actually done the compute yet we\\'re going to do compute but first we\\'re going to visualize the task  \\ngraph like we did here and let\\'s try to reason what the task graph would look like right so the task graph first it\\'s  \\ngoing to read in all of these things and then it\\'ll probably perform this selector on each of these different Pandas  \\ndata frames comprising the dash data frame and then it will compute the max of each of those and then do a max on all  \\nthose maxes I think that\\'s what I would assume is happening here great so that\\'s what we\\'re what we\\'re doing we\\'re  \\nreading this so we read the first perform the first read csv get this das data frame get item I think is that selection  \\nthen we\\'re taking the max we\\'re doing the same for all of them then we take all of these max\\'s and aggregate them and  \\nthen take the max of that OK so that that\\'s essentially what\\'s happening when I call compute which I\\'m going to do now  \\nmoment of truth OK so that took around eight seconds and it tells us the max and I I\\'m sorry let\\'s let\\'s just get out  \\nsome of our dashboards up as well huh I think in this notebook we are using the single machine scheduler Hugo so I don\\'t  \\nthink there is a dashboard to be seen exactly yeah thank you for that that that catch James great is even better James  \\nwe have a question around using dark for reinforcement learning can you can you speak to that yeah so it depends on this  \\nI mean yeah short answer yes you can use GAs to train reinforcement learning models so there\\'s a package that Hugo will  \\ntalk about called Dask ML that we\\'ll see in the next notebook for distributing machine learning that paralyzes and and  \\ndistributes some existing models using desks so for instance things like random forces forest inside kit learn so so yes  \\nyou can use das to do distributed training for models I\\'m not actually sure if Dask ML implements any reinforcement  \\nlearning models in particular but that is certainly something that that can be done yeah and I\\'ll I\\'ll build on that by  \\nsaying we are about to jump into machine  \\n \\n\\n#### 00:44:00,000::\\t\\t4 minutes Mark -> new paragraph \\n \\nlearning I don\\'t think as James said I don\\'t think there\\'s reinforcement learning explicitly that that one can do but  \\nyou of course can use the das scheduler yourself to you know to distribute any reinforcement learning stuff you you have  \\nas well and that\\'s actually another another point to make that maybe James can speak to a bit more is that the dark team  \\nof course built all of these high-level collections and task arrays and dust data frames and were pleasantly surprised  \\nwhen you know maybe even up to half the people using dust came in all like we love all that but we\\'re going to use the  \\nscheduler for our own bespoke use cases right yeah exactly yeah the original intention was to like make basically a num  \\nlike a parallel numpy so that was like the desk array stuff like run run numpy and parallel on your laptop and and yeah  \\nso in order to do that we ended up building a distributed scheduler which sort of does arbitrary task computations so  \\nnot just things like you know parallel numpy but really whatever you\\'d like to throw at it and it turns out that ended  \\nup being really useful for people and so yeah now people use that sort of on their own just using the distributed  \\nscheduler to do totally custom algorithms in parallel in addition to these like nice collections like you saw Hugo  \\npresents the dash data frame API is you know the same as the panda\\'s API so there is this like familiar space you can  \\nuse things like the high-level collections but you can also run whatever custom like Hugo said bespoke computations you  \\nmight have exactly and it\\'s it\\'s been wonderful to see so many people so many people do that and the first thing as  \\nwe\\'ll see here the first thing to think about is if if you\\'re doing lifestyle compute if there\\'s anything you can you  \\nknow parallelize embarrassingly as they say right so just if you\\'re doing a hyper parameter search you just run some on  \\none worker and some on the other and there there\\'s no interaction effect so you don\\'t need to worry about that as  \\nopposed to if you\\'re trying to do you know train on streaming data where you may require it all to happen on on on the  \\nsame worker OK yeah so even think about trying to compute the standard deviation of a of a a univariate data set right  \\nin in that case you can\\'t just send you can\\'t just compute the standard deviation on two workers and then combine the  \\nresult in some some way you need to do something slightly slightly more nuanced and slightly slightly clever more clever  \\nI mean you still can actually in in that case but you can\\'t just do it as naively as that but so now we\\'re talking about  \\nparallel and distributed machine learning we have 20 minutes left so this is kind of going to be a whirlwind tour but  \\nyou know whirlwinds when safe exciting and informative I just want to make clear the material in this notebook is based  \\non the open source content from Dask\\'s tutorial repository as there\\'s a bunch of stuff we\\'ve shown you today the reason  \\nwe\\'ve done that is because they did it so well so I just want to give a shout out to all the das contributors OK so what  \\nwe\\'re going to do now is just break down machine learning scaling problems into two categories just review a bit of  \\nscikit-learn in passing solve a machine learning problem with single Michelle single Michelle I don\\'t know who she is  \\nbut single Michelle wow single machine and parallelism with scikit-learning joblib then solve an l problem with an ML  \\nproblem with multiple machines and parallelism using dark as well and we won\\'t have Time to burst for the cloud I don\\'t  \\nthink but you can also play play around with that OK so as I mentioned before when thinking about distributed compute a  \\nlot of people do it when they have large data they don\\'t necessarily think about the large model limit and this  \\nschematic kind of speaks to that if you\\'ve got a small model that fits in ram you don\\'t need to think about  \\n \\n\\n#### 00:48:00,480::\\t\\t4 minutes Mark -> new paragraph \\n \\ndistributed compute if your data size if your data is larger than your ram so your computer\\'s ram bound then you want to  \\nstart going to a distributed setting or if your model is big and CPU bound such as like large-scale hyper-parameter  \\nsearches or like ensemble blended models of like machine learning algorithms whatever it is and then of course we have  \\nthe you know big data big model limit where distributed computer desk is incredibly handy as I\\'m sure you could imagine  \\nOK and that\\'s really what I\\'ve what I\\'ve gone through here a bird\\'s-eye view of the strategies we think about if it\\'s in  \\nmemory in the bottom left quadrant just use scikit-learn or your favorite ML library otherwise known as scikit-learn  \\nfor me anyway I was going to make a note about XGBoost but I but I won\\'t for large models you can use joblib and your  \\nfavorite circuit learn estimator for large data sets use our dark ML estimators so we\\'re gonna do a whirlwind tour of  \\nscikit-learn in in five minutes we\\'re going to load in some data so we\\'ll actually generate it we\\'ll import scikit-  \\nlearn for our ML algorithm create an estimator and then check the accuracy of the model OK so once again I\\'m actually  \\ngoing to clear all outputs after restarting the kernel OK so this is a utility function of scikit-learn to create some  \\ndata sets so I\\'m going to make a classification data set with four features and 10 000 samples and just have a quick  \\nview of some of it so just a reminder on ML x is the samples matrix the size of x is the number of samples in terms of  \\nrows number of features as columns and then a feature or an attribute is what we\\'re trying to predict essentially OK so  \\nwhy is the predictor variable which we\\'re where which we\\'re or the target variable which we\\'re trying to predict so  \\nlet\\'s have a quick view of why it\\'s zeros and ones in in this case OK so yep that\\'s what I\\'ve said here why are the  \\ntargets which are real numbers for regression tasks or integers for classification or any other discrete sets of values  \\nno words about unsupervised learning at the moment we\\'re just going to support we\\'re going to fit a support vector  \\nclassifier for this example so let\\'s just load the appropriate scikit-learn module we don\\'t really need to discuss what  \\nsupport vector classifiers are at the moment now this is one of the very beautiful things about the scikit-learn API in  \\nterms of fitting the the model we instantiate a classifier and we want to fit it to the features with respect to the  \\ntarget OK so the first argument is the features second argument is the target variable so we\\'ve done that now I\\'m not  \\ngoing to worry about inspecting the learn features I just want to see how accurate it was OK and once we see how  \\naccurate it was I\\'m not gonna do this but then we can make a prediction right using estimator dot predict on a new a new  \\ndata set so this estimator will tell us so this score will tell us the accuracy and essentially that\\'s the proportion or  \\npercentage a fraction of the results that were that the estimator got correct and we\\'re doing this on the training data  \\nset we\\'ve just trained the model on this so this is telling us the accuracy on the on the training data set OK so it\\'s  \\n90  \\n \\n\\n#### 00:52:01,760::\\t\\t4 minutes Mark -> new paragraph \\n \\naccurate on the training data set if you dive into this a bit more you\\'ll recognize that if we we really want to know  \\nthe accuracy on a holdout set or a test set and it should be probably a bit lower because this is what we use to fit it  \\nOK but all that having been said I expect you know if if this is all resonating with you it means we can really move on  \\nto the distributed stuff in in a second but the other thing that that\\'s important to note is that we\\'ve trained it but a  \\nlot of model a lot of estimators and models have hyper parameters that affect the fit but you that we need to specify up  \\nfront instead of being learned during training so you know there\\'s a c parameter here there\\'s a are we using shrinking  \\nor not so we specify those we didn\\'t need to specify them because there are default values but here we specify them OK  \\nand then we\\'re going to look at the score now OK this is amazing we\\'ve got 50 accuracy which is the worst score possible  \\njust think about this if if you\\'ve got binary classification task and you\\'ve got 40 accuracy then you just flip the  \\nlabels and that changes to 60 accuracy so it\\'s amazing that we\\'ve actually hit 50 accuracy we\\'re to be congratulated on  \\nthat and what I want to note here is that we have two sets of hyper parameters we\\'ve used one\\'s created 90 actual model  \\nwith 90 accuracy another one one with 50 accuracy so we want to find the best hyper parameters essentially and that\\'s  \\nwhy hyper parameter optimization is is so important there are several ways to do hyper parameter optimization one is  \\ncalled grid search cross validation I won\\'t talk about cross validation it\\'s essentially a more robust analogue of train  \\ntest split where you train on a subset of your data and compute the accuracy on a test on a holdout set or a test set  \\ncross validation is a as I said a slightly more robust analog of this it\\'s called grid search because we have a grid of  \\nhyper parameters so we have you know in this case we have a hyper parameter c we have a hyper parameter kernel and we  \\ncan imagine them in a in a grid and we\\'re performing we\\'re checking out the score over all this gr over this entire grid  \\nof hyper parameters OK so to do that I import grid search csv now I\\'m going to compute the estimator over over these  \\ntrain the estimator over over this grid and as you see this is taking Time now OK and what I wanted to make clear and I  \\nthink should be becoming clearer now is that if we have a large hyper parameter sweep we want to do on a small data set  \\ndas can be useful for that OK because we can send some of the parameters to one worker some to another and they can  \\nperform them in parallel so that\\'s embarrassingly parallel because you\\'re you\\'re doing the same work as you would  \\notherwise but sending it to a bunch of different workers we saw that took 30 seconds which is in my realm of comfort as  \\na data scientist I\\'m happy to wait 30 seconds if I had to wait much longer if this grid was bigger I\\'d start to get  \\nprobably a bit frustrated but we see that it computed it for c is equal to all combinations of these essentially OK so  \\nthat\\'s really all I wanted to say there and then we can see the best parameters and the best score so the best score was  \\n0.098 and it was c10 and the kernel rbf a radial basis function it doesn\\'t even Matter what that is though for the  \\npurposes of this so we\\'ve got 10 minutes left we\\'re going to we\\'re going to make it I can feel it I have a good I have a  \\ngood sense  \\n \\n\\n#### 00:56:00,400::\\t\\t4 minutes Mark -> new paragraph \\n \\n a good after the I mean this demo is actually going incredibly well given the initial technical hurdles so touchwood  \\nHugo OK so what we\\'ve done is we\\'ve really segmented ML scaling problems into two categories CPU bound and ram bound and  \\nI I really I can\\'t emphasize that enough because I see so many people like jumping in to use new cool technologies  \\nwithout perhaps taking it being a bit mindful and intentional about it and reasoning about when things are useful and  \\nand when not I suppose the one point there is that sure data science is a technical discipline but there are a lot of  \\nother aspects to it involving this type of reasoning as well so we then carried out a typical sklearn workflow for ML  \\nproblems with small models and small data and we reviewed hyper parameters and hyper parameter optimization so in this  \\nsection we\\'ll see how joblib which is a set of tools to provide lightweight pipelining in Python gives us parallelism  \\non our laptop and then we\\'ll see how dark ML can give us awesome parallelism on on clusters OK so essentially what I\\'m  \\ndoing here is I\\'m doing exactly the same as above with a grid search but I\\'m using the quark the keyword argument n jobs  \\nwhich tells you how many tasks to run in parallel using the cause available on your local workstation and specifying  \\nminus one jobs means the it just runs them the maximum possible OK so I\\'m going to execute that great so we should be  \\ndone in a second feel free to ask any questions in the chat oh Alex has a great question in the Q&A does das have see a  \\nsequel and query optimizer I\\'m actually so excited that [music] and James maybe you can provide a couple of links to  \\nthis we\\'re really excited to have seen dark dust SQL developments there recently so that\\'s dark hyphen hyphen SQL and  \\nwe\\'re actually we\\'re working on some some content and a blog post and maybe a live live coding session about that in in  \\nthe near future so if anyone if you want updates from from coil feel free to go to our website and sign up for our  \\nmailing list and we\\'ll let you know about all of this type of stuff but the short answer is yes Alex and it\\'s getting  \\nbetter and if James is able to post post a link there that would be that would be fantastic so we\\'ve done link in the  \\nchat fantastic [music] and so we\\'ve we\\'ve seen how we have [music] single machine parallelism here using the using the  \\nend jobs quark and in the final minutes let\\'s see multiple multi-machine parallelism with Dask OK so what I\\'m going to  \\ndo is I\\'m going to do my imports and create my client incentive my client and check it out OK so once again I\\'m working  \\nlocally I hit search and that\\'ll task is pretty smart in terms of like knowing which which client I want to check out do  \\nthe tasks stream because it\\'s my favorite I\\'ll do the cluster map otherwise known as the pew pew map and then I want  \\nsome progress we all we all crave progress don\\'t we and maybe my workers tab OK great so we\\'ve got that up and running  \\nnow I\\'m going to do a slightly larger hyper parameter search OK so remember we had just a couple for c a couple for  \\nkernel we\\'re going to do more we have some for shrinking now I\\'m actually going to comment that out because I don\\'t know  \\nhow long that\\'s going to take if you\\'re coding them on Binder now this May actually take far far too long for you but  \\nwe\\'ll we\\'ll see so I\\'ll execute this code and we should see just sick no we shouldn\\'t see any work happening yet but  \\nwhat I\\'m doing here is oh looks like OK my clusters back up great we\\'re doing our grid search but we\\'re going to use  \\nDask as as the back end right and this is a context manager where we\\'re asserting that and and we can just discuss the  \\nthe syntax there but it\\'s not particularly important currently I\\'m going to execute this now and let\\'s see fantastic  \\nwe\\'ll see all this data transfer happening here we\\'ll see our tasks happening here we can see these big batches of fit  \\nand score fit so fitting fitting the models then finding how well they perform via this k-fold cross validation which is  \\nreally cool and let\\'s just yep we can see what\\'s happening here we can see we currently have 12 processing we\\'ve got  \\nseven in memory and we have several more that we need to do our desk workers we can see us oh we can see our CPU usage  \\nwe can see how we can see CPU usage across all the workers which is which is pretty cool seeing that distribution is is  \\nreally nice whenever some form of b swarm plot if you have enough would would be useful there or even some form of  \\ncumulative distribution function or something like that not a histogram people OK you can go to my Bayesian tutorial  \\nthat I\\'ve taught here before to hear me rave about the the horrors of histograms so we saw that talk a minute which is  \\ngreat and we split it across you know eight cores or whatever it is and now we\\'ll have a look once again we get the same  \\nbest performer which is which is a sanity check and that\\'s pretty cool I think we have a we actually have a few minutes  \\nleft so I am gonna just see if I can oh let me think yeah I will see if I can burst burst to the cloud and and and do  \\nthis that will take a minute a minute or two to create the cluster again but while we\\'re while we\\'re doing that I\\'m  \\nwondering if we have any any questions or if anyone has any feedback on on this workshop I very much welcome welcome  \\nthat perhaps if there are any final messages you\\'d you\\'d like to say James while we\\'re spinning this up you can you can  \\nlet me know yeah sure I just also first off wanted to say thanks everyone for attending and like bearing  \\n \\n\\n#### 01:04:01,119::\\t\\t4 minutes Mark -> new paragraph \\n \\nbearing with us with the technical difficulties really appreciate that real quick I\\'m just yeah so if you have if you  \\nhave questions please post in the Q&A section while the cold cluster\\'s spinning up Theodore posted in the last largest  \\nexample of grid search how much performance gain did we get from using das and not just in jobs hmm that\\'s a great  \\nquestion and we actually didn\\'t see let\\'s see so it took 80 seconds ah let me get this they\\'re actually not comparable  \\nbecause I did the grid search over a different set of hyper parameters I did it over a larger set of hyper parameters  \\nright so when I did end jobs I did it there were only it was a two by two grid of hyper parameters whereas when I did it  \\nwith with Dask it was a one two three four five six six by three so let\\'s just reason about that this one was eighteen  \\nsix by three is eighteen which took eighty seconds and this one was two by two so it was four and it took 26 seconds so  \\na minor gain I think with this hyper parameter search if you multiply that by by four you\\'ll well 4.2 4.5 you\\'ll need  \\nthat would have taken maybe two minutes or something something like that so we saw some increase in efficiency not a  \\ngreat deal but James maybe you can say more to this part of the reason for that is that we\\'re doing it on kind of a very  \\nsmall example so we won\\'t necessarily see the gains in efficiency with a data set this size and with a small hyper  \\nparameter suite like this is that right yeah yeah and yeah exactly and I guess also this is more of an kind of an  \\nillustrative point here I guess so you\\'re just using directly using in jobs with something like joblib by default we\\'ll  \\nuse local threads and processes on like whatever machine you happen to be running on so like in this case on Hugo\\'s  \\nlaptop one of the real advantages of using joblib with the das back in will actually dispatch back to to run tasks on a  \\nDask cluster is that your cluster can expand beyond what local resources you have so you can run you know you can  \\nbasically scale out like for instance using the coil cluster to have many many CPUs and a large amount of ram that you  \\nwouldn\\'t have on your locally table to run and there you\\'ll see both large performance gains as well as you\\'ll be able  \\nto expand your the set of possible problems you can solve to larger than ram scenarios so you\\'re out of out of core  \\ntraining exactly and thank you Jack this was absolutely unplanned and we didn\\'t plan that question but that\\'s a  \\nwonderful segue into me now performing exactly the same compute with the same code using the Dask as the parallel back  \\nend on a on a coiled cluster which is an AWS cluster right so we can I\\'m more currently anyway so I will execute this  \\ncode and it\\'s exactly the same as we did whoa OK great so we see our tasks task stream here you see once again we see  \\nthe majority is being batch fit and and getting the scores out similarly we see the same result being the best I\\'ll just  \\nnotice that for this for this small task doing it on the cloud took 20 seconds doing it locally for me took 80 seconds  \\nso that\\'s a four-fold increase in performance on a very small task so imagine what that does if you can take the same  \\ncode as you\\'ve written  \\n \\n\\n#### 01:08:00,240::\\t\\t4 minutes Mark -> new paragraph \\n \\nhere and burst to the cloud with with one click or however however you do it I think that that\\'s incredibly powerful and  \\nthat the fact that your code and what\\'s happening in the back end with Dask generalizes immediately to the new setting  \\nof working on a cluster I personally find very exciting and if you work with larger data sets or building larger models  \\nor big hyper parameter sweeps I\\'m pretty sure it\\'s an exciting option for all of you also so on that note I\\'d like to  \\nreiterate James what James said and thanking you all so much for joining us for asking great questions and for bearing  \\nwith us through some some technical technical hurdles but it made it even even funnier when when we got up and running  \\nonce again I\\'d love to thank Mark Christina and and the rest of the organizers for doing such a wonderful job and doing  \\nsuch a great service to the data science and machine learning community and ecosystem worldwide so thank you once again  \\nfor having us thank you Hugo and James I have to say like with all the technical difficulties I was actually giggling  \\nbecause it was kind of funny yeah but we\\'re very sorry and we thank you for your patience and sticking through it and I  \\nwill be editing this video to you know make it as efficient as possible and have that available Tim supercard thank you  \\ngreat and I\\'ll just ask you if you are interested in checking out coiled go to our website if you want to check out our  \\nproduct go to cloud.coil.io we started building this company in February we\\'re really excited about building a new  \\nproduct so if you\\'re interested reach out we\\'d love to chat with you about what we\\'re doing and what we\\'re up to and  \\nit\\'s wonderful to be in the same community as you all, so thanks!  \\n   '\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'<!-- Editing Guide: The pipe (|) position in this comment is 120:                                                       | -->\n",
      "### Introduction\n",
      "\n",
      "Okay hello and welcome to Data Umbrella's webinar for October so I'm just going to go over the agenda I'm going to do a  \n",
      "brief introduction then there will be the workshop by Hugo and James and you can ask questions along the way in the chat  \n",
      "or actually the best place to ask questions is the Q&A and there's an option to upvote as well so yet asking the Q&A if  \n",
      "you happen to post it on the chat by mistake I can also transfer it over to Q&A so that would be fine too and this  \n",
      "webinar is being recorded. Briefly about me. I am a statistician and data scientist and I am the founder of Data Umbrella.  \n",
      "I am on a lot of platforms as Reshama so feel free to follow me on Twitter and LinkedIn. We have a code of conduct we're  \n",
      "dedicated to providing a harassment-free experience for everyone. Thank you for helping to make this a welcoming friendly  \n",
      "professional community for all and this code of conduct applies to the chat as well. So our mission is to provide an  \n",
      "inclusive community for underrepresented persons in data science and we are an all volunteer-run organization you can  \n",
      "support Data Umbrella by doing the following things: You can follow our code of conduct and keep our community a place  \n",
      "where everybody wants to keep coming to; You can donate to our open collective and that helps to pay meet-up dues and  \n",
      "other operational costs and you can check out this link here on GitHub we have this new initiative where all the videos  \n",
      "are being transcribed and so is to make them more accessible. So we take the YouTube videos and we put the raw there and  \n",
      "so we've had a number of volunteers help us transcribe it so feel free to check out this link and maybe if you do this  \n",
      "video maybe the two speakers will follow you on Twitter. I can't promise anything, but it's possible Data Umbrella has a  \n",
      "job board and it's at jobs.org and once this gets started I'll put some links in the chat. The job that we are  \n",
      "highlighting today is the machine learning engineer job by Development Seed. Development Seed is based in  \n",
      "Washington DC and Lisbon Portugal and they do I'm going to go to the next slide what they do is they're doing social  \n",
      "good work and so they're doing for instance mapping elections from Afghanistan to the US analyzing public health and  \n",
      "economic data from Palestine to Illinois and leading the strategy and development behind data world bank and some other  \n",
      "organizations. I will share a link to their job posting in the chat as well as soon as I finish this brief  \n",
      "introduction. Check out our website for resources there's a lot of resources on learning Python and R also for  \n",
      "contributing to open source also for guides on accessibility and responsibility and allyship. We have a monthly  \n",
      "newsletter that goes out towards the end of the month and it has information on our upcoming events. We have two great  \n",
      "events coming up in November and December on open source so subscribe to our newsletter to be in the know. We are on all  \n",
      "social media platforms as Data Umbrella Meetup is the best place to join to find out about upcoming events our website  \n",
      "has resources follow us on Twitter we also share a lot of information on LinkedIn and if you want to subscribe to our  \n",
      "YouTube channel we record all of our talks and post them there within about a week of the talk so it's a good way to get  \n",
      "information. OK and now we are ready to get started so I will put myself on mute and I will hand it over to Hugo and James  \n",
      "and let you take over but thank you all for joining!   \n",
      " \n",
      "\n",
      "#### 00:04:03,120::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "I just want to thank Reshama Christina and everyone else who tied all the tireless effort that  \n",
      "that goes into putting these meet-ups and these online sessions together I think one thing I want to say is actually  \n",
      "the last in-person workshop I gave either at the end of February or early March was Data Umbrella's inaugural  \n",
      "tutorial and Meetup if I recall correctly on Bayesian thinking and hacker statistics and simulation and  \n",
      "that type of stuff so it's just wonderful to be back particularly with my colleague and friend James we're  \n",
      "building really cool distributed data science products at coiled we'll say a bit about that but we'll do some  \n",
      "introductions in a bit I just wanted to get you all accustomed to it was February thank you Reshama we're working  \n",
      "with Jupyter notebooks in a GitHub repository the repository is pinned to the top of the chat this is what it looks like  \n",
      "these are all the files this is the file system now we use something called Binder which is a project out of and related  \n",
      "to project Jupyter which provides infrastructure to run notebooks without any local installs so there are two  \n",
      "ways you can code along on this tutorial the first is and I won't get you to do this yet is to launch Binder the  \n",
      "reason I won't get you to do that yet is because once you launch it we have 10 minutes to start coding or the Binder  \n",
      "session Times out I've been burnt by that before actually several Times I'm surprised I even remembered it this time the  \n",
      "other thing you can do is install everything locally by cloning the repository downloading anaconda creating a conda  \n",
      "environment if you haven't done that I suggest you do not do that now and you launch the Binder James is going to  \n",
      "start by telling us a few things about GAs and distributed computing in general my question for you James is  \n",
      "if we get people to launch this now will we get to execute a cell code cell in 10 minutes I would let's hold off for now  \n",
      "maybe yep maybe I'll indicate when we should launch Binder OK fantastic cool and just what I'm looking at right now is  \n",
      "the GitHub repository on your browser OK exactly so I will not launch Binder now I will not get you to now I've I'm  \n",
      "doing this locally and we see that I'm in notebook zero and if you want to actually have a look at this notebook before  \n",
      "launching Binder, it's in the notebooks Data Umbrella subdirectory and it's notebook zero and we're going to hopefully  \n",
      "make it through the overview then chatting about Dask, Dask delayed and data framing and machine learning great so we  \n",
      "have Hashim has said you could open in VSCode as well. You could. I mean that would require all your local installs and  \n",
      "that that type of stuff as well but we're to introduce me and James we work at coiled where we build products for  \n",
      "distributed computing in infrastructure as we'll see one of the big problems with like bursting to the cloud is all the  \n",
      "like Kubernetes AWS docker stuff so we build a one-click host of deployments for das but for data science and machine  \n",
      "learning in general James maintains task along with Matt Matt Rocklin who created Dask with a team of people and was  \n",
      "working with Continuum Anaconda at the Time and James is a software engineer at coiled and I run data science evangelism  \n",
      "Marketing work on a bunch of product stuff as well wear a bunch of different hats occasionally.  \n",
      " \n",
      "\n",
      "#### 00:08:01,680::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "There are many ways to think about distributed compute and how to do it in in Python we're going to present  \n",
      "hey James you're muted I'm taking it I went away based on what I see in the chat you did you did but now we're back I've  \n",
      "introduced you I've introduced me I've mentioned that there are many ways to do distributed compute in the Python  \n",
      "ecosystem and we'll be chatting about one called Dask and maybe I'll pass you in a second but I'll say one thing that I  \n",
      "really like about my background isn't in distributed compute my background's in Pythonic data science when thinking  \n",
      "about bursting to larger data sets and larger models there are a variety of options the thing that took me attracted me  \n",
      "to desk originally I saw Cameron's note the ghost in the machine aren't playing nice tonight I think that ain't that the  \n",
      "truth is that dark plays so nicely with the entire PyData ecosystem so as we'll see if you want to write dash code for  \n",
      "data frames dash data frames it really mimics your Pandas code same with numpy same with scikit-learn OK and the other  \n",
      "thing is dark essentially runs the Python code under the hood so your mental model of what's happening is actually  \n",
      "corresponds to the code being being executed OK now I'd like to pass over to James but it looks like he's disappeared  \n",
      "again I'm still here if you can hear me I've just turned my camera off oh yeah OK great I'm gonna turn my camera  \n",
      "hopefully that will help yeah and I might do do the same for bandwidth bandwidth issues so if if you want to jump in and  \n",
      "and talk about dark at a high level I'm sharing my screen and we can scroll through yeah that sounds great so that's  \n",
      "sort of a nutshell you can think of it as being composed of two main well components the first we call collections these  \n",
      "are the user interfaces that you use to actually construct a computation you would like to compute in parallel or on  \n",
      "distributed hardware there are a few different interfaces that Dask implements. For instance, there's Dask array for doing  \n",
      "nd array computations there's Dask dataframe for working with tabular data you can think of those as like Dask array as  \n",
      "a parallel version of numpy. Dask dataframe has a parallel version of Pandas and so on there are also a couple other  \n",
      "interfaces that we'll be talking about das delayed for instance we'll talk about that today we'll also talk about the  \n",
      "futures API those are sort of for lower level custom algorithms in sort of paralyzing existing existing code the main  \n",
      "takeaway is that there are several sort of familiar APIs that desk implements and that will use today to actually  \n",
      "construct your computation so that's the first part of desk it is these dash collections you then take these collections  \n",
      "set up your steps for your computation and then pass them off to the second component which are desk schedulers and  \n",
      "these will actually go through and execute your computation potentially in parallel there are two flavors of schedulers  \n",
      "that desk offers the first is a are called single machine schedulers and these just take advantage of your local  \n",
      "hardware they will spin up a a local thread or process pool and start submitting tasks in your computation to to be  \n",
      "executed in parallel either on multiple threads or multiple processes there's also a distributed scheduler or maybe a  \n",
      "better term for would actually be called the advanced scheduler because it works well on a single machine but it also  \n",
      "scales out to multiple machines so for instance as you'll see later we will actually spin up a distributed scheduler  \n",
      "that has workers on remote  \n",
      " \n",
      "\n",
      "#### 00:12:00,160::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "machines on AWS so you can actually scale out beyond your local resources like say what's on your laptop kind of  \n",
      "scrolling down then to the image of the cluster we can see the main components of the distributed scheduler and James I  \n",
      "might get people to spin up the Binder now because we're going to execute codes now is a good point yep so just here's a  \n",
      "quick break point before you know a teaser for schedulers and what's happening there I'll ask you to in the repository  \n",
      "there's also the link to the Binder click on launch Binder I'm going to open it in a new tab and what this will create  \n",
      "is an environment in which you can just execute the code in in the notebooks OK so hopefully by the Time we've gotten  \n",
      "gone through this section this will be ready to start executing code so if everyone wants to do that to code along  \n",
      "otherwise just watch or if you're running things locally also cool thanks James yeah yeah no problem thank you so so  \n",
      "yeah looking at the image for the distributed scheduler we're not gonna have Time to go into the a lot of detail about  \n",
      "the distributed scheduler in this workshop so but we do want to provide at least a high level overview of the the  \n",
      "different parts and components of the distributed scheduler so the first part I want to talk about is in the diagram  \n",
      "what's labeled as a client so this is the user facing entry point to a cluster so wherever you are running your Python  \n",
      "session that could be in a Jupyter lab session like we are here that could be in a Python script somewhere you will  \n",
      "create and instantiate a client object that connects to the second component which is the das scheduler so each desk  \n",
      "cluster has a single scheduler in it that sort of keeps track of all of the state for all of the the state of your  \n",
      "cluster and all the tasks you'd like to compute so from your client you might start submitting tasks to the cluster the  \n",
      "schedule will receive those tasks and compute things like all the dependencies needed for that task like say you're  \n",
      "implementing you say you want to compute task c but that actually requires first you have to compute task b and task a  \n",
      "like there are some dependency structures there it'll compute those dependencies as well as keep track of them it'll  \n",
      "also communicate with all the workers to understand what worker is working on which task and as space frees up on the  \n",
      "workers it will start farming out new tasks to compute to the workers so in this particular diagram there are three das  \n",
      "distributed workers here however you can have as you can have thousands of workers if you'd like so the workers are the  \n",
      "things that actually compute the tasks they also store the results of your tasks and then serve them back to you and the  \n",
      "client the scheduler basically manages all the state needed to perform the computations and you submit tasks from the  \n",
      "client so that's sort of a quick whirlwind tour of the different components for the distributed scheduler and at this  \n",
      "point I think it'd be great to actually see see some of this in action Hugo would like to take over absolutely thank you  \n",
      "for that wonderful introduction to Dask and and the schedulers in particular and we are going to see that with dark in  \n",
      "action I'll just note that this tab in which I launched the Binder is up and running if you're going to execute code  \n",
      "here click on notebooks click on Data Umbrella oop and then go to the overview notebook and you can drag around we'll  \n",
      "see the utility of these these dashboards in a second but you can you know drag your stuff around to to make you know  \n",
      "however you want to want to structure it and then you can execute code in here I'm not going to do that I'm going to do  \n",
      "this locally at the moment but just to see dust in action to begin with I'm going to I'm actually going to  \n",
      " \n",
      "\n",
      "#### 00:16:02,720::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "restart kernel and clear my outputs so I'm going to import from dash distributed the client the sorry the other thing I  \n",
      "wanted to mention is we made a decision around content for this we do have a notebook that we we love to teach on  \n",
      "schedulers but we decided to switch it out for machine learning for this workshop in particular we are teaching a  \n",
      "similar although distinct workshop at PyData global so we may see some of you there in which we'll be going more in  \n",
      "depth into schedulers as well so if you want to check that out definitely do so we instantiate the client which as James  \n",
      "mentioned is kind of what we work with as the user to submit our code so that will take take a few seconds OK it's got a  \n",
      "port in you so it's going going elsewhere what I'll just first get you to notice is that it tells us where our dashboard  \n",
      "is and we'll see those tools in a second tells us about our cluster that we have four workers eight cores between eight  \n",
      "and nine gigs of of ram OK now this is something I really love about Dask all the diagnostic tools if I click on the  \n",
      "little desk thing here and we've modified the Binder so that that exists there as well we can see I'll hit search and it  \n",
      "should that now corresponds to the the scheduler now I want to look at the task stream which will tell us in real Time  \n",
      "what's happening I also want to look at the cluster map so we see here this is already really cool we've got all of our  \n",
      "workers around here and our scheduler scheduler there and when we start doing some compute we'll actually see  \n",
      "information flowing between these and the other thing maybe I'll yeah I'll include a little progress and that can be an  \n",
      "alternate tab to ask I'm wondering perhaps I also want to include something about the workers yeah OK great so we've got  \n",
      "a bunch of stuff that's that's pretty interesting there and so the next thing I'm going to do we've got a little utility  \n",
      "file which downloads some of the data and this is what it does is if you're in Binder it downloads a subset of the data  \n",
      "if you're anywhere else it loads a larger set for this particular example we're dealing with a small data set you see  \n",
      "the utility of dark and distributed compute when it generalizes to larger data sets but for pedagogical purposes we're  \n",
      "going to sit with a smaller data set so that we can actually run run the code there's a trade-off there so actually that  \n",
      "was already downloaded it seems but you should all see it download I'm actually going to run that in the Binder just to  \n",
      "you should start seeing downloading NYC flights data set done extracting creating json data etc OK now what we're going  \n",
      "to do is we're going to read in this data as a Dask data frame and what I want you to notice is that it really the das  \n",
      "code mimics Pandas code so instead of pd read csv we've got dd read csv we've got you know this is the file path the  \n",
      "first argument we're doing some parse date setting some data types OK we've got a little wild card regular expression  \n",
      "there to to join to do a bunch of them and then we're performing a group by OK so we're grouping by the origin of these  \n",
      "flight flight data we're looking at the the mean departure delay group by origin the the one difference I want to make  \n",
      "clear is that in das we need a compute method that's because das performs lazy computation it won't actually do anything  \n",
      "because you don't want it to do anything on really large data sets until you explicitly tell it tell it to compute so  \n",
      "I'm going to execute this now and we should see some information  \n",
      " \n",
      "\n",
      "#### 00:20:01,520::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "transfer between the scheduler and the workers and we should see tasks starting starting to be done OK so moment of  \n",
      "truth fantastic so we call this a pew pew plot because we see pew pew pew we saw a bunch of data transfer happening  \n",
      "between them these are all our cause and we can see tasks happening it tells us what tasks there are we can see that  \n",
      "most of the Time was spent reading csvs then we have some group bias on chunks and and that type of stuff so  \n",
      "that's a really nice diagnostic tool to see what most of your work is is actually doing under dark work as you can see  \n",
      "memory used CPU use more fine-grained examples there so I I'd love to know if in the Q&A I'm going to ask were you able  \n",
      "to execute this code and if you were in Binder just a thumb up a vote would be no would be fantastic much appreciated so  \n",
      "as we've mentioned I just wanted to say a few things about tutorial goals the goal is to cover the basics of dark and  \n",
      "distributed compute we'd love for you to walk away with an understanding of when to use it when to not what it has to  \n",
      "offer we're going to be covering the basics of Dask delayed which although not immediately applicable to data science  \n",
      "provides a wonderful framework for thinking about Dask how dark works and understanding how it works under the hood then  \n",
      "we're going to go into dark data frames and then machine learning hopefully due to the technical considerations with  \n",
      "we've got less Time than than we thought we would but we'll definitely do the best we can we may have less Time to do  \n",
      "exercises so we've had two people who are able to execute this code if you if you tried to execute it in Binder and were  \n",
      "not able to perhaps post that in the Q&A but we also have several exercises and I'd like you to take a minute just to do  \n",
      "this exercise the I I'm not asking you to do this because I want to know if you're able to print hello world I'm  \n",
      "essentially asking you to do it so you get a sense of how these exercises work so if you can take 30 seconds to print  \n",
      "hello world then we'll we'll move on after that so just take 30 seconds now and it seems like we have a few more people  \n",
      "who are able to execute code which which was great OK fantastic so you will put your solution there for some reason I  \n",
      "have an extra cell here so I'm just going to clip that and to see a solution I'll just get you to execute this cell and  \n",
      "it provides the solution and then we can execute it and compare it to the the output of what you had OK hello world so  \n",
      "as as we saw I've done all this locally you may have done it on Binder there is an option to work directly from the  \n",
      "cloud and I'll I'll take you through this there are many ways to do this as I mentioned we're working on one way with  \n",
      "coil and I'll explain the rationale behind that in in a second but I'll show you how easy it is to get a cluster up and  \n",
      "running on on AWS without even interacting with AWS for free for example you can follow along by signing into coiled  \n",
      "cloud to be clear this is not a necessity and it does involve you signing up to our product so I just wanted to be  \n",
      "absolutely transparent about that it does not involve any credit card information or anything  \n",
      " \n",
      "\n",
      "#### 00:24:01,520::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "along those lines and in my opinion it does give a really nice example of how to run stuff on the cloud to do so you can  \n",
      "sign in at cloud.coiled.io you can also pip install coiled and then do authentication you can also spin up this  \n",
      "this hosted coiled notebook so I'm going to spin that up now and I'm going to post that here actually yep I'm gonna post  \n",
      "that in the ch chat if you let me get this right if you've if you've never logged in to code before it'll ask you to  \n",
      "sign up using gmail or GitHub so feel free to do that if you'd like if not that's also also cool but I just wanted to be  \n",
      "explicit about that the reason I want to do this is to show how Dask can be leveraged to do work on really large datasets  \n",
      "so you will recall that I had between eight and nine gigs of ram on my local system. Oh wow! Anthony says on iPad  \n",
      "\"unable to execute\" on Binder, incredible! I don't have a strong sense of how Binder works on iPad. I do know that I was able  \n",
      "to to check to use a Binder on my iPhone several years ago on my way to scipy doing code review for someone for Eric  \n",
      "Maher I think for what that that's worth but back to this we have this NYC taxi data set which is over 10 gigs it won't  \n",
      "even I can't even store that in local memory I don't have enough ram to store that so we do need either to do it locally  \n",
      "in an out of core mode of some sort or we can we can burst to the cloud and we're actually going to burst to the cloud  \n",
      "using using coiled so the notebook is running here for me and but I'm actually gonna do it from my local local notebook  \n",
      "but you'll see and once again feel free to code along here it's spinning up a notebook and James who is is my co-  \n",
      "instructor here is to be I'm I'm so grateful all the work is done on our notebooks in coiled you can launch the cluster  \n",
      "here and then analyze the entire over 10 gigs of data there I'm going to do it here so to do that I import coiled and  \n",
      "then I import the dash distributed stuff and then I can create my own software environment cluster configuration I'm not  \n",
      "going to do that because the standard coiled cluster configuration software environment works now I'm going to spin up a  \n",
      "cluster and instantiate a client now because we're spinning up a cluster in in the cloud it'll take it'll take a minute  \n",
      "a minute or two enough Time to make a cup of coffee but it's also enough Time for me to just talk a bit about why this  \n",
      "is important and there are a lot of a lot of good good people working on on similar things but part of the motivation  \n",
      "here is that if you want to you don't always want to do distributed data science OK first I'd ask you to look at instead  \n",
      "of using dark if you can optimize your Pandas code right second I'd ask if you've got big data sets it's a good question  \n",
      "do you actually need all the data so I would if you're doing machine learning plot your learning curve see how accurate  \n",
      "see how your accuracy or whatever your metric of interest is improves as you increase the amount of data right and if it  \n",
      "plateaus before you get to a large data size then you may as well most of the Time use your small data see if sub  \n",
      "sampling can actually give you the results you need so you can get a bigger bigger access to a bigger machine so you  \n",
      "don't have to burst to the cloud but after all these things if you do need to boast burst to the cloud until recently  \n",
      "you've had to get an AWS account you've had to you know set up containers with docker and or Kubernetes and do all of  \n",
      "these kind of  \n",
      " \n",
      "\n",
      "#### 00:28:00,640::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "I suppose devopsy software engineering foo stuff which which if you're into that I I absolutely encourage you encourage  \n",
      "you to do that but a lot of working data scientists aren't paid to do that and I don't necessarily want to so that's  \n",
      "something we're working on is thinking about these kind of one-click hosted deployments so you don't have to do all of  \n",
      "that having said that I very much encourage you to try doing that stuff if if you're interested we'll see that the the  \n",
      "cluster has just been created and what I'm going to do we see that oh I'm sorry I've done something funny here I'm I'm  \n",
      "referencing the previous client anna James yeah it looks like you should go ahead and connect a new client to the coil  \n",
      "cluster and making sure not to re-execute the cluster creation exactly so would that be how would I what's the call here  \n",
      "I would just open up a new cell and say client equals capital client and then pass in the cluster like open parentheses  \n",
      "cluster yeah great OK fantastic and what we're seeing is a slight version this we don't need to worry about this this is  \n",
      "essentially saying that the environment on the cloud mis is there's a slight mismatch with my with my local environment  \n",
      "we're fine with that I'm going to look here for a certain reason the the dashboard isn't quite working here at the  \n",
      "moment James would you suggest I just click on this and open a new yeah click on the ecs dashboard link oh yes fantastic  \n",
      "so yep there's some bug with the local dashboards that we're we're currently currently working on but what we'll see now  \n",
      "just a SEC I'm going to remove all of this we'll see now that I have access to 10 workers I have access to 40 cores and  \n",
      "I have access to over 170 gigs of memory OK so now I'm actually going to import this data set and it's the entire year  \n",
      "of data from 2019 and we'll start seeing on on the diagnostics all the all the processing happening OK so oh actually  \n",
      "not yet because we haven't called compute OK so it's done this lazily we've imported it it shows kind of like Pandas  \n",
      "when you show a data frame the column names and data types but it doesn't show the data because we haven't loaded it yet  \n",
      "it does tell you how many partitions it is so essentially and we'll see this soon das data frames correspond to  \n",
      "collections of Pandas data frames so they're really 127 Pandas data frames underlying this task data frame so now I'm  \n",
      "going to do the compute well I'm going to set myself up for the computation to do a group by passenger gown and look at  \n",
      "the main tip now that took a very small amount of Time we see the IPython magic Timing there because we haven't computed  \n",
      "it now we're actually going to compute and James if you'll see in the chat Eliana said her coil coiled authentication  \n",
      "failed I don't know if you're able to to help with that but if you are that would be great and it may be difficult to  \n",
      "debug in but look as we see we have the task stream now and we see how many you know we've got 40 cores working together  \n",
      "we saw the processing we saw the bytes stored it's over 10 gigs as I said and we see we were able  \n",
      " \n",
      "\n",
      "#### 00:32:01,519::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "to do our basic analytics we were able to do it on a 10 plus gig data set in in 21.3 seconds which is pretty pretty  \n",
      "exceptional if any any code based issues come up and they're correlated in particular so if you have questions about the  \n",
      "code execution please ask in the Q&A not in the chat because others cannot vote it and I will definitively prioritize  \n",
      "questions on technical stuff particularly ones that up that are upvoted but yeah I totally agree thanks thanks very much  \n",
      "so yeah let's jump into into data frames so of course we write here that in the last exercise we used ask delayed to  \n",
      "parallelize loading multiple csv files into a Pandas DataFrame we're not we we haven't done that but you can definitely  \n",
      "go through and have a look at that but I think perhaps even more immediately relevant for a data science crowd and an  \n",
      "analytics crowd is which is what I see here from the reasons people people have joined is jumping into Dask dataframes  \n",
      "and as I said before, a Dask dataframe really feels like a Pandas data frame but internally it's composed of many  \n",
      "different data frames this is one one way to think about it that we have all these Pandas data frames and the  \n",
      "collection of them is a dark data frame and as we saw before they're partitioned we saw when we loaded the taxi data set  \n",
      "in the dash data frame was 127 partitions right where each partition was a normal panda Pandas data frame and they can  \n",
      "live on disk as they did early in the first example dark in action or they can live on other machines as when I spun up  \n",
      "a coiled cluster and and did it on on AWS something I love about Dask data frames I mean I ran about this all the  \n",
      "time it's how it's the Pandas API and and Matt Matt Rocklin actually has a post on on the blog called a brief history of  \n",
      "Dask in which he talks about the technical goals of us but also talks about a social goal of task which in Matt's words  \n",
      "is to invent nothing he wanted and the team wanted the Dask API to be as comfortable and familiar for users as possible  \n",
      "and that's something I really appreciate about it so we see we have element element wires on operations we have the our  \n",
      "favorite row eyes selections we have loc we have the common aggregations we saw group buyers before we have is-ins we  \n",
      "have date Time string accessors oh James we forgot to I forgot to edit this and I it should be grouped by I don't know  \n",
      "what what a fruit buy is but that's something we'll make sure the next iteration to to get right at least we've got it  \n",
      "right there and in the code but have a look at the dash data frame API docs to check out what's happening and a lot of  \n",
      "the Time dash data frames can serve as drop in replacements for Pandas data frames the one thing that I just want to  \n",
      "make clear as I did before is that you need to call compute because of the lazy laser compute property of das so this is  \n",
      "wonderful to talk about when to use data frames so if your data fits in memory use Pandas if your data fits in memory  \n",
      "and your code doesn't run super quickly I wouldn't go to Dask I'd try to I'd do my best to optimize my Pandas code  \n",
      "before trying to get gains gains and efficiency but dark itself becomes useful when the data set you want to analyze is  \n",
      "larger than your machine's ram where you normally run into memory errors and that's what we saw  \n",
      " \n",
      "\n",
      "#### 00:36:01,520::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "with the taxicab example the other example that we'll see when we get to [music] machine learning is you can do machine  \n",
      "learning on a small data set that fits in memory but if you're building big models or training over like a lot of  \n",
      "different hyper parameters or different types of models you can you can parallelize that using using dark so there is  \n",
      "you know you want to use dash perhaps in the big data or medium to big data limit as we see here or in the medium to big  \n",
      "model limit where training for example takes and takes a lot of Time OK so without further ado let's get started with  \n",
      "das data frames you likely ran this preparation file to get the data in the previous notebook but if you didn't execute  \n",
      "that now we're going to get our file names by doing doing a few joins and we see our file is a string data NYC flights a  \n",
      "wildcard to access all of them dot dot csv and we're going to import our Dask.dataframe and read in our dataframe  \n",
      "parsing some dates setting some sending some data types OK I'll execute that we'll see we have 10 partitions as we noted  \n",
      "before if this was a Pandas data frame we'd see a bunch of entries here we don't we see only the column names and the  \n",
      "data types of the columns and the reason is as we've said it explicitly here is the representation of the data frame  \n",
      "object contains no data it's done Dask has done enough work to read the start of the file so that we know a bit about it  \n",
      "some of the important stuff and then further column types and column names and data types OK but we don't once again we  \n",
      "don't let's say we've got 100 gigs of data we don't want to like do this call and suddenly it's reading all that stuff  \n",
      "in and doing a whole bunch of compute until we explicitly tell it to OK now this is really cool if you know a bit of  \n",
      "Pandas you'll know that you can there's an attribute columns which prints out it's well it's actually the columns form  \n",
      "an index right the Pandas index object and we get the we get the column names there cool Pandas in dark form we can  \n",
      "check out the data types as well as we would in Pandas we see we've got some ins for the day of the week we've got some  \n",
      "floats for departure Time maybe we'd actually prefer that to be you know a date Time at some point we've got some  \n",
      "objects which generally are the most general on objects so generally strings so that's all Pandasey type stuff in  \n",
      "addition das data frames have an attribute n partitions which tells us the number of partitions and we saw before that  \n",
      "that's 10 so I'd expect to see 10 here hey look at that now this is something that we talk about a lot in the delayed  \n",
      "notebook is really the task graph and I don't want to say too much about that but really it's a visual schematic of of  \n",
      "the order in which different types of compute happen OK and so the task graph for read csv tells us what happens when we  \n",
      "call compute and essentially it reads csv 10 ten Times zero indexed of course because Python it reads csv ten different  \n",
      "Times into these ten different Pandas Pandas data frames and if there were group buys or stuff after that we'd see them  \n",
      "happen in in the in the graph there and we may see an example of this in a second so once again as with Pandas we're  \n",
      "going to view the the head of the data frame great and we see a bunch of stuff you know we we see the first first five  \n",
      "rows I'm actually also gonna gonna have a look at the  \n",
      " \n",
      "\n",
      "#### 00:40:02,240::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "the tail the final five rows that may take longer because it's accessing the the final I I there's a joke and it may not  \n",
      "even be a joke how much data analytics is actually biased by people looking at the first five rows before actually you  \n",
      "know interrogating the data more more seriously so how would all of our results look different if if our files were  \n",
      "ordered in in a different way that's another conversation for a more philosophical conversation for another Time so now  \n",
      "I want to show you some computations with dark data frames OK so since dash data frames implement a Pandas like API we  \n",
      "can just write our familiar Pandas codes so I want to look at the column departure delay and look at the maximum of that  \n",
      "column I'm going to call that max delay so you can see we're selecting the column and then applying the max method as we  \n",
      "would with Pandas. Oh what happened there gives us some Dask scalar series and what's happened is we haven't called compute  \n",
      "right so it hasn't actually done the compute yet we're going to do compute but first we're going to visualize the task  \n",
      "graph like we did here and let's try to reason what the task graph would look like right so the task graph first it's  \n",
      "going to read in all of these things and then it'll probably perform this selector on each of these different Pandas  \n",
      "data frames comprising the dash data frame and then it will compute the max of each of those and then do a max on all  \n",
      "those maxes I think that's what I would assume is happening here great so that's what we're what we're doing we're  \n",
      "reading this so we read the first perform the first read csv get this das data frame get item I think is that selection  \n",
      "then we're taking the max we're doing the same for all of them then we take all of these max's and aggregate them and  \n",
      "then take the max of that OK so that that's essentially what's happening when I call compute which I'm going to do now  \n",
      "moment of truth OK so that took around eight seconds and it tells us the max and I I'm sorry let's let's just get out  \n",
      "some of our dashboards up as well huh I think in this notebook we are using the single machine scheduler Hugo so I don't  \n",
      "think there is a dashboard to be seen exactly yeah thank you for that that that catch James great is even better James  \n",
      "we have a question around using dark for reinforcement learning can you can you speak to that yeah so it depends on this  \n",
      "I mean yeah short answer yes you can use GAs to train reinforcement learning models so there's a package that Hugo will  \n",
      "talk about called Dask ML that we'll see in the next notebook for distributing machine learning that paralyzes and and  \n",
      "distributes some existing models using desks so for instance things like random forces forest inside kit learn so so yes  \n",
      "you can use das to do distributed training for models I'm not actually sure if Dask ML implements any reinforcement  \n",
      "learning models in particular but that is certainly something that that can be done yeah and I'll I'll build on that by  \n",
      "saying we are about to jump into machine  \n",
      " \n",
      "\n",
      "#### 00:44:00,000::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "learning I don't think as James said I don't think there's reinforcement learning explicitly that that one can do but  \n",
      "you of course can use the das scheduler yourself to you know to distribute any reinforcement learning stuff you you have  \n",
      "as well and that's actually another another point to make that maybe James can speak to a bit more is that the dark team  \n",
      "of course built all of these high-level collections and task arrays and dust data frames and were pleasantly surprised  \n",
      "when you know maybe even up to half the people using dust came in all like we love all that but we're going to use the  \n",
      "scheduler for our own bespoke use cases right yeah exactly yeah the original intention was to like make basically a num  \n",
      "like a parallel numpy so that was like the desk array stuff like run run numpy and parallel on your laptop and and yeah  \n",
      "so in order to do that we ended up building a distributed scheduler which sort of does arbitrary task computations so  \n",
      "not just things like you know parallel numpy but really whatever you'd like to throw at it and it turns out that ended  \n",
      "up being really useful for people and so yeah now people use that sort of on their own just using the distributed  \n",
      "scheduler to do totally custom algorithms in parallel in addition to these like nice collections like you saw Hugo  \n",
      "presents the dash data frame API is you know the same as the panda's API so there is this like familiar space you can  \n",
      "use things like the high-level collections but you can also run whatever custom like Hugo said bespoke computations you  \n",
      "might have exactly and it's it's been wonderful to see so many people so many people do that and the first thing as  \n",
      "we'll see here the first thing to think about is if if you're doing lifestyle compute if there's anything you can you  \n",
      "know parallelize embarrassingly as they say right so just if you're doing a hyper parameter search you just run some on  \n",
      "one worker and some on the other and there there's no interaction effect so you don't need to worry about that as  \n",
      "opposed to if you're trying to do you know train on streaming data where you may require it all to happen on on on the  \n",
      "same worker OK yeah so even think about trying to compute the standard deviation of a of a a univariate data set right  \n",
      "in in that case you can't just send you can't just compute the standard deviation on two workers and then combine the  \n",
      "result in some some way you need to do something slightly slightly more nuanced and slightly slightly clever more clever  \n",
      "I mean you still can actually in in that case but you can't just do it as naively as that but so now we're talking about  \n",
      "parallel and distributed machine learning we have 20 minutes left so this is kind of going to be a whirlwind tour but  \n",
      "you know whirlwinds when safe exciting and informative I just want to make clear the material in this notebook is based  \n",
      "on the open source content from Dask's tutorial repository as there's a bunch of stuff we've shown you today the reason  \n",
      "we've done that is because they did it so well so I just want to give a shout out to all the das contributors OK so what  \n",
      "we're going to do now is just break down machine learning scaling problems into two categories just review a bit of  \n",
      "scikit-learn in passing solve a machine learning problem with single Michelle single Michelle I don't know who she is  \n",
      "but single Michelle wow single machine and parallelism with scikit-learning joblib then solve an l problem with an ML  \n",
      "problem with multiple machines and parallelism using dark as well and we won't have Time to burst for the cloud I don't  \n",
      "think but you can also play play around with that OK so as I mentioned before when thinking about distributed compute a  \n",
      "lot of people do it when they have large data they don't necessarily think about the large model limit and this  \n",
      "schematic kind of speaks to that if you've got a small model that fits in ram you don't need to think about  \n",
      " \n",
      "\n",
      "#### 00:48:00,480::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "distributed compute if your data size if your data is larger than your ram so your computer's ram bound then you want to  \n",
      "start going to a distributed setting or if your model is big and CPU bound such as like large-scale hyper-parameter  \n",
      "searches or like ensemble blended models of like machine learning algorithms whatever it is and then of course we have  \n",
      "the you know big data big model limit where distributed computer desk is incredibly handy as I'm sure you could imagine  \n",
      "OK and that's really what I've what I've gone through here a bird's-eye view of the strategies we think about if it's in  \n",
      "memory in the bottom left quadrant just use scikit-learn or your favorite ML library otherwise known as scikit-learn  \n",
      "for me anyway I was going to make a note about XGBoost but I but I won't for large models you can use joblib and your  \n",
      "favorite circuit learn estimator for large data sets use our dark ML estimators so we're gonna do a whirlwind tour of  \n",
      "scikit-learn in in five minutes we're going to load in some data so we'll actually generate it we'll import scikit-  \n",
      "learn for our ML algorithm create an estimator and then check the accuracy of the model OK so once again I'm actually  \n",
      "going to clear all outputs after restarting the kernel OK so this is a utility function of scikit-learn to create some  \n",
      "data sets so I'm going to make a classification data set with four features and 10 000 samples and just have a quick  \n",
      "view of some of it so just a reminder on ML x is the samples matrix the size of x is the number of samples in terms of  \n",
      "rows number of features as columns and then a feature or an attribute is what we're trying to predict essentially OK so  \n",
      "why is the predictor variable which we're where which we're or the target variable which we're trying to predict so  \n",
      "let's have a quick view of why it's zeros and ones in in this case OK so yep that's what I've said here why are the  \n",
      "targets which are real numbers for regression tasks or integers for classification or any other discrete sets of values  \n",
      "no words about unsupervised learning at the moment we're just going to support we're going to fit a support vector  \n",
      "classifier for this example so let's just load the appropriate scikit-learn module we don't really need to discuss what  \n",
      "support vector classifiers are at the moment now this is one of the very beautiful things about the scikit-learn API in  \n",
      "terms of fitting the the model we instantiate a classifier and we want to fit it to the features with respect to the  \n",
      "target OK so the first argument is the features second argument is the target variable so we've done that now I'm not  \n",
      "going to worry about inspecting the learn features I just want to see how accurate it was OK and once we see how  \n",
      "accurate it was I'm not gonna do this but then we can make a prediction right using estimator dot predict on a new a new  \n",
      "data set so this estimator will tell us so this score will tell us the accuracy and essentially that's the proportion or  \n",
      "percentage a fraction of the results that were that the estimator got correct and we're doing this on the training data  \n",
      "set we've just trained the model on this so this is telling us the accuracy on the on the training data set OK so it's  \n",
      "90  \n",
      " \n",
      "\n",
      "#### 00:52:01,760::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "accurate on the training data set if you dive into this a bit more you'll recognize that if we we really want to know  \n",
      "the accuracy on a holdout set or a test set and it should be probably a bit lower because this is what we use to fit it  \n",
      "OK but all that having been said I expect you know if if this is all resonating with you it means we can really move on  \n",
      "to the distributed stuff in in a second but the other thing that that's important to note is that we've trained it but a  \n",
      "lot of model a lot of estimators and models have hyper parameters that affect the fit but you that we need to specify up  \n",
      "front instead of being learned during training so you know there's a c parameter here there's a are we using shrinking  \n",
      "or not so we specify those we didn't need to specify them because there are default values but here we specify them OK  \n",
      "and then we're going to look at the score now OK this is amazing we've got 50 accuracy which is the worst score possible  \n",
      "just think about this if if you've got binary classification task and you've got 40 accuracy then you just flip the  \n",
      "labels and that changes to 60 accuracy so it's amazing that we've actually hit 50 accuracy we're to be congratulated on  \n",
      "that and what I want to note here is that we have two sets of hyper parameters we've used one's created 90 actual model  \n",
      "with 90 accuracy another one one with 50 accuracy so we want to find the best hyper parameters essentially and that's  \n",
      "why hyper parameter optimization is is so important there are several ways to do hyper parameter optimization one is  \n",
      "called grid search cross validation I won't talk about cross validation it's essentially a more robust analogue of train  \n",
      "test split where you train on a subset of your data and compute the accuracy on a test on a holdout set or a test set  \n",
      "cross validation is a as I said a slightly more robust analog of this it's called grid search because we have a grid of  \n",
      "hyper parameters so we have you know in this case we have a hyper parameter c we have a hyper parameter kernel and we  \n",
      "can imagine them in a in a grid and we're performing we're checking out the score over all this gr over this entire grid  \n",
      "of hyper parameters OK so to do that I import grid search csv now I'm going to compute the estimator over over these  \n",
      "train the estimator over over this grid and as you see this is taking Time now OK and what I wanted to make clear and I  \n",
      "think should be becoming clearer now is that if we have a large hyper parameter sweep we want to do on a small data set  \n",
      "das can be useful for that OK because we can send some of the parameters to one worker some to another and they can  \n",
      "perform them in parallel so that's embarrassingly parallel because you're you're doing the same work as you would  \n",
      "otherwise but sending it to a bunch of different workers we saw that took 30 seconds which is in my realm of comfort as  \n",
      "a data scientist I'm happy to wait 30 seconds if I had to wait much longer if this grid was bigger I'd start to get  \n",
      "probably a bit frustrated but we see that it computed it for c is equal to all combinations of these essentially OK so  \n",
      "that's really all I wanted to say there and then we can see the best parameters and the best score so the best score was  \n",
      "0.098 and it was c10 and the kernel rbf a radial basis function it doesn't even Matter what that is though for the  \n",
      "purposes of this so we've got 10 minutes left we're going to we're going to make it I can feel it I have a good I have a  \n",
      "good sense  \n",
      " \n",
      "\n",
      "#### 00:56:00,400::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      " a good after the I mean this demo is actually going incredibly well given the initial technical hurdles so touchwood  \n",
      "Hugo OK so what we've done is we've really segmented ML scaling problems into two categories CPU bound and ram bound and  \n",
      "I I really I can't emphasize that enough because I see so many people like jumping in to use new cool technologies  \n",
      "without perhaps taking it being a bit mindful and intentional about it and reasoning about when things are useful and  \n",
      "and when not I suppose the one point there is that sure data science is a technical discipline but there are a lot of  \n",
      "other aspects to it involving this type of reasoning as well so we then carried out a typical sklearn workflow for ML  \n",
      "problems with small models and small data and we reviewed hyper parameters and hyper parameter optimization so in this  \n",
      "section we'll see how joblib which is a set of tools to provide lightweight pipelining in Python gives us parallelism  \n",
      "on our laptop and then we'll see how dark ML can give us awesome parallelism on on clusters OK so essentially what I'm  \n",
      "doing here is I'm doing exactly the same as above with a grid search but I'm using the quark the keyword argument n jobs  \n",
      "which tells you how many tasks to run in parallel using the cause available on your local workstation and specifying  \n",
      "minus one jobs means the it just runs them the maximum possible OK so I'm going to execute that great so we should be  \n",
      "done in a second feel free to ask any questions in the chat oh Alex has a great question in the Q&A does das have see a  \n",
      "sequel and query optimizer I'm actually so excited that [music] and James maybe you can provide a couple of links to  \n",
      "this we're really excited to have seen dark dust SQL developments there recently so that's dark hyphen hyphen SQL and  \n",
      "we're actually we're working on some some content and a blog post and maybe a live live coding session about that in in  \n",
      "the near future so if anyone if you want updates from from coil feel free to go to our website and sign up for our  \n",
      "mailing list and we'll let you know about all of this type of stuff but the short answer is yes Alex and it's getting  \n",
      "better and if James is able to post post a link there that would be that would be fantastic so we've done link in the  \n",
      "chat fantastic [music] and so we've we've seen how we have [music] single machine parallelism here using the using the  \n",
      "end jobs quark and in the final minutes let's see multiple multi-machine parallelism with Dask OK so what I'm going to  \n",
      "do is I'm going to do my imports and create my client incentive my client and check it out OK so once again I'm working  \n",
      "locally I hit search and that'll task is pretty smart in terms of like knowing which which client I want to check out do  \n",
      "the tasks stream because it's my favorite I'll do the cluster map otherwise known as the pew pew map and then I want  \n",
      "some progress we all we all crave progress don't we and maybe my workers tab OK great so we've got that up and running  \n",
      "now I'm going to do a slightly larger hyper parameter search OK so remember we had just a couple for c a couple for  \n",
      "kernel we're going to do more we have some for shrinking now I'm actually going to comment that out because I don't know  \n",
      "how long that's going to take if you're coding them on Binder now this May actually take far far too long for you but  \n",
      "we'll we'll see so I'll execute this code and we should see just sick no we shouldn't see any work happening yet but  \n",
      "what I'm doing here is oh looks like OK my clusters back up great we're doing our grid search but we're going to use  \n",
      "Dask as as the back end right and this is a context manager where we're asserting that and and we can just discuss the  \n",
      "the syntax there but it's not particularly important currently I'm going to execute this now and let's see fantastic  \n",
      "we'll see all this data transfer happening here we'll see our tasks happening here we can see these big batches of fit  \n",
      "and score fit so fitting fitting the models then finding how well they perform via this k-fold cross validation which is  \n",
      "really cool and let's just yep we can see what's happening here we can see we currently have 12 processing we've got  \n",
      "seven in memory and we have several more that we need to do our desk workers we can see us oh we can see our CPU usage  \n",
      "we can see how we can see CPU usage across all the workers which is which is pretty cool seeing that distribution is is  \n",
      "really nice whenever some form of b swarm plot if you have enough would would be useful there or even some form of  \n",
      "cumulative distribution function or something like that not a histogram people OK you can go to my Bayesian tutorial  \n",
      "that I've taught here before to hear me rave about the the horrors of histograms so we saw that talk a minute which is  \n",
      "great and we split it across you know eight cores or whatever it is and now we'll have a look once again we get the same  \n",
      "best performer which is which is a sanity check and that's pretty cool I think we have a we actually have a few minutes  \n",
      "left so I am gonna just see if I can oh let me think yeah I will see if I can burst burst to the cloud and and and do  \n",
      "this that will take a minute a minute or two to create the cluster again but while we're while we're doing that I'm  \n",
      "wondering if we have any any questions or if anyone has any feedback on on this workshop I very much welcome welcome  \n",
      "that perhaps if there are any final messages you'd you'd like to say James while we're spinning this up you can you can  \n",
      "let me know yeah sure I just also first off wanted to say thanks everyone for attending and like bearing  \n",
      " \n",
      "\n",
      "#### 01:04:01,119::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "bearing with us with the technical difficulties really appreciate that real quick I'm just yeah so if you have if you  \n",
      "have questions please post in the Q&A section while the cold cluster's spinning up Theodore posted in the last largest  \n",
      "example of grid search how much performance gain did we get from using das and not just in jobs hmm that's a great  \n",
      "question and we actually didn't see let's see so it took 80 seconds ah let me get this they're actually not comparable  \n",
      "because I did the grid search over a different set of hyper parameters I did it over a larger set of hyper parameters  \n",
      "right so when I did end jobs I did it there were only it was a two by two grid of hyper parameters whereas when I did it  \n",
      "with with Dask it was a one two three four five six six by three so let's just reason about that this one was eighteen  \n",
      "six by three is eighteen which took eighty seconds and this one was two by two so it was four and it took 26 seconds so  \n",
      "a minor gain I think with this hyper parameter search if you multiply that by by four you'll well 4.2 4.5 you'll need  \n",
      "that would have taken maybe two minutes or something something like that so we saw some increase in efficiency not a  \n",
      "great deal but James maybe you can say more to this part of the reason for that is that we're doing it on kind of a very  \n",
      "small example so we won't necessarily see the gains in efficiency with a data set this size and with a small hyper  \n",
      "parameter suite like this is that right yeah yeah and yeah exactly and I guess also this is more of an kind of an  \n",
      "illustrative point here I guess so you're just using directly using in jobs with something like joblib by default we'll  \n",
      "use local threads and processes on like whatever machine you happen to be running on so like in this case on Hugo's  \n",
      "laptop one of the real advantages of using joblib with the das back in will actually dispatch back to to run tasks on a  \n",
      "Dask cluster is that your cluster can expand beyond what local resources you have so you can run you know you can  \n",
      "basically scale out like for instance using the coil cluster to have many many CPUs and a large amount of ram that you  \n",
      "wouldn't have on your locally table to run and there you'll see both large performance gains as well as you'll be able  \n",
      "to expand your the set of possible problems you can solve to larger than ram scenarios so you're out of out of core  \n",
      "training exactly and thank you Jack this was absolutely unplanned and we didn't plan that question but that's a  \n",
      "wonderful segue into me now performing exactly the same compute with the same code using the Dask as the parallel back  \n",
      "end on a on a coiled cluster which is an AWS cluster right so we can I'm more currently anyway so I will execute this  \n",
      "code and it's exactly the same as we did whoa OK great so we see our tasks task stream here you see once again we see  \n",
      "the majority is being batch fit and and getting the scores out similarly we see the same result being the best I'll just  \n",
      "notice that for this for this small task doing it on the cloud took 20 seconds doing it locally for me took 80 seconds  \n",
      "so that's a four-fold increase in performance on a very small task so imagine what that does if you can take the same  \n",
      "code as you've written  \n",
      " \n",
      "\n",
      "#### 01:08:00,240::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "here and burst to the cloud with with one click or however however you do it I think that that's incredibly powerful and  \n",
      "that the fact that your code and what's happening in the back end with Dask generalizes immediately to the new setting  \n",
      "of working on a cluster I personally find very exciting and if you work with larger data sets or building larger models  \n",
      "or big hyper parameter sweeps I'm pretty sure it's an exciting option for all of you also so on that note I'd like to  \n",
      "reiterate James what James said and thanking you all so much for joining us for asking great questions and for bearing  \n",
      "with us through some some technical technical hurdles but it made it even even funnier when when we got up and running  \n",
      "once again I'd love to thank Mark Christina and and the rest of the organizers for doing such a wonderful job and doing  \n",
      "such a great service to the data science and machine learning community and ecosystem worldwide so thank you once again  \n",
      "for having us thank you Hugo and James I have to say like with all the technical difficulties I was actually giggling  \n",
      "because it was kind of funny yeah but we're very sorry and we thank you for your patience and sticking through it and I  \n",
      "will be editing this video to you know make it as efficient as possible and have that available Tim supercard thank you  \n",
      "great and I'll just ask you if you are interested in checking out coiled go to our website if you want to check out our  \n",
      "product go to cloud.coil.io we started building this company in February we're really excited about building a new  \n",
      "product so if you're interested reach out we'd love to chat with you about what we're doing and what we're up to and  \n",
      "it's wonderful to be in the same community as you all, so thanks!  \n",
      "   '\n"
     ]
    }
   ],
   "source": [
    "print(T)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def redo_transcript_cleanup(formatted_transcript):\n",
    "    \"\"\"\n",
    "    Return cleaned up event_dict['formatted_transcript'].\n",
    "    Wrapper to reprocess the current transcript.\n",
    "    Use case: At least one of the text-processing files\n",
    "    was amended => repeat text cleanup.\n",
    "    \"\"\"\n",
    "    if not len(formatted_transcript):\n",
    "        return ''\n",
    "\n",
    "    comment_line = ''\n",
    "    line1_idx = formatted_transcript.find('\\n')\n",
    "    if line1_idx > 0 :\n",
    "        line1 = formatted_transcript[:line1_idx]\n",
    "        comment_line = line1 if line1.startswith('<!--') else ''\n",
    "        formatted_transcript = formatted_transcript[line1_idx:]\n",
    "\n",
    "    txt_len = len(formatted_transcript)\n",
    "    chunksize = (1024 * 4)\n",
    "    n_chunks = int(np.ceil(txt_len/chunksize))\n",
    "\n",
    "    uppercase_list = TRX.readcsv(TRX.upper_file).upper.tolist()\n",
    "    titlecase_list = (TRX.readcsv(TRX.people_file).people.tolist()\n",
    "                    + TRX.readcsv(TRX.names_file).names.tolist()\n",
    "                    + TRX.readcsv(TRX.places_file).places.tolist())\n",
    "    corrections = TRX.get_corrections_dict()\n",
    "\n",
    "    md_parags = []\n",
    "    prev_e = 0\n",
    "    if n_chunks > 1:\n",
    "        for e in range(chunksize, txt_len, chunksize):\n",
    "            parag = formatted_transcript[prev_e:e]\n",
    "            new_parag = TRX.clean_text(parag,\n",
    "                                       uppercase_list,\n",
    "                                       titlecase_list,\n",
    "                                       corrections)\n",
    "            md_parags.append(new_parag)\n",
    "            prev_e = e\n",
    "    #last one (or only one):\n",
    "    parag = formatted_transcript[prev_e:]\n",
    "    new_parag = TRX.clean_text(parag,\n",
    "                               uppercase_list,\n",
    "                               titlecase_list,\n",
    "                               corrections)\n",
    "    md_parags.append(new_parag)\n",
    "    new_txt = comment_line + \"\".join(md_parags)\n",
    "    return new_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_transcript = AC.PC.page.children[1].children[1].value  # trx text\n",
    "txt_len = len(formatted_transcript)\n",
    "chunksize = (1024 * 4)\n",
    "n_chunks = TRX.math.ceil(txt_len/chunksize)\n",
    "n_chunks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "formatted_transcript = AC.PC.page.children[1].children[1].value  # trx text\n",
    "formatted_transcript[:126]\n",
    "#len('<!-- Editing Guide: The pipe (|) position in this comment is 120:                                                       | -->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "txa = AC.PC.page.children[1].children[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gui.left_sidebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names \n",
      " 'bayesian'\n"
     ]
    }
   ],
   "source": [
    "grid = AC.PC.page.children[0]\n",
    "footer_g = grid.children[3].children[0].children[0]\n",
    "v_file = footer_g.children[0].children[0].value\n",
    "v_entries = footer_g.children[0].children[1].children[1].value or None\n",
    "print(v_file, '\\n', v_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bayesian']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid, msg = CTR.validate_user_list(v_entries, v_file)\n",
    "print(msg)\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee206fb5f4a4849b277aff6053575da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black', height='160px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "class OutputWidgetHandler(logging.Handler):\n",
    "    \"\"\" Custom logging handler sending logs to an output widget \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(OutputWidgetHandler, self).__init__(*args, **kwargs)\n",
    "        layout = {\n",
    "            'width': '100%',\n",
    "            'height': '160px',\n",
    "            'border': '1px solid black'\n",
    "        }\n",
    "        self.out = ipw.Output(layout=layout)\n",
    "\n",
    "    def emit(self, record):\n",
    "        \"\"\" Overload of logging.Handler method \"\"\"\n",
    "        formatted_record = self.format(record)\n",
    "        new_output = {\n",
    "            'name': 'stdout',\n",
    "            'output_type': 'stream',\n",
    "            'text': formatted_record+'\\n'\n",
    "        }\n",
    "        self.out.outputs = (new_output, ) + self.out.outputs\n",
    "\n",
    "    def show_logs(self):\n",
    "        \"\"\" Show the logs \"\"\"\n",
    "        display(self.out)\n",
    "\n",
    "    def clear_logs(self):\n",
    "        \"\"\" Clear the current logs \"\"\"\n",
    "        self.out.clear_output()\n",
    "\n",
    "\n",
    "logger = logging.getLogger('Audit_Tests')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = OutputWidgetHandler()\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s  - [%(levelname)s] %(message)s'))\n",
    "logger.addHandler(handler)\n",
    "\n",
    "handler.show_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.clear_logs()\n",
    "logger.info('Starting program')\n",
    "\n",
    "try:\n",
    "    logger.info('About to try something dangerous...')\n",
    "    1.0/0.0\n",
    "except Exception as e:\n",
    "    logger.exception('Oops: ',exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_string.Textarea"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_box.GridBox"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_box.HBox"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_box.VBox"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_selectioncontainer.Accordion"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = AC.PC.page.children[0]  #grid header\n",
    "type(grid)\n",
    "main = grid.children[1]\n",
    "type(main)\n",
    "main_vbx1 = main.children[2]\n",
    "type(main_vbx1)\n",
    "main_out_idn = main_vbx1.children[0].outputs[0]['text'][5:-1]\n",
    "footer = grid.children[3]\n",
    "footer_acc = footer.children[0]\n",
    "type(footer_acc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "footer_acc.selected_index = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Current EDIT tasks:\n",
    "* Problem: cannot disable footer or its Accordion => footer loading done by ckick_btn_load()?\n",
    "* Problem: reprocessing the transcript after text-processing files update should not entails redoing the processing of the xml_captions.\n",
    "* FEAT: Only the MODIFY page will have the option to change the time-chunking parameter (which is in the xml file); right now: not exposed.\n",
    "\n",
    "## Solution?\n",
    "* Current: TR.redo_initial_transcript() is used\n",
    "* Needed: TR.YT.reprocess_text() => existing_transcript = initial_transcript + all changes saved by Editor => prior changes will be kept\n",
    "\n",
    "### => refactor self.xml_caption_to_text\n",
    "\n",
    "\n",
    "- Extract wrap function\n",
    "- Remove TW from clean_text\n",
    "- Add do_wrap param to clean_text\n",
    "\n",
    "### Limitation: only the remaining lowercase text will be affected since all replacements assume lowercase text.\n",
    "```\n",
    "self.YT.get_initial_transcript(replace=True)\n",
    "def reprocess_text(txt):\n",
    "    \"\"\"\n",
    "    txt could be from self.txa_editarea.value or get_transcript_text(self)\n",
    "    \"\"\"\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_box.GridBox"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_box.HBox"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_output.Output"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'05-mini-max-demo-foo.md'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = AC.PC.page.children[0]  #grid header\n",
    "type(grid)\n",
    "main = grid.children[1]\n",
    "type(main)\n",
    "main_out_idn = main.children[2]\n",
    "type(main_out_idn)\n",
    "main_out_idn.outputs[0]['text'][5:-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_form = AC.PC.page.children[1].children[0]\n",
    "input_form "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Test: Horizontal RadioButtons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39b3d57ecf54e8e8704fa260c52e3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(flex_flow='row'), options=('Audio', 'Video'), value='Audio')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lo_radio = ipw.Layout(flex_flow='row')\n",
    "av_radio2 = ipw.RadioButtons(options=['Audio','Video'], value='Audio',\n",
    "                            layout=lo_radio)\n",
    "av_radio2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "ctr_kids = len(app.center.children) # n tabs\n",
    "for k in range(ctr_kids):\n",
    "    type(app.center.children[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Fixed: problem with event numbering from df :: new event dict in Meta\n",
    "```\n",
    "    def new_event_dict(self):\n",
    "        \"\"\"\n",
    "        Create a 'starter' event dict with event id generated\n",
    "        from the readme table df.\n",
    "        \"\"\"\n",
    "        new_dict = self.get_event_dict()\n",
    "\n",
    "        # Update dict with defaults:\n",
    "        new_dict['year'] = self.year\n",
    "        new = self.df.index.argmax() + self.row_offset\n",
    "        self.idn = idn_frmt(new)\n",
    "        new_dict['idn'] = self.idn\n",
    "        new_dict['transcriber'] = '?'\n",
    "        new_dict['extra_references'] = ''\n",
    "        new_dict['has_transcript'] = False\n",
    "        new_dict['status'] = TrStatus.TODO.value\n",
    "        new_dict['notes'] = ''\n",
    "        new_dict['video_href_w'] = DEF_IMG_W #thumbnail\n",
    "        \n",
    "        v1 = self.insertion_idx(HDR_TPL.format(**new_dict))\n",
    "        new_dict['trans_idx'] = v1\n",
    "        return new_dict\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "1. Produce the program flow chart depending on user status, e.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Utils for documenting the project\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import pydot_layout\n",
    "filter_dir(nx.drawing.nx_pydot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: https://nbviewer.jupyter.org/github/xflr6/graphviz/blob/master/examples/notebook.ipynb\n",
    "\n",
    "import os\n",
    "from graphviz import Digraph, Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dir(Digraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Digraph?\n",
    "Init signature:\n",
    "Digraph(\n",
    "    name=None,\n",
    "    comment=None,\n",
    "    filename=None,\n",
    "    directory=None,\n",
    "    format=None,\n",
    "    engine=None,\n",
    "    encoding='utf-8',\n",
    "    graph_attr=None,\n",
    "    node_attr=None,\n",
    "    edge_attr=None,\n",
    "    body=None,\n",
    "    strict=False,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Digraph.render?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROGRAMFILES']\n",
    "os.environ['CONDA_PREFIX']\n",
    "#C:\\Program Files\\Graphviz 2.44.1\\bin"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "last = 'C:\\\\Users\\\\catch\\\\Anaconda3\\\\envs\\\\p37\\\\Library\\\\bin\\\\graphviz'\n",
    "len(last)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.environ[\"PATH\"] = os.environ[\"PATH\"][:-54]\n",
    "os.environ[\"PATH\"][-60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_gv_envir():\n",
    "    \"\"\" Ad-hoc fix to have Graphiz (v2.38) working on my system. \n",
    "    Note that in case the error ExecutableNotFound occurs, the path to \n",
    "    graphviz must be added to the PATH variable, e.g:\n",
    "    > \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" \n",
    "    > \"ExecutableNotFound: \n",
    "       failed to execute ['dot', '-Tsvg'], make sure the Graphviz executables are\n",
    "       on your systems' PATH\"\n",
    "    The above is not sufficient: the error occurred even though graphviz, dot and\n",
    "    neato are all on my system path.\n",
    "    Calling this function on failed `try` solved the problem. (?)\n",
    "\"\"\"\n",
    "    gviz = os.path.join(os.environ['PROGRAMFILES'], 'Graphviz 2.44.1', 'bin')\n",
    "    os.environ[\"PATH\"] += os.pathsep + gviz\n",
    "    cnd_gv = os.path.join(os.environ['CONDA_PREFIX'], 'Library', 'bin', 'python-graphviz') #'graphviz')\n",
    "    os.environ[\"PATH\"] += os.pathsep + cnd_gv\n",
    "    return gviz, cnd_gv\n",
    "\n",
    "set_gv_envir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test:\n",
    "gvfile = DIR_IMG.joinpath('tbl.gv')\n",
    "\n",
    "dot_dg = Digraph(comment='The Round Table', filename=gvfile, engine='dot')\n",
    "\n",
    "dot_dg.node('A', 'King Arthur')\n",
    "dot_dg.node('B', 'Sir Bedevere the Wise')\n",
    "dot_dg.node('L', 'Sir Lancelot the Brave')\n",
    "\n",
    "dot_dg.edges(['AB', 'AL'])\n",
    "dot_dg.edge('B', 'L', constraint='false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_dg.render(format='png', view=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".\n",
    "```\n",
    "# in utils_docs.py:\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_node(\"ROOT\")\n",
    "\n",
    "for i in range(5):\n",
    "    G.add_node(\"Child_%i\" % i)\n",
    "    G.add_node(\"Grandchild_%i\" % i)\n",
    "    G.add_node(\"Greatgrandchild_%i\" % i)\n",
    "\n",
    "    G.add_edge(\"ROOT\", \"Child_%i\" % i)\n",
    "    G.add_edge(\"Child_%i\" % i, \"Grandchild_%i\" % i)\n",
    "    G.add_edge(\"Grandchild_%i\" % i, \"Greatgrandchild_%i\" % i)\n",
    "\n",
    "# write dot file to use with graphviz\n",
    "# run \"dot -Tpng test.dot >test.png\"\n",
    "nx.nx_agraph.write_dot(G,'test.dot')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# same layout using matplotlib with no labels\n",
    "plt.title('draw_networkx')\n",
    "pos=graphviz_layout(G, prog='dot')\n",
    "nx.draw(G, pos, with_labels=False, arrows=False)\n",
    "plt.savefig('nx_test.png')\n",
    "#..............\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "g=nx.DiGraph()\n",
    "g.add_edges_from([(1,2), (1,3), (1,4), (2,5), (2,6), (2,7), (3,8), (3,9),\n",
    "                  (4,10), (5,11), (5,12), (6,13)])\n",
    "p=nx.drawing.nx_pydot.to_pydot(g)\n",
    "p.write_png('example.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = {'User Type:':['Admin', 'Tanscriber'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test:\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_node(\"ROOT\")\n",
    "\n",
    "for i in range(5):\n",
    "    G.add_node(\"Child_%i\" % i)\n",
    "    G.add_node(\"Grandchild_%i\" % i)\n",
    "    G.add_node(\"Greatgrandchild_%i\" % i)\n",
    "\n",
    "    G.add_edge(\"ROOT\", \"Child_%i\" % i)\n",
    "    G.add_edge(\"Child_%i\" % i, \"Grandchild_%i\" % i)\n",
    "    G.add_edge(\"Grandchild_%i\" % i, \"Greatgrandchild_%i\" % i)\n",
    "\n",
    "pos = nx.drawing.nx_pydot.graphviz_layout(G)\n",
    "nx.draw(G, pos=pos)\n",
    "\n",
    "# write dot file to use with graphviz\n",
    "# run \"dot -Tpng test.dot >test.png\"\n",
    "\n",
    "dotfile = DIR_DATA.joinpath('test.dot')\n",
    "nx.nx_pydot.write_dot(G, dotfile)\n",
    "\n",
    "p = nx.drawing.nx_pydot.to_pydot(G)\n",
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
