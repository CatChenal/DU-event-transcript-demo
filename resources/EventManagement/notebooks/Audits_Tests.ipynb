{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Run the first cell! (collapsed in JupyterLab)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path added to sys.path: C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/resources/EventManagement\n",
      "\n",
      "Python ver: 3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]\n",
      "Python env: du37\n",
      "OS:         win32\n",
      "Current dir: C:\\Users\\catch\\Documents\\GitHub\\DU-event-transcript-demo\\resources\\EventManagement\\notebooks\n",
      "\n",
      "Last updated: 2021-10-14T15:36:17.268559-04:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.7.11\n",
      "IPython version      : 7.27.0\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "sys   : 3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]\n",
      "numpy : 1.20.3\n",
      "pandas: 1.3.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To get multiple outputs from one code cell (without using print()):\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython import get_ipython\n",
    "from IPython.display import HTML, Markdown, Image\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# For documenting the current environment:\n",
    "def sys_info():\n",
    "    frmt = '\\nPython ver: {}\\nPython env: {}\\n'\n",
    "    frmt += 'OS:         {}\\nCurrent dir: {}\\n'\n",
    "    print(frmt.format(sys.version, \n",
    "                      Path(sys.prefix).name,\n",
    "                      sys.platform,\n",
    "                      Path.cwd()))\n",
    "\n",
    "# For enabling imports from current project code:\n",
    "def add_to_sys_path(this_path, up=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Prepend this_path to sys.path.\n",
    "    If up=True, path refers to parent folder (1 level up).\n",
    "    \"\"\"\n",
    "    newp = Path(this_path).as_posix() # no str method (?)\n",
    "    if up:\n",
    "        newp = Path(this_path).parent.as_posix()\n",
    "\n",
    "    msg = F'Path already in sys.path: {newp}'\n",
    "    if newp not in sys.path:\n",
    "        sys.path.insert(1, newp)\n",
    "        msg = F'Path added to sys.path: {newp}'\n",
    "    if verbose:\n",
    "        print(msg)\n",
    "\n",
    "# If this ipynb file is inside a folder, eg ./notebooks, \n",
    "# the project code is assumed to reside 1 level up:\n",
    "nb_folder = 'notebooks'\n",
    "add_to_sys_path(Path.cwd(), up=Path.cwd().name.startswith(nb_folder))\n",
    "\n",
    "\n",
    "# For py modules/methods discovery:\n",
    "def filter_dir(mdl, filter_str=None, start_with_str='_', exclude=True):\n",
    "    \"\"\"Filter dir(mdl) for method discovery.\n",
    "       Input:\n",
    "       :param mdl (object): module, optionally with submodule path(s), e.g. mdl.submdl1.submdl2.\n",
    "       :param filter_str (str, None): filter all method names containing that string.\n",
    "       :param start_with_str (str, '_'), exclude (bool, True): start_with_str and exclude work \n",
    "              together to perform search on non-dunder methods (default).\n",
    "       Example:\n",
    "       >filter_dir(re) # lists the public methods of the re module.\n",
    "    \"\"\"\n",
    "    search_dir = [d for d in dir(mdl) if not d.startswith(start_with_str) == exclude]\n",
    "    if filter_str is None:\n",
    "        return search_dir\n",
    "    else:\n",
    "        filter_str = filter_str.lower()\n",
    "        return [d for d in search_dir if d.lower().find(filter_str) != -1]\n",
    "\n",
    "# To create often-used subfolders:\n",
    "def get_project_dirs(which=['data', 'images'],\n",
    "                     use_parent=True):\n",
    "    '''Create folder(s) named in `which` at the ipynb parent level.'''\n",
    "    if use_parent:\n",
    "        dir_fn = Path.cwd().parent.joinpath\n",
    "    else:\n",
    "        dir_fn = Path.cwd().joinpath\n",
    "        \n",
    "    dir_lst = []    \n",
    "    for d in which:\n",
    "        DIR = dir_fn(d)\n",
    "        if not DIR.exists():\n",
    "            Path.mkdir(DIR)\n",
    "        dir_lst.append(DIR)\n",
    "    return dir_lst\n",
    "\n",
    "#DIR_DATA, DIR_IMG = get_project_dirs()\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.set_option(\"display.max_colwidth\", 200)\n",
    "from pprint import pprint as pp\n",
    "\n",
    "\n",
    "# For documenting the current environment:\n",
    "def show_versions():\n",
    "    txt = '<pre><br>'\n",
    "    txt += F'Python:\\t\\t{sys.version}<br>'\n",
    "    txt += F'Python env:\\t{Path(sys.prefix).name}<br>'\n",
    "    txt += F'Numpy:\\t\\t{np.__version__}<br>'\n",
    "    txt += F'Scipy:\\t\\t{sp.__version__}<br>'\n",
    "    txt += F'Pandas:\\t\\t{pd.__version__}<br>'\n",
    "    txt += F'Matplotlib:\\t{mpl.__version__}<br>'\n",
    "    txt += F'Currrent dir: {Path.cwd()}'\n",
    "    txt += '</pre>'\n",
    "    div = f\"\"\"<div class=\"alert alert-info\"><b>Versions:</b><br>{txt}</div>\"\"\"\n",
    "    return HTML(div)\n",
    "\n",
    "\n",
    "# autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "#..................\n",
    "sys_info()\n",
    "\n",
    "no_wmark = False\n",
    "try:\n",
    "    %load_ext watermark\n",
    "    %watermark\n",
    "except ModuleNotFoundError:\n",
    "    no_wmark = True\n",
    "\n",
    "if no_wmark:\n",
    "    show_versions()\n",
    "else:\n",
    "    %watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manage import (EventMeta as Meta,\n",
    "                    EventTranscription as TRX,\n",
    "                    Controls as CTR,\n",
    "                    Utils as UTL,\n",
    "                    Audit as AUD)\n",
    "\n",
    "from collections import OrderedDict, Counter\n",
    "import ipywidgets as ipw\n",
    "import re"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "slider_title = ipw.HTML('<em>Vertical Box Example</em>')\n",
    "slider = ipw.IntSlider()\n",
    "ipw.VBox([slider_title, slider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manage.tests import test_EventMeta as testMeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add dummy event:\n",
      " - video url: https://www.youtube.com/watch?v=MHAjCcBfT_A\n",
      " - meetup url: https://www.meetup.com/data-umbrella/events/274778387/\n",
      ".update_dict - Assigning new dummy event dict to Meta object...\n",
      "Updated dict:\n",
      "OrderedDict([('presenter', 'Cat Chenal, Reshama Shaikh'), ('title', 'Automating Audio Transcription.'), ('event_url', 'https://www.meetup.com/data-umbrella/events/274778387/'), ('yt_video_id', 'MHAjCcBfT_A'), ('slides_url', 'N.A.'), ('repo_url', 'https://github.com/CatChenal'), ('notebook_url', 'N.A.'), ('transcriber', '?'), ('extra_references', '## Other References\\n- Binder:  <url>\\n- Paper:  <Paper url or citation>  \\n- Wiki:  This is an excellent [wiki on audio foo](http://en.wikipedia.org/wiki/Main_Page)  \\n'), ('video_href', 'http://www.youtube.com/watch?feature=player_embedded&v=MHAjCcBfT_A'), ('video_href_src', 'http://img.youtube.com/vi/MHAjCcBfT_A/0.jpg'), ('video_href_alt', 'Automating Audio Transcription.'), ('video_href_w', '25%'), ('formatted_transcript', 'N.A.'), ('year', 2021), ('idn', '02'), ('video_url', 'https://www.youtube.com/watch?v=MHAjCcBfT_A'), ('title_kw', 'audio-foo'), ('transcript_md', '02-cat-reshama-audio-foo.md'), ('audio_track', WindowsPath('C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/resources/EventManagement/data/meta/2021_02_MHAjCcBfT_A.mp4')), ('audio_text', WindowsPath('C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/resources/EventManagement/data/meta/2021_02_MHAjCcBfT_A.txt')), ('has_transcript', False), ('trans_idx', 730), ('status', 'Not yet processed (editor needed)'), ('notes', 'Dummy entry for demo.'), ('video_embed', '\\n<iframe width=\"560\" height=\"315\" \\n        src=\"https://www.youtube-nocookie.com/embed/MHAjCcBfT_A?cc_load_policy=1&autoplay=0\" \\n        frameborder=\"0\">\\n</iframe>\\n')])\n",
      ".update_readme() - Saving Readme file...\n",
      ".save_transcript_md - Saving Transcript Markdown file...\n"
     ]
    }
   ],
   "source": [
    "dum = testMeta.test_add_event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Speakers: Cat Chenal and  Reshama Shaikh \n",
       "- Transcript:  https://github.com/data-umbrella/event-transcripts/blob/main/02-cat-reshama-audio-foo.md \n",
       "- Meetup Event:  https://www.meetup.com/data-umbrella/events/274778387/ \n",
       "- Video:  https://www.youtube.com/watch?v=MHAjCcBfT_A \n",
       "## Other References\n",
       "- Binder:  <url>\n",
       "- Paper:  <Paper url or citation>  \n",
       "- Wiki:  This is an excellent [wiki on audio foo](http://en.wikipedia.org/wiki/Main_Page)  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Speakers: Cat Chenal and  Reshama Shaikh \n",
      "- Transcript:  https://github.com/data-umbrella/event-transcripts/blob/main/02-cat-reshama-audio-foo.md \n",
      "- Meetup Event:  https://www.meetup.com/data-umbrella/events/274778387/ \n",
      "- Video:  https://www.youtube.com/watch?v=MHAjCcBfT_A \n",
      "## Other References\n",
      "- Binder:  <url>\n",
      "- Paper:  <Paper url or citation>  \n",
      "- Wiki:  This is an excellent [wiki on audio foo](http://en.wikipedia.org/wiki/Main_Page)  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Automated video description:\n",
    "\n",
    "Markdown(dum.get_video_desc())\n",
    "# OR:\n",
    "print(dum.get_video_desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Test: Using consecutinous event numbering instead of numbering per years (current): What has to change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['presenter', 'title', 'event_url', 'yt_video_id', 'slides_url', 'repo_url', 'notebook_url', 'transcriber', 'extra_references', 'video_href', 'video_href_src', 'video_href_alt', 'video_href_w', 'formatted_transcript', 'year', 'idn', 'video_url', 'title_kw', 'transcript_md', 'audio_track', 'audio_text', 'has_transcript', 'trans_idx', 'status', 'notes', 'video_embed'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Existing event; renumbered as per consecutinous numbering scheme\n",
    "first_yr_21 = Meta.TranscriptMeta(idn=1, year=2021)\n",
    "\n",
    "\n",
    "first_yr_21.event_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trx len:  84380\n"
     ]
    }
   ],
   "source": [
    "if first_yr_21.event_dict['has_transcript']:\n",
    "    print('Trx len: ', len(first_yr_21.event_dict['formatted_transcript']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EventMeta.TranscriptMeta(idn=01, year=2021)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#__repr__\n",
    "first_yr_21\n",
    "#__str__\n",
    "print(first_yr_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Local README file: README.md\n",
       "---\n",
       "---\n",
       "<p >\n",
       " <a href=\"https://www.dataumbrella.org\" target=\"_blank\"> <img src=\"images/full_logo_transparent.png\" height=\"30%\" width=\"30%\" /> </a>\n",
       "</p>\n",
       "\n",
       "# Event Transcripts (Demo README)\n",
       "\n",
       "## [Videos](https://www.youtube.com/c/DataUmbrella/videos)\n",
       "Subscribe to our YouTube [Data Umbrella channel](https://www.youtube.com/c/DataUmbrella/videos).\n",
       "\n",
       "## [Contributing Guide](CONTRIBUTING.md)\n",
       "Review our [Contributing Instructions](CONTRIBUTING.md) before beginning editing / transcribing work.  \n",
       "\n",
       "<!-- main_tbl_start -->\n",
       "| #  | Speaker             | Talk Transcript  | Transcriber  | Status | Notes |\n",
       "|--- |---                  |---               |---           |---     |---    | \n",
       "| 01| Hugo Bowne-Anderson| Bayesian Data Science| N.A.| Not recorded| | \n",
       "| 02| Bruno Goncalves| Time Series Modeling| N.A.| Not recorded| | \n",
       "| 03| Ty Shaikh| [Webscraping Poshmark](2020/03-ty-shaikh-webscraping.md)| Ty Shaikh| Needs reviewer| | \n",
       "| 04| Ali Spittel| [Navigating Your Tech Career](2020/04-ali-spittel-career.md)| Janine| Needs reviewer| | \n",
       "| 05| Andreas Mueller| [Crash Course in Contributing to Scikit-learn](2020/05-andreas-mueller-contributing.md)| Reshama Shaikh| Complete| | \n",
       "| 06| Reshama Shaikh| [Example PR for Scikit-learn](2020/06-reshama-shaikh-sklearn-pr.md)| Reshama Shaikh, Mark| Complete| | \n",
       "| 07| Shailvi Wakhlu| [Fixing Bad Data (Using SQL)](2020/07-shailvi-wakhlu-fixing-data.md)| Juanita| Complete| | \n",
       "| 08| Matt Brems| [Data Science with Missing Data](2020/08-matt-brems-missing-data.md)| Barbara Graniello Batlle| Needs reviewer| | \n",
       "| 09| Sam Bail| [Intro to Terminal](2020/09-sam-bail-terminal.md)| Isaack| Complete| | \n",
       "| 10| Emily Robinson| [Build a Career in Data Science](2020/10-emily-robinson-career.md)| Kevin| Complete| | \n",
       "| 11| Rebecca Kelly| [Kdb Time Series Database](2020/11-rebecca-kelly-kdb.md)| Coretta| Needs reviewer| Paragraphs are too long| \n",
       "| 12| Mridu Bhatnagar| [Build a Bot](2020/12-mridu-bhatnagar-bot.md)| ?| Not yet processed (editor needed)| | \n",
       "| 13| Liz DiLuzio| [Creating Nimble Data Processes](2020/13-liz-diluzio-data-process.md)| Lily| Complete| | \n",
       "| 14| Megan Robertson| [3 Lessons From 3 Years of Data Science](2020/14-megan-robertson-career.md)| Sethupathy| Needs reviewer| Headers should not be in capital letters, etc| \n",
       "| 15| Emma Gouillart| [Data Visualization with Plotly](2020/15-emma-gouillart-plotly.md)| ?| Not yet processed (editor needed)| | \n",
       "| 16| Hugo Bowne-Anderson, James Bourbeau| [Data Science and Machine Learning at Scale](2020/16-hugo-james-dask.md)| Cynthia| Needs reviewer| | \n",
       "| 17| Carol Willing| [Contributing to Core Python](2020/17-carol-willing-python.md)| ?| Not yet processed (editor needed)| | \n",
       "| 18| Thomas Fan| [Streamlit for Data Science](2020/18-thomas-fan-streamlit.md)| ?| Not yet processed (editor needed)| | \n",
       "| 19| Matti Picus| [Contributing to NumPy](2020/19-matti-picus-numpy.md)| ?| Not yet processed (editor needed)| | \n",
       "| 20| Marco Gorelli| [Contributing to pandas](2020/20-marco-gorelli-pandas.md)| ?| Not yet processed (editor needed)| | \n",
       "| 21| Cat Chenal| [Automating Audio Tanscription.](2020/21-cat-chenal-foo-demo.md)| Billy Bop| Not yet processed (editor needed)| | \n",
       "| 01| Nick Janetakis| [Creating a Command Line Focused Development Environment](2021/01-nick-janetakis-command.md)| ?| Partial (new editor requested)| | \n",
       "| 02| Cat Chenal, Reshama Shaikh| [Automating Audio Transcription.](2021/02-cat-reshama-audio-foo.md)| ?| Not yet processed (editor needed)| Dummy entry for demo.| \n",
       "<!-- main_tbl_end -->\n",
       "<!-- Note: There should not be any empty table row before the end of table marker above.-->\n",
       "\n",
       "## NEW!\n",
       "### This is the README file of the event management demo (or dev) project for the Data Umbrella transcript repo. Further details about the implementation and its benefits, i.e. the 'sales points' and specifications of the project, can be found in the [Dev Project README](./resources/EventManagement/dev_only_docs/README.md) and associated files in the `/EventManagement/dev_only_docs` folder. \n",
       "\n",
       "\n",
       "## Try this demo!\n",
       "- Check the installation instructions in [DEMO.md](./DEMO.md)\n",
       "\n",
       "\n",
       "### [Project README](./resources/EventManagement/README.md)\n",
       "\n",
       "---\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mdfile = Meta.REPO_PATH.joinpath(Meta.MAIN_README)\n",
    "UTL.show_md_file(mdfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Talk Transcript</th>\n",
       "      <th>Transcriber</th>\n",
       "      <th>Status</th>\n",
       "      <th>Notes</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>Hugo Bowne-Anderson</td>\n",
       "      <td>Bayesian Data Science</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>Not recorded</td>\n",
       "      <td></td>\n",
       "      <td>N.A.</td>\n",
       "      <td>N.A.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02</td>\n",
       "      <td>Bruno Goncalves</td>\n",
       "      <td>Time Series Modeling</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>Not recorded</td>\n",
       "      <td></td>\n",
       "      <td>N.A.</td>\n",
       "      <td>N.A.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03</td>\n",
       "      <td>Ty Shaikh</td>\n",
       "      <td>[Webscraping Poshmark](2020/03-ty-shaikh-websc...</td>\n",
       "      <td>Ty Shaikh</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>03-ty-shaikh-webscraping.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04</td>\n",
       "      <td>Ali Spittel</td>\n",
       "      <td>[Navigating Your Tech Career](2020/04-ali-spit...</td>\n",
       "      <td>Janine</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>04-ali-spittel-career.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05</td>\n",
       "      <td>Andreas Mueller</td>\n",
       "      <td>[Crash Course in Contributing to Scikit-learn]...</td>\n",
       "      <td>Reshama Shaikh</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>05-andreas-mueller-contributing.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>06</td>\n",
       "      <td>Reshama Shaikh</td>\n",
       "      <td>[Example PR for Scikit-learn](2020/06-reshama-...</td>\n",
       "      <td>Reshama Shaikh, Mark</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>06-reshama-shaikh-sklearn-pr.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>07</td>\n",
       "      <td>Shailvi Wakhlu</td>\n",
       "      <td>[Fixing Bad Data (Using SQL)](2020/07-shailvi-...</td>\n",
       "      <td>Juanita</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>07-shailvi-wakhlu-fixing-data.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08</td>\n",
       "      <td>Matt Brems</td>\n",
       "      <td>[Data Science with Missing Data](2020/08-matt-...</td>\n",
       "      <td>Barbara Graniello Batlle</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>08-matt-brems-missing-data.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>09</td>\n",
       "      <td>Sam Bail</td>\n",
       "      <td>[Intro to Terminal](2020/09-sam-bail-terminal.md)</td>\n",
       "      <td>Isaack</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>09-sam-bail-terminal.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Emily Robinson</td>\n",
       "      <td>[Build a Career in Data Science](2020/10-emily...</td>\n",
       "      <td>Kevin</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>10-emily-robinson-career.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Rebecca Kelly</td>\n",
       "      <td>[Kdb Time Series Database](2020/11-rebecca-kel...</td>\n",
       "      <td>Coretta</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td>Paragraphs are too long</td>\n",
       "      <td>2020</td>\n",
       "      <td>11-rebecca-kelly-kdb.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Mridu Bhatnagar</td>\n",
       "      <td>[Build a Bot](2020/12-mridu-bhatnagar-bot.md)</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>12-mridu-bhatnagar-bot.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Liz DiLuzio</td>\n",
       "      <td>[Creating Nimble Data Processes](2020/13-liz-d...</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>13-liz-diluzio-data-process.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Megan Robertson</td>\n",
       "      <td>[3 Lessons From 3 Years of Data Science](2020/...</td>\n",
       "      <td>Sethupathy</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td>Headers should not be in capital letters, etc</td>\n",
       "      <td>2020</td>\n",
       "      <td>14-megan-robertson-career.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Emma Gouillart</td>\n",
       "      <td>[Data Visualization with Plotly](2020/15-emma-...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>15-emma-gouillart-plotly.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Hugo Bowne-Anderson, James Bourbeau</td>\n",
       "      <td>[Data Science and Machine Learning at Scale](2...</td>\n",
       "      <td>Cynthia</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>16-hugo-james-dask.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Carol Willing</td>\n",
       "      <td>[Contributing to Core Python](2020/17-carol-wi...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>17-carol-willing-python.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Thomas Fan</td>\n",
       "      <td>[Streamlit for Data Science](2020/18-thomas-fa...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>18-thomas-fan-streamlit.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Matti Picus</td>\n",
       "      <td>[Contributing to NumPy](2020/19-matti-picus-nu...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>19-matti-picus-numpy.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Marco Gorelli</td>\n",
       "      <td>[Contributing to pandas](2020/20-marco-gorelli...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>20-marco-gorelli-pandas.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Cat Chenal</td>\n",
       "      <td>[Automating Audio Tanscription.](2020/21-cat-c...</td>\n",
       "      <td>Billy Bop</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>21-cat-chenal-foo-demo.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>01</td>\n",
       "      <td>Nick Janetakis</td>\n",
       "      <td>[Creating a Command Line Focused Development E...</td>\n",
       "      <td>?</td>\n",
       "      <td>Partial (new editor requested)</td>\n",
       "      <td></td>\n",
       "      <td>2021</td>\n",
       "      <td>01-nick-janetakis-command.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>02</td>\n",
       "      <td>Cat Chenal, Reshama Shaikh</td>\n",
       "      <td>[Automating Audio Transcription.](2021/02-cat-...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td>Dummy entry for demo.</td>\n",
       "      <td>2021</td>\n",
       "      <td>02-cat-reshama-audio-foo.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     N                              Speaker  \\\n",
       "0   01                  Hugo Bowne-Anderson   \n",
       "1   02                      Bruno Goncalves   \n",
       "2   03                            Ty Shaikh   \n",
       "3   04                          Ali Spittel   \n",
       "4   05                      Andreas Mueller   \n",
       "5   06                       Reshama Shaikh   \n",
       "6   07                       Shailvi Wakhlu   \n",
       "7   08                           Matt Brems   \n",
       "8   09                             Sam Bail   \n",
       "9   10                       Emily Robinson   \n",
       "10  11                        Rebecca Kelly   \n",
       "11  12                      Mridu Bhatnagar   \n",
       "12  13                          Liz DiLuzio   \n",
       "13  14                      Megan Robertson   \n",
       "14  15                       Emma Gouillart   \n",
       "15  16  Hugo Bowne-Anderson, James Bourbeau   \n",
       "16  17                        Carol Willing   \n",
       "17  18                           Thomas Fan   \n",
       "18  19                          Matti Picus   \n",
       "19  20                        Marco Gorelli   \n",
       "20  21                           Cat Chenal   \n",
       "21  01                       Nick Janetakis   \n",
       "22  02           Cat Chenal, Reshama Shaikh   \n",
       "\n",
       "                                      Talk Transcript  \\\n",
       "0                               Bayesian Data Science   \n",
       "1                                Time Series Modeling   \n",
       "2   [Webscraping Poshmark](2020/03-ty-shaikh-websc...   \n",
       "3   [Navigating Your Tech Career](2020/04-ali-spit...   \n",
       "4   [Crash Course in Contributing to Scikit-learn]...   \n",
       "5   [Example PR for Scikit-learn](2020/06-reshama-...   \n",
       "6   [Fixing Bad Data (Using SQL)](2020/07-shailvi-...   \n",
       "7   [Data Science with Missing Data](2020/08-matt-...   \n",
       "8   [Intro to Terminal](2020/09-sam-bail-terminal.md)   \n",
       "9   [Build a Career in Data Science](2020/10-emily...   \n",
       "10  [Kdb Time Series Database](2020/11-rebecca-kel...   \n",
       "11      [Build a Bot](2020/12-mridu-bhatnagar-bot.md)   \n",
       "12  [Creating Nimble Data Processes](2020/13-liz-d...   \n",
       "13  [3 Lessons From 3 Years of Data Science](2020/...   \n",
       "14  [Data Visualization with Plotly](2020/15-emma-...   \n",
       "15  [Data Science and Machine Learning at Scale](2...   \n",
       "16  [Contributing to Core Python](2020/17-carol-wi...   \n",
       "17  [Streamlit for Data Science](2020/18-thomas-fa...   \n",
       "18  [Contributing to NumPy](2020/19-matti-picus-nu...   \n",
       "19  [Contributing to pandas](2020/20-marco-gorelli...   \n",
       "20  [Automating Audio Tanscription.](2020/21-cat-c...   \n",
       "21  [Creating a Command Line Focused Development E...   \n",
       "22  [Automating Audio Transcription.](2021/02-cat-...   \n",
       "\n",
       "                 Transcriber                             Status  \\\n",
       "0                       N.A.                       Not recorded   \n",
       "1                       N.A.                       Not recorded   \n",
       "2                  Ty Shaikh                     Needs reviewer   \n",
       "3                     Janine                     Needs reviewer   \n",
       "4             Reshama Shaikh                           Complete   \n",
       "5       Reshama Shaikh, Mark                           Complete   \n",
       "6                    Juanita                           Complete   \n",
       "7   Barbara Graniello Batlle                     Needs reviewer   \n",
       "8                     Isaack                           Complete   \n",
       "9                      Kevin                           Complete   \n",
       "10                   Coretta                     Needs reviewer   \n",
       "11                         ?  Not yet processed (editor needed)   \n",
       "12                      Lily                           Complete   \n",
       "13                Sethupathy                     Needs reviewer   \n",
       "14                         ?  Not yet processed (editor needed)   \n",
       "15                   Cynthia                     Needs reviewer   \n",
       "16                         ?  Not yet processed (editor needed)   \n",
       "17                         ?  Not yet processed (editor needed)   \n",
       "18                         ?  Not yet processed (editor needed)   \n",
       "19                         ?  Not yet processed (editor needed)   \n",
       "20                 Billy Bop  Not yet processed (editor needed)   \n",
       "21                         ?     Partial (new editor requested)   \n",
       "22                         ?  Not yet processed (editor needed)   \n",
       "\n",
       "                                            Notes  year  \\\n",
       "0                                                  N.A.   \n",
       "1                                                  N.A.   \n",
       "2                                                  2020   \n",
       "3                                                  2020   \n",
       "4                                                  2020   \n",
       "5                                                  2020   \n",
       "6                                                  2020   \n",
       "7                                                  2020   \n",
       "8                                                  2020   \n",
       "9                                                  2020   \n",
       "10                        Paragraphs are too long  2020   \n",
       "11                                                 2020   \n",
       "12                                                 2020   \n",
       "13  Headers should not be in capital letters, etc  2020   \n",
       "14                                                 2020   \n",
       "15                                                 2020   \n",
       "16                                                 2020   \n",
       "17                                                 2020   \n",
       "18                                                 2020   \n",
       "19                                                 2020   \n",
       "20                                                 2020   \n",
       "21                                                 2021   \n",
       "22                          Dummy entry for demo.  2021   \n",
       "\n",
       "                                  name  \n",
       "0                                 N.A.  \n",
       "1                                 N.A.  \n",
       "2          03-ty-shaikh-webscraping.md  \n",
       "3             04-ali-spittel-career.md  \n",
       "4   05-andreas-mueller-contributing.md  \n",
       "5      06-reshama-shaikh-sklearn-pr.md  \n",
       "6     07-shailvi-wakhlu-fixing-data.md  \n",
       "7        08-matt-brems-missing-data.md  \n",
       "8              09-sam-bail-terminal.md  \n",
       "9          10-emily-robinson-career.md  \n",
       "10             11-rebecca-kelly-kdb.md  \n",
       "11           12-mridu-bhatnagar-bot.md  \n",
       "12      13-liz-diluzio-data-process.md  \n",
       "13        14-megan-robertson-career.md  \n",
       "14         15-emma-gouillart-plotly.md  \n",
       "15               16-hugo-james-dask.md  \n",
       "16          17-carol-willing-python.md  \n",
       "17          18-thomas-fan-streamlit.md  \n",
       "18             19-matti-picus-numpy.md  \n",
       "19          20-marco-gorelli-pandas.md  \n",
       "20           21-cat-chenal-foo-demo.md  \n",
       "21        01-nick-janetakis-command.md  \n",
       "22         02-cat-reshama-audio-foo.md  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_yr_21.df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filter_dir(first_yr_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('presenter', 'Nick Janetakis'),\n",
      "             ('title',\n",
      "              'Creating a Command Line Focused Development Environment'),\n",
      "             ('event_url',\n",
      "              'https://www.meetup.com/data-umbrella/events/274778387/'),\n",
      "             ('yt_video_id', 'y4fYxmE0HZM'),\n",
      "             ('slides_url',\n",
      "              'https://github.com/nickjj/nyhackr-cli-dev-env/blob/master/nyhackr-cli-dev-env.pdf'),\n",
      "             ('repo_url', 'https://github.com/nickjj/nyhackr-cli-dev-env'),\n",
      "             ('notebook_url', 'N.A.'),\n",
      "             ('transcriber', '?'),\n",
      "             ('extra_references',\n",
      "              '## Timestamps\\n'\n",
      "              '- NOTE TO EDITOR:  these timestamps should be incorporated into '\n",
      "              'the transcript as paragraph headers. (They have been '\n",
      "              'reformatted into list items for proper parsing.) \\n'\n",
      "              '- Intro:  0:00 \\n'\n",
      "              \"- What we're going to cover:  7:14; Talk starts \\n\"\n",
      "              '- A few practical demos of using tmux, vim and other tools:  '\n",
      "              '9:44 \\n'\n",
      "              '- Parsing CSV sales data on the command line using a few Unix '\n",
      "              'tools:  16:48 \\n'\n",
      "              '- Picking a terminal on all major operating systems:  23:44 \\n'\n",
      "              '- Comparing shells (sh vs bash vs zsh vs fish):  25:55 \\n'\n",
      "              '- Configuring your Bash profile:  29:22 \\n'\n",
      "              '- Configuring and accessing your Bash history:  32:29 \\n'\n",
      "              '- Creating and using Bash aliases:  33:58 \\n'\n",
      "              '- Configuring your prompt (including ANSI escape codes and git '\n",
      "              'branch):  36:48 \\n'\n",
      "              '- Efficiently searching your Bash history with CTRL + r and '\n",
      "              'FZF:  43:25 \\n'\n",
      "              '- Setting a Shebang and striving to be POSIX compliant:  '\n",
      "              '47:39 \\n'\n",
      "              '- Using and configuring tmux:  49:42 \\n'\n",
      "              '- Rapid fire descriptions of a few useful Vim plugins that I '\n",
      "              'use:  1:03:32 \\n'\n",
      "              '- Going over what dotfiles are and how to install + configure '\n",
      "              'everything:  1:08:00 \\n'\n",
      "              '- Main takeaway of the talk:  1:13:35 \\n'\n",
      "              '- Questions and answers:  1:14:46 \\n'),\n",
      "             ('video_href',\n",
      "              'http://www.youtube.com/watch?feature=player_embedded&v=y4fYxmE0HZM'),\n",
      "             ('video_href_src', 'http://img.youtube.com/vi/y4fYxmE0HZM/0.jpg'),\n",
      "             ('video_href_alt',\n",
      "              'Nick Janetakis: Command Line Focused Development Environment'),\n",
      "             ('video_href_w', '25%'),\n",
      "             ('formatted_transcript',\n",
      "              '\\n'\n",
      "              '<!-- Editing Guide: The pipe (|) position in this comment is '\n",
      "              '120:                                                       | '\n",
      "              '-->\\n'\n",
      "              '### Introduction\\n'\n",
      "              '\\n'\n",
      "              'Hello everyone and welcome to our Data Umbrella webinar this is '\n",
      "              \"our first webinar of 2021. I'm going to do a quick  \\n\"\n",
      "              \"introduction and then Nick will do his talk and we'll open it \"\n",
      "              'up for Q&A you can also ask questions in the question tab  \\n'\n",
      "              \"throughout the presentation and we'll sort of break whenever \"\n",
      "              \"it's a good time to break and this webinar is being  \\n\"\n",
      "              'recorded Data Umbrella is an inclusive community for '\n",
      "              'underrepresented persons in data science and we are volunteer '\n",
      "              'run a  \\n'\n",
      "              \"brief couple of things about me I'm a statistician data \"\n",
      "              'scientist and I have an ms in masters in statistics and an '\n",
      "              'mba  \\n'\n",
      "              'from NYU and I am the founder of Data Umbrella if you are '\n",
      "              \"interested in learning more about what I'm doing you can  \\n\"\n",
      "              'follow me on Twitter LinkedIn or GitHub I have the same handle '\n",
      "              \"raishma s we have a code of conduct here and we're really  \\n\"\n",
      "              'really intentional about providing a respectful welcoming '\n",
      "              'professional environment so please make sure that you follow  \\n'\n",
      "              \"our code of conduct it's listed on our website and also this \"\n",
      "              'applies to the chat as well so please you know be  \\n'\n",
      "              'professional about what we post on the chat so the ways to '\n",
      "              'support our community is the most importantly as I said  \\n'\n",
      "              'following our code of conduct we have a discord channel the '\n",
      "              \"link is on our website and so it's a great place to ask and  \\n\"\n",
      "              'answer questions there if you want to share events through your '\n",
      "              \"community there that's a good place to do it and we have  \\n\"\n",
      "              'for all of our events we record them and we have a list of '\n",
      "              'transcripts on our GitHub repo so if you would like to help  \\n'\n",
      "              'us edit it feel free to email us and I also have detailed '\n",
      "              'instructions on how to go about doing that another way that  \\n'\n",
      "              'you can support us is donate to our open collective it helps us '\n",
      "              'cover operational costs such as Meetup dues and other  \\n'\n",
      "              'other operational costs and we greatly appreciate your support '\n",
      "              'we are on all social media platforms as Data Umbrella so  \\n'\n",
      "              'the best place to find out about upcoming events is on our '\n",
      "              'Meetup group all of our events are posted as I said on the  \\n'\n",
      "              'video library on Data Umbrella we have a job board as well and '\n",
      "              'a newsletter which goes out once a month and we have  \\n'\n",
      "              'resources on our website website related to diversity alice '\n",
      "              'allyship inclusive language and more on our video library on  \\n'\n",
      "              'YouTube we have a playlist for contributing to open source and '\n",
      "              'so we have for the library is numpy scikit-learn Pandas  \\n'\n",
      "              'and core Python there are some videos so you know for those of '\n",
      "              \"us in data science open source is critical and it's a  \\n\"\n",
      "              'good way to figure out and learn how you can contribute we also '\n",
      "              'have a playlist on career advice we have three speakers  \\n'\n",
      "              \"who've done a really informed really done informative talks \"\n",
      "              'about it so if you are looking for career advice please  \\n'\n",
      "              'check those out this is just a small screenshot of some of the '\n",
      "              'events that we have on our YouTube so check it out  \\n'\n",
      "              \"depending on what topic you're interested in our job board is \"\n",
      "              'jobs at jobs.dataumbrella.org if your company is hiring  \\n'\n",
      "              'they can post jobs there and we can share it with our community '\n",
      "              'on in our newsletter as well as in upcoming webinars our  \\n'\n",
      "              'highlighted job that we have is cloud infrastructure engineer '\n",
      "              'at coiled and it is a distributed infrastructure so feel  \\n'\n",
      "              'free to check that out to learn more about the position this is '\n",
      "              'just a list of some of the resources that we have on our  \\n'\n",
      "              'website open source guides to using inclusive language how  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:04:00,799::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              'people can be allies how to handle burnout and resources for AI '\n",
      "              \"ethics as I mentioned before we're on all platforms  \\n\"\n",
      "              'under Data Umbrella so depending on your choice whichever '\n",
      "              'platform you prefer to you prefer to use follow us on that  \\n'\n",
      "              'platform I want to share some upcoming events that we have we '\n",
      "              'have intro to sphinx for Python documentation presented by  \\n'\n",
      "              'Melissa weber and that is on January 28th and the link to that '\n",
      "              'to sign up is on our Meetup page we have our flagship  \\n'\n",
      "              'event for the year which is a scikit-learn open source sprint '\n",
      "              'this this sprint is focused on participating getting  \\n'\n",
      "              'participants from the regions of Africa and middle East and so '\n",
      "              \"if you're in those regions please check out our website  \\n\"\n",
      "              \"it's afme2021.dataumbrella.org if you have any questions feel \"\n",
      "              'free to email us the information about contacting us is on  \\n'\n",
      "              \"the application form on the website there's also want to share \"\n",
      "              \"a community event that's coming up which is global  \\n\"\n",
      "              'diversity cfp day and the goal of this event is to provide '\n",
      "              'resources for underrepresented or marginalized groups in '\n",
      "              'tech  \\n'\n",
      "              'to submit pf cfps and get started speaking at Meetup events and '\n",
      "              'conferences and there are live streams there are six  \\n'\n",
      "              'different live streams for different regions in the world and '\n",
      "              \"there's some great talks and workshops coming out the  \\n\"\n",
      "              'website is globaldiversitycfpday.com feel free to check that '\n",
      "              'out and I want to now introduce our speaker for this event  \\n'\n",
      "              'it is Nick giannitakis and Nick is a developer and cis admin he '\n",
      "              'is an independent freelancer course creator and host of  \\n'\n",
      "              'podcasts running in production he has hundreds of blog posts '\n",
      "              'blog posts and YouTube videos focused on building and  \\n'\n",
      "              'deploying web apps as along with development environment tips I '\n",
      "              'reached out to Nick because you know studying using the  \\n'\n",
      "              'command line is a really really useful skill for data '\n",
      "              \"scientists and I think it's really going to be beneficial to  \\n\"\n",
      "              \"people in our community whether they're aspiring data \"\n",
      "              'scientists or experienced practicing data scientist and you '\n",
      "              'can  \\n'\n",
      "              \"find Nick and more information on Nick's website Nick \"\n",
      "              'johnnytakis.com and GitHub is at Nick jj and Nick is also on  \\n'\n",
      "              'Twitter as at Nick jonatakis and with that I am going to turn '\n",
      "              'the camera and microphone over to Nick OK thank you for  \\n'\n",
      "              'the introduction let me just share my screen really quick do '\n",
      "              'you see anything it shows your slides by the way just a  \\n'\n",
      "              'heads up here we go OK so everyone can see this now I hope so '\n",
      "              'hey everyone thanks a lot for coming my name is Nick  \\n'\n",
      "              \"genitakis and today we're going to go over creating a command \"\n",
      "              'line driven development environment by the end of this  \\n'\n",
      "              \"talk you'll be well on your way to having a tricked out \"\n",
      "              'terminal along with being familiar with tools like tmx and '\n",
      "              'vim  \\n'\n",
      "              'and also being comfy using command line tools to solve real '\n",
      "              \"world problems we're also going to go over how you can  \\n\"\n",
      "              'manage your dot files these are files that typically live in '\n",
      "              'your home directory and are responsible for configuring a  \\n'\n",
      "              'bunch of command line tools just a heads up everything we go '\n",
      "              'over today is going to work on all major distros of Linux  \\n'\n",
      "              'macos and Windows 10. in the case of Windows it will be '\n",
      "              \"expected that you're using wsl1 or wsl2 which is the Windows  \\n\"\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:08:00,879::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              \"Windows subsystem for Linux that's because we're going to focus \"\n",
      "              'on shell scripting in a unix or unix-like environment  \\n'\n",
      "              \"not so much powershell also we're going to cover a lot of \"\n",
      "              \"ground in this talk but don't worry I'm not going to leave \"\n",
      "              'you  \\n'\n",
      "              'hanging on installing and configuring everything on your own '\n",
      "              'from scratch so feel free to check out this get repo  \\n'\n",
      "              'afterwards it has a very skimmable reference for everything '\n",
      "              \"we're going to do as well as this slide deck you can just  \\n\"\n",
      "              'download it there as well with that said let me quickly '\n",
      "              \"introduce myself my name is Nick genitakis and I've been a  \\n\"\n",
      "              'freelance developer for the last 20-ish years and I would say '\n",
      "              \"for the last seven or so years I've been doing a lot of  \\n\"\n",
      "              \"devops work that's mainly setting up Linux servers working with \"\n",
      "              'various cloud hosting providers deploying code writing  \\n'\n",
      "              'shell scripts and using tools like docker ansible and terraform '\n",
      "              \"now the specifics aren't too important but I do think  \\n\"\n",
      "              'working with a bunch of different technologies and projects has '\n",
      "              'led me to having a command line focused development  \\n'\n",
      "              \"environment now chances are you're not going to be using the \"\n",
      "              'same exact tech stack as me but we can all use the same  \\n'\n",
      "              'command line tools to solve our unique problems for example I '\n",
      "              'found myself bouncing around so many different freelance  \\n'\n",
      "              'and open source projects and I wanted a way to quickly switch '\n",
      "              'between them to get right back into our project as if I  \\n'\n",
      "              'never left that led me to discovering tmux and eventually vim '\n",
      "              \"and by the way if you decide you don't want to use vim as  \\n\"\n",
      "              \"a primary code editor that's completely fine I only started \"\n",
      "              'using vim about two years ago mainly because the combination  \\n'\n",
      "              'of using it with tmux really let me quickly switch between '\n",
      "              \"projects on the command line but even if you don't use vim  \\n\"\n",
      "              \"there's so much more to the command line than cone editing and \"\n",
      "              'tmux is really useful on its own in any case now before  \\n'\n",
      "              'we dive into the basics of using tmx and other tools let me '\n",
      "              'quickly go over a couple of practical examples of how I use  \\n'\n",
      "              \"tmux vim and various command line tools in my day to day I'm \"\n",
      "              'going to blaze through this without explaining everything  \\n'\n",
      "              'in detail so you can get a high level overview of how '\n",
      "              'everything fits together and see what types of workflows you '\n",
      "              'can  \\n'\n",
      "              'create this way you can start thinking about how all of this '\n",
      "              'applies back to your end your dev environment then after  \\n'\n",
      "              \"that we'll go back and cover how everything works and how \"\n",
      "              'everything can be configured in more detail also just a '\n",
      "              'heads  \\n'\n",
      "              'up for the sake of time and avoiding silly mistakes like typos '\n",
      "              'I decided to go with animated gifs for demoing a few  \\n'\n",
      "              \"things that means you'll occasionally see animations that are \"\n",
      "              'looping indefinitely and they may not line up exactly with  \\n'\n",
      "              \"what I'm saying but I think you'll get the gist of it lastly \"\n",
      "              \"feel free to ask questions if anything isn't clear because  \\n\"\n",
      "              \"I am prepared to jump into a live terminal if needed let's do \"\n",
      "              \"this so here's a brand new terminal that I just opened and  \\n\"\n",
      "              \"let's say I want to start working on the source code to my \"\n",
      "              'Flask course which is a web application running in docker I  \\n'\n",
      "              'was working on that last night but today is a new day instead '\n",
      "              'of opening everything from scratch I can attach to a tmx  \\n'\n",
      "              \"session that's running in the background it already has vim \"\n",
      "              'open with the last file I was editing along with the entire  \\n'\n",
      "              \"app running inside of docker it's like I never left I can \"\n",
      "              \"immediately get back into the deck of it next up let's say I  \\n\"\n",
      "              'want to do a project-wide search for all the tests in the '\n",
      "              'project I can do a search for the test underscore string in  \\n'\n",
      "              \"vim and it'll do a fuzzy search and list all the matches also \"\n",
      "              'instead of a string I could have used a regular expression  \\n'\n",
      "              'after that vim will present me a way to filter and select the '\n",
      "              'matches I want and it even shows a little preview of the  \\n'\n",
      "              'file as I cycle through the matches my font size is massive for '\n",
      "              'the sake of this talk but normally you can see a lot  \\n'\n",
      "              'more of the file in the preview then from here I can pick a '\n",
      "              'file and it opens in the main buffer if I want to look at  \\n'\n",
      "              'multiple files at once vim makes it easy to open horizontal and '\n",
      "              'vertical splits it looks a little cramped here because  \\n'\n",
      "              'the font size is so large with my normal setup I can fit four '\n",
      "              '80 character width files side by side  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:12:00,720::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              'on a 2560x1440 monitor running at one to one scaling also I '\n",
      "              \"didn't show it there but it's really easy to switch between  \\n\"\n",
      "              \"the different splits using hotkeys or the mouse now let's say \"\n",
      "              'that you want to open a different set of files but you  \\n'\n",
      "              \"don't want to lose your existing layout of files with a vim you \"\n",
      "              'can open files in a new tab and this tab becomes a  \\n'\n",
      "              'bucket to hold as many files as you want then you can switch '\n",
      "              \"between tabs up top using hotkeys or the mouse so let's say  \\n\"\n",
      "              \"we did some work and now it's time to commit our changes I can \"\n",
      "              'hop over to the second tmux window on the bottom open a  \\n'\n",
      "              'split and make my git commit message as an aside tmux also '\n",
      "              'supports horizontal and vertical splits I also like using '\n",
      "              'vim  \\n'\n",
      "              'for writing commit messages because it helps me adhere to best '\n",
      "              'practices such as warning me through syntax highlighting  \\n'\n",
      "              'if the first line is too long or if I have any typos next up '\n",
      "              \"let's say you have a few tmux splits open and you really  \\n\"\n",
      "              \"want to view the log details in full size but you don't want to \"\n",
      "              'close your existing splits tmux lets you zoom in and out  \\n'\n",
      "              \"of any split you want it's I'm just hitting a hotkey there to \"\n",
      "              \"toggle the zoom in and out also it's worth mentioning you  \\n\"\n",
      "              \"can do the same thing with vim splits too I didn't show it for \"\n",
      "              \"the sake of time and just as a reminder we're going to  \\n\"\n",
      "              'see how to do all these things later on when we dive into the '\n",
      "              \"details next up let's say I want to jump to a completely  \\n\"\n",
      "              \"different tmok session in this case it's my podcast site and \"\n",
      "              'maybe I came here to add a reference link or something like  \\n'\n",
      "              'that after doing whatever I needed to do I can jump over to the '\n",
      "              'second tmux window on the bottom and deploy the site  \\n'\n",
      "              \"with ansible ansible's deploy command is kind of long and I \"\n",
      "              \"don't want to have to hit the up arrow 100 times to find it  \\n\"\n",
      "              'so I can reverse search my history using the same tool that we '\n",
      "              'saw in fim to fuzzy search through files in a project  \\n'\n",
      "              \"this tool is called fcf by the way lastly here's a quick demo \"\n",
      "              'of doing GitHub style Markdown previews in vim technically  \\n'\n",
      "              \"it's not real time since it's not updated on every character \"\n",
      "              'press but you can turn that option on if you want I keep it  \\n'\n",
      "              'off for performance reasons instead it updates every time I '\n",
      "              'switch vim modes or save the file also on the right vim puts  \\n'\n",
      "              \"aaron's exclamation points near the line count that's to let me \"\n",
      "              'know that these lines have changed based on the most  \\n'\n",
      "              \"recent git commit it'll show new lines and deleted lines with \"\n",
      "              'different markers too honestly we could spend the whole  \\n'\n",
      "              \"hour looking at vim and tmux features but there's so much more \"\n",
      "              \"to the command line than using vim so let's switch gears  \\n\"\n",
      "              'and go over a few practical examples of using command line '\n",
      "              \"tools for example let's say that I'm working on something \"\n",
      "              'and  \\n'\n",
      "              'it sparks an idea for a blog post I can just run a custom '\n",
      "              'script I created called drafts and start typing whatever I  \\n'\n",
      "              'want after running that command it automatically creates a date '\n",
      "              \"labeled file in my personal website's drafts folder if I  \\n\"\n",
      "              'rerun the same script it opens the file in vim so I can start '\n",
      "              \"fleshing out the draft right now that's typically what  \\n\"\n",
      "              'happens in practice this might seem like such a minor thing but '\n",
      "              'it really helps remove friction when starting new blog  \\n'\n",
      "              'posts because the tool I use to build the blog expects files to '\n",
      "              'be in this dated format not having to create this file  \\n'\n",
      "              \"manually means it's one less thing I need to think about as for \"\n",
      "              \"the draft script it's a simple script and don't worry if  \\n\"\n",
      "              \"you can't read the text the real takeaway here is the command \"\n",
      "              'line is there to help you solve whatever unique problems  \\n'\n",
      "              \"you have it's almost like a direct link to your brain except \"\n",
      "              'you just have to type what you want instead of just  \\n'\n",
      "              'thinking it for now in another example I created a script to '\n",
      "              'toggle dark and light mode for my terminal tmux vim and  \\n'\n",
      "              'other tools I prefer using light themes in my day to day but '\n",
      "              'after running a poll I found most folks like seeing a dark  \\n'\n",
      "              'theme in my video tutorials so with the script I can quickly '\n",
      "              \"change between the two it's open source and my public dot  \\n\"\n",
      "              \"files and we'll take a look at that later in one more example \"\n",
      "              \"let's say I was heading over to New York City later  \\n\"\n",
      "              \"tonight and I wasn't sure if I needed a really heavy jacket \"\n",
      "              'because I live 90 minutes away and weather is crazy like  \\n'\n",
      "              'that not a problem I can just run weather nyc and  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:16:02,320::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              \"that's what we see here there's almost a tool for everything in \"\n",
      "              'this case the weather command is a very simple function  \\n'\n",
      "              'I set up to curl wttr.in which is a website that shows you the '\n",
      "              \"weather in a browser but it's also optimized for  \\n\"\n",
      "              \"displaying the weather on the command line there's also dozens \"\n",
      "              'of general purpose unix tools that you can use to solve  \\n'\n",
      "              \"problems around processing and filtering text I I'm trying to \"\n",
      "              'keep this folks talk focused on workflows and more  \\n'\n",
      "              'generally your development environment but besides using tmx '\n",
      "              'and vim piping together a few unix commands and writing  \\n'\n",
      "              'your own scripts to solve your specific problems is a huge part '\n",
      "              'of using the command line it becomes another tool at  \\n'\n",
      "              \"your disposal with that said let's take a couple of minutes and \"\n",
      "              'go over one example of doing this so imagine if you have  \\n'\n",
      "              'a csv file with a bunch of transactions in it and you want to '\n",
      "              'extract that information like how much money you generated  \\n'\n",
      "              'but also have an ability to filter the results on specific '\n",
      "              \"product names date ranges and so on the csv file we're  \\n\"\n",
      "              'looking at here is a simplified version of a csv that my course '\n",
      "              'platform spits out with a bit of anonymized data in  \\n'\n",
      "              \"practice you'll find payment gateways like stripe and paypal \"\n",
      "              'supplying your transaction information in csv files so this  \\n'\n",
      "              'is a real world use case as an aside vim has a plugin to help '\n",
      "              \"view and display csv data and we'll go over that in a  \\n\"\n",
      "              \"little bit but viewing this data isn't super useful typically I \"\n",
      "              'want to figure out things like how many courses were  \\n'\n",
      "              'sold this month and have an ability to filter the results by '\n",
      "              'course name and so on and that begins with removing the csv  \\n'\n",
      "              \"headers said stands for stream editor and in practice it's a \"\n",
      "              'very useful tool for doing a find and replace in text but  \\n'\n",
      "              \"in this case we're using it to delete the first line in the \"\n",
      "              'file the 1d part stands for delete the first line if we  \\n'\n",
      "              'swapped out the one with a two then the first two lines would '\n",
      "              'have been deleted instead one neat thing about the command  \\n'\n",
      "              \"line is there's often more than one way to do something if you \"\n",
      "              'googled around for how to delete the first line in a file  \\n'\n",
      "              \"you'll find all sorts of different answers and most of them \"\n",
      "              'will probably work for example we could have used the tail  \\n'\n",
      "              'command and passed in these flags to get the same exact result '\n",
      "              \"technically tail is faster than set on Linux but it's  \\n\"\n",
      "              'also slower on Mac OS by default at least according to stack '\n",
      "              'overflow although I find the tl version to be less readable  \\n'\n",
      "              \"for this type of use case it's confusing because plus one \"\n",
      "              'actually outputs the entire file so we need to offset what '\n",
      "              'we  \\n'\n",
      "              \"want by one and then there's also differences between using one \"\n",
      "              'and plus one one takeaway here is you should be mindful  \\n'\n",
      "              'of performance but readability almost always wins especially if '\n",
      "              \"the performance difference isn't worth anything  \\n\"\n",
      "              'considering for example if we had thousands of rows and the '\n",
      "              'said version finished in five milliseconds but the tail  \\n'\n",
      "              \"version finished in 3 milliseconds it doesn't really matter for \"\n",
      "              'a script that I run once a month on my dev box but I  \\n'\n",
      "              'will remember what 1d does instead 3 months from now which has '\n",
      "              'way more value in the end as an aside tail is still a  \\n'\n",
      "              'great tool if we wanted to get the last line or last couple of '\n",
      "              'lines of the file then tail is for sure the way to go  \\n'\n",
      "              'here you can even have tail watch a file for changes in real '\n",
      "              'time and output new lines as they come in that could be  \\n'\n",
      "              'really useful for monitoring web server logs but moving on '\n",
      "              \"we're going to use the said version and the next thing we'll  \\n\"\n",
      "              'want to do is filter out a specific column of data this column '\n",
      "              \"is the net amount of sense and it's the thing we want to  \\n\"\n",
      "              'sum up in the end unix was invented a long time ago but they '\n",
      "              'made some great design decisions such as being able to pipe  \\n'\n",
      "              \"the output of one program as input into another and you'll also \"\n",
      "              'find that text streams are the protocol that most unix  \\n'\n",
      "              'commands use that means most command line tools expect text as '\n",
      "              'input and produce text as output this is remarkably  \\n'\n",
      "              'powerful because suddenly it means you can combine hundreds of '\n",
      "              'tools in a number of different ways to solve  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:20:00,320::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              'very specific problems that you might have and unix pipes are '\n",
      "              'the secret sauce that lets us send the output of one  \\n'\n",
      "              'program as input to another pipes are defined with the vertical '\n",
      "              'line and you can see that they are sitting in between  \\n'\n",
      "              'said and cut that means in this case the output of the set '\n",
      "              'command is now being fed in as input to the cut command and  \\n'\n",
      "              'this cut command is going to delimit the output by a comma '\n",
      "              'since this is a csv file and we want to grab the fifth '\n",
      "              'column  \\n'\n",
      "              'which is the net amount in cents if we ever wanted to calculate '\n",
      "              'the transaction fees for a specific payment gateway then  \\n'\n",
      "              'we can change this five to a four and now it would get the '\n",
      "              'previous column which are those fees and this is why command  \\n'\n",
      "              'line tools are amazing we get to break down the problem into '\n",
      "              'bite-sized chunks and then work on only a very specific  \\n'\n",
      "              'tiny problem at hand I think this fits the functional '\n",
      "              'programming model very nicely speaking of which a lot of  \\n'\n",
      "              'functional programming languages also have the idea of a pipe '\n",
      "              'operator where you can send the output of a function as  \\n'\n",
      "              \"input to another function it's a great way to break up a \"\n",
      "              \"problem next up now that we have a list of amounts we'll \"\n",
      "              'want  \\n'\n",
      "              'to sum them up but before we can sum them we need to transform '\n",
      "              'the list of numbers into a math equation that adds each  \\n'\n",
      "              \"value for that there's the paste command and don't ask me why \"\n",
      "              \"it's name paste because I have no idea it has nothing to  \\n\"\n",
      "              'do with your clipboard all I know is when we use these flags it '\n",
      "              'becomes what we see here if you wanted to do something  \\n'\n",
      "              'else like subtract the values then you can replace the plus at '\n",
      "              'the end with a hyphen and technically you can use this  \\n'\n",
      "              \"paste tool for anything it's not limited to math if we put an \"\n",
      "              'underscore instead then it would delimit them with that by  \\n'\n",
      "              'the way the dash d is the delimiter and dash s puts everything '\n",
      "              'onto one line and now that we have our equation ready to  \\n'\n",
      "              'go we can use the basic calculator command aka the bc command '\n",
      "              'to sum them all up and there we go we solved our problem  \\n'\n",
      "              \"by piping together a few commands in practice when you're more \"\n",
      "              'familiar with these commands you can create pipelines  \\n'\n",
      "              \"like this in a few minutes by the way there's a gacha here too \"\n",
      "              'with the bc command if you ever want to support floating  \\n'\n",
      "              \"point numbers you'll need to add the lflag to it I didn't add \"\n",
      "              \"it here since we're dealing with summing whole numbers so  \\n\"\n",
      "              \"there's never going to be a decimal point next up let's filter \"\n",
      "              'the results by a specific course by using the grep tool  \\n'\n",
      "              \"this is a tool you'll likely find yourself using all the time \"\n",
      "              'you can provided a text stream or a file and then we can  \\n'\n",
      "              \"match whatever search string we want in our case it's a regular \"\n",
      "              'expression that makes sure comma bsawf is at the very  \\n'\n",
      "              'end of the line this is nice because it means we no longer need '\n",
      "              'to use the set command to chop out the first line since  \\n'\n",
      "              'this grep pattern will only ever match csv rows that have this '\n",
      "              'specific text and then from here we can use what we did  \\n'\n",
      "              'before to sum up the net sales and suddenly we have the total '\n",
      "              'sales for a specific course I also decided to throw in two  \\n'\n",
      "              'more examples here at the end the first one lets us count the '\n",
      "              'number of sales for a specific month here we just take the  \\n'\n",
      "              'output of what grep produces and pipe it into the word count '\n",
      "              'command the l flag counts the number of lines instead of  \\n'\n",
      "              'words alternatively grep has a c flag to get a count of the '\n",
      "              'results instead of returning each match on a new line in  \\n'\n",
      "              'practice I would use this second command if I wanted the count '\n",
      "              \"it's less typing less overhead and more straightforward  \\n\"\n",
      "              \"to read so that's a mini crash course in combining a couple of \"\n",
      "              'commands together to solve a real world problem the cool  \\n'\n",
      "              'thing is if you find yourself writing scripts like this and '\n",
      "              'they start to get a bit unwieldy then you can put them into  \\n'\n",
      "              \"a dedicated script file and run them like that we'll talk more \"\n",
      "              'about shell scripts a bit later on and that leads us into  \\n'\n",
      "              'the next part of this talk we just covered a bunch of practical '\n",
      "              'workflows and examples of using the command line now  \\n'\n",
      "              \"it's time to get into the details about setting up your \"\n",
      "              'development environment and that begins with picking a '\n",
      "              'terminal  \\n'\n",
      "              \"and configuring your shell we're going to spend quite a bit of \"\n",
      "              'time  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:24:00,000::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              \"on this because it's pretty core to your environment the \"\n",
      "              'interesting thing about tmux by the way is it gives you tabs  \\n'\n",
      "              'splits and a way to search up and down a buffer in any terminal '\n",
      "              'this allows us to pick a terminal that focuses on speed  \\n'\n",
      "              'and having the least amount of input latency when you press '\n",
      "              'keys you can al you can also optimize for quality of life  \\n'\n",
      "              'enhancements too like customizable hotkeys easily being able to '\n",
      "              'zoom in and out being able to click URLs and also have  \\n'\n",
      "              \"solid unicode support for displaying icons and emojis if you're \"\n",
      "              'going to be spending a lot of time typing something in  \\n'\n",
      "              'the command line it should feel really good and make you happy '\n",
      "              \"while doing it for example I'm using Windows and the  \\n\"\n",
      "              \"Microsoft terminal has really really low key press latency it's \"\n",
      "              'like night and day compared to most other terminals on  \\n'\n",
      "              'Windows I tried a whole bunch and the other one that came close '\n",
      "              \"was wsl tty but I've gone with the Microsoft terminal  \\n\"\n",
      "              \"because it's more integrated with being able to easily switch \"\n",
      "              'between wsl distros as for anyone running native Linux  \\n'\n",
      "              \"xterm is a very lightweight terminal and that's pretty much the \"\n",
      "              \"lowest input latency that I've ever encountered on any  \\n\"\n",
      "              \"os also with a bit of configuration it's all the marks for the \"\n",
      "              'quality of life improvements I do run native Linux on a  \\n'\n",
      "              'modified chromebook and that always amazes me at how nice the '\n",
      "              'terminal experience is for Mac OS to be honest I am not  \\n'\n",
      "              \"100 sure since I have no first hand experience but I've heard \"\n",
      "              'really good things about item 2 so I might want to start  \\n'\n",
      "              \"there if you want to give it a shot in addition to that there's \"\n",
      "              'cross-platform terminals like alacrity which is  \\n'\n",
      "              'something you may want to check out too but personally on '\n",
      "              \"Windows and Linux I've always used the Microsoft terminal \"\n",
      "              'and  \\n'\n",
      "              \"xterm and I think the takeaway here is it doesn't matter too \"\n",
      "              'much on which terminal you pick because tmux brings a lot  \\n'\n",
      "              'to the table so just stack up on speed and quality of life '\n",
      "              \"improvements and I wouldn't worry about too much about \"\n",
      "              'having  \\n'\n",
      "              'native terminal support for things like tabs splits and '\n",
      "              \"searching next up it's time to pick a shell and your shell  \\n\"\n",
      "              \"provides a command line interface to your operating system it's \"\n",
      "              'both interactive and it can be used as a scripting  \\n'\n",
      "              'language right now I happen to be running bash and you can '\n",
      "              \"check which shell you're currently running by running echo  \\n\"\n",
      "              \"dollar sign zero there's also dollar sign shell too but this is \"\n",
      "              'technically the default shell for your system or user  \\n'\n",
      "              'not your current shell to help clarify that another shell is sh '\n",
      "              'which is the bourne shell and it was released way back  \\n'\n",
      "              \"in 1979 if you're curious its successor bash stands for the \"\n",
      "              'born again shell and that was released about 10 years later  \\n'\n",
      "              \"in 1989 but just because I'm running bash here doesn't mean I \"\n",
      "              \"can't use the original boring shell you can run it by  \\n\"\n",
      "              'running sh notice how the prompt changed and now running dollar '\n",
      "              'sign zero returns sh instead of bash but dollar sign  \\n'\n",
      "              'shell still returns bash because for my nic user on the system '\n",
      "              'bash is set as the default typically in modern systems  \\n'\n",
      "              \"you wouldn't use sh as your interactive shell in your day to \"\n",
      "              \"day it's missing very nice to have features like being able  \\n\"\n",
      "              'to tap complete commands or use the arrow keys to move the '\n",
      "              \"cursor around however when writing shell scripts it's often \"\n",
      "              'a  \\n'\n",
      "              \"good idea to target your scripts for the shell since it's the \"\n",
      "              \"most compatible across systems although there's not too  \\n\"\n",
      "              \"much harm in targeting bash2 because it's available on most \"\n",
      "              \"Linux systems and Mac OS we'll talk about shell scripting in  \\n\"\n",
      "              \"a bit but for now let's focus on the shells themselves besides \"\n",
      "              \"sh and bash there's also newer shells such as z shell  \\n\"\n",
      "              'which is labeled as zsh just like bash extended the original '\n",
      "              \"boron shell z shell extends bash personally I'm not a huge  \\n\"\n",
      "              \"fan of it but it's not because it's bad or I have anything \"\n",
      "              \"against it I'm just content with bash and I don't really \"\n",
      "              'feel  \\n'\n",
      "              \"like I'm missing out on anything at the moment by the way it's \"\n",
      "              \"worth pointing out if you're running a modern version of  \\n\"\n",
      "              'Mac OS if you follow along before and you ran the echo dollar '\n",
      "              'sign zero command chances are you saw z shell as your  \\n'\n",
      "              \"shell that's because Mac OS made it the default shell in the \"\n",
      "              'catalina release back in 2019 but even so bash is still  \\n'\n",
      "              'available to be run  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:28:02,799::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              \"on Mac OS next up there's fish which is an even more recent \"\n",
      "              'shell and it stands for friendly interactive shell unlike z  \\n'\n",
      "              'shell fish is not compatible with sh or bash because the '\n",
      "              'creators thought certain design decisions from back in the '\n",
      "              'day  \\n'\n",
      "              'were poorly designed so they went their own way honestly this '\n",
      "              \"shell isn't for me mainly due to that lack of  \\n\"\n",
      "              'compatibility but it might be OK for you it has a lot of the '\n",
      "              'out of the box features like auto suggestions but you can  \\n'\n",
      "              'also get that to work with other shells too it just takes a bit '\n",
      "              'more effort with that said at least now you know these  \\n'\n",
      "              \"options exist we only have so much time here so we can't spend \"\n",
      "              'a ton of time breaking down the pros and cons of each  \\n'\n",
      "              'shell but overall your shell is there to make it a pleasant '\n",
      "              'experience to use your system from the command line if  \\n'\n",
      "              \"you're curious now you can Google for things like bash versus z \"\n",
      "              'shell versus fish and you can make an informed decision  \\n'\n",
      "              'just be aware that this is an excellent topic to get lost in '\n",
      "              'yak shaving or bike shedding in other words you can spend a  \\n'\n",
      "              'lot of time here and not wind up with a lot of games the real '\n",
      "              'gains come from learning how to create tools that help you  \\n'\n",
      "              'solve your exact problems and getting comfy with workflows that '\n",
      "              \"let you do whatever you need to do quickly it's not a  \\n\"\n",
      "              'life or that death decision either you can always switch your '\n",
      "              \"show later on without too much fuss so that's a basic  \\n\"\n",
      "              \"rundown of shells since I happen to use bash let's go over how \"\n",
      "              'I have it configured although in a lot of cases even if  \\n'\n",
      "              \"you plan to use z shell or phish most of what we're about to \"\n",
      "              'cover still applies most shells have a few config files in  \\n'\n",
      "              \"your home directory with bash you'll have at least the bashrc \"\n",
      "              'file and maybe a bash under skype underscore profile too  \\n'\n",
      "              'in my case I decided to rename my bash profile to profile so '\n",
      "              \"that it'll work with any shell although technically this  \\n\"\n",
      "              \"profile file has one line in it that's specific to bash so it's \"\n",
      "              'not entirely portable across all shells with that said  \\n'\n",
      "              \"let's take a look at this file just a heads up before we get \"\n",
      "              \"into this I'm only going to be showing a few lines at a  \\n\"\n",
      "              \"time so it's easier to talk about this entire file along with \"\n",
      "              'all of my config files are in my dot files repo on GitHub  \\n'\n",
      "              \"and we'll cover that near the end as for this profile file it \"\n",
      "              \"runs once when you log into your system so it's a good  \\n\"\n",
      "              \"spot to put things that don't change very often for example I \"\n",
      "              'modify my path here so that any executable scripts or  \\n'\n",
      "              'binaries that exist in my home directories dot local bin '\n",
      "              'directory are available to run without supplying the entire  \\n'\n",
      "              \"path that's how I ran that draft script from before it exists \"\n",
      "              'in my dot local bin directory and I can run it like any  \\n'\n",
      "              \"other command that's already on my system path like rep ls and \"\n",
      "              'some other commands that we took a look at before this is  \\n'\n",
      "              'also where I like to set system defaults for certain tools that '\n",
      "              'I use the editor variable controls which text editor to  \\n'\n",
      "              'open for example if a tool needs to open a text editor it opens '\n",
      "              'vim by default due to this being set the editor variable  \\n'\n",
      "              'is a standard so a bunch of tools know how to read from it we '\n",
      "              'saw that before when I wrote a git commit message get new  \\n'\n",
      "              'to open vim due to this setting you can also choose to set your '\n",
      "              'own variables here if you want I really really like this  \\n'\n",
      "              'pattern because it means your settings are stored in one spot '\n",
      "              'you know if 10 different tools decided to open your  \\n'\n",
      "              'default text editor it means you only need to change this '\n",
      "              'variable in one spot if you wanted to use something else '\n",
      "              'next  \\n'\n",
      "              \"up there's a bunch of configuration to use syntax highlighting \"\n",
      "              'in certain tools like less and man the man command opens  \\n'\n",
      "              'a reference manual for many different tools for example if you '\n",
      "              \"run man grep it'll open the manual for grep this is a  \\n\"\n",
      "              'good way to learn about a specific tool you can also see their '\n",
      "              'syntax highlighting without setting those environment  \\n'\n",
      "              'variables we just saw this would be all one solid color which '\n",
      "              'in my opinion is a bit harder to read lastly this line  \\n'\n",
      "              'does the dollar sign zero trick we did before to see what shell '\n",
      "              \"we're running and even if we're running bash and we have  \\n\"\n",
      "              'the bashrc file present it sources that file sourcing it '\n",
      "              'basically means running the script and any shell-related  \\n'\n",
      "              'changes will take  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:32:01,679::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              'effect in your current shell if you are running a z-shell or '\n",
      "              'phish you would want to modify this for whatever shell  \\n'\n",
      "              \"you're using next up let's take a look at the rc file for the \"\n",
      "              \"sake of time I'm going to try to keep things brief rather  \\n\"\n",
      "              'than go into the gory details of everything this file executes '\n",
      "              'every time you start a new terminal session that means  \\n'\n",
      "              'every time you open a new terminal window or spawn a new tmx '\n",
      "              'window this bashrc file is going to run top to bottom the  \\n'\n",
      "              'first five settings control your history basically if you want '\n",
      "              'to save the last 50 000 lines of history ensure each line  \\n'\n",
      "              'has its own time stamp avoid adding duplicate or empty lines '\n",
      "              'and also append to the history file instead of rewriting a  \\n'\n",
      "              'new file on every entry your history is saved in your home '\n",
      "              \"directory with bash it'll be in a bash underscore history  \\n\"\n",
      "              'file mine is nearly empty now because prior to this talk I '\n",
      "              \"backed up my real history file so that I didn't have to \"\n",
      "              'worry  \\n'\n",
      "              'about showing sensitive information during the talk some of '\n",
      "              'them contain client information about client work and I  \\n'\n",
      "              \"didn't want to leak those out by accident speaking of history \"\n",
      "              'you can take a look at your history by running the history  \\n'\n",
      "              \"command that's useful to find commands you've run in the past \"\n",
      "              \"and with saving 50 000 entries chances are you'll be able  \\n\"\n",
      "              \"to find commands you've run months ago but dropping through \"\n",
      "              'this output would be kind of painful so bash provides the  \\n'\n",
      "              'control r keyboard shortcut to reverse search your bash history '\n",
      "              \"we'll go over that in more detail once we encounter a  \\n\"\n",
      "              'different part of this bashrc file the next couple of settings '\n",
      "              \"are default values of the bashrc file it's not worth  \\n\"\n",
      "              'spending any time on them but I included them here anyways '\n",
      "              'although it is worth mentioning that on a Linux system you  \\n'\n",
      "              'can check out what the default rc file is by cutting out Etsy '\n",
      "              \"scale that dash rc that could come in handy if you're  \\n\"\n",
      "              'tweaking your bash rc file and you want to double check the '\n",
      "              \"default value also in the scale directory there's a few  \\n\"\n",
      "              \"other default config files to look at if you're curious next up \"\n",
      "              'we have aliases and these are really really useful the  \\n'\n",
      "              'basic idea here is if an aliases file exists then we load it in '\n",
      "              \"there's two files here because my dot files are public  \\n\"\n",
      "              'on GitHub and I wanted the option to have a local aliases file '\n",
      "              \"that's ignored from version control this locofile  \\n\"\n",
      "              'contains aliases that are only ever going to be useful to me '\n",
      "              'and are super specific to private files on my file system  \\n'\n",
      "              'like quickly accessing client project and things like that with '\n",
      "              \"that said let's take a look at the regular aliases file  \\n\"\n",
      "              'which by the way in bash would be named bash underscore aliases '\n",
      "              'by default I decided to go with aliases to make it a bit  \\n'\n",
      "              'more portable across different shells it still works as aliases '\n",
      "              \"because it's tied into the file being sourced from the  \\n\"\n",
      "              'bashrc file we just looked at technically we can name this file '\n",
      "              'anything we want as long as it matches up with what you  \\n'\n",
      "              \"source the first couple of aliases are defined by default it's \"\n",
      "              'mainly for enabling colors in popular commands like ls  \\n'\n",
      "              'and grp but it also sets up popular aliases like ll I use this '\n",
      "              'one all the time to list directories and files my real  \\n'\n",
      "              'aliases file has over 20 different functions and aliases and we '\n",
      "              'could legit spend the next two hours going over it but  \\n'\n",
      "              'instead I want to focus on the big picture here in my aliases '\n",
      "              \"file you'll notice there's both functions and aliases and  \\n\"\n",
      "              'they both allow you to access your function or alias as if it '\n",
      "              'were a script on your system path if you plan to pass in  \\n'\n",
      "              'arguments like you see here then it would make sense to use a '\n",
      "              'function otherwise you can set up an alias for example  \\n'\n",
      "              'with the weather command we can pass in a city zip code or '\n",
      "              'nothing and it all works the same the dollar sign one ends '\n",
      "              'up  \\n'\n",
      "              \"being the value you passed in if it's empty then it's treated \"\n",
      "              'as an empty string but in this case we have an alias that  \\n'\n",
      "              'takes no arguments and we saw this command being run during the '\n",
      "              'demo instead of just running the script on its own it  \\n'\n",
      "              'also sources the bachelor c file because it ends up changing '\n",
      "              'the color of a specific tool that depends on bash being  \\n'\n",
      "              \"reloaded that's kind of a neat trick because the alias and \"\n",
      "              'script both have the same name  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:36:00,079::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              \"in this case the alias takes precedence over the script's path \"\n",
      "              \"the takeaway here is don't be afraid to create aliases  \\n\"\n",
      "              'that help you in your day-to-day you should try to mold your '\n",
      "              \"dev environment to fit your exact needs I think there's  \\n\"\n",
      "              'really a whole mindset or philosophy around that especially '\n",
      "              'when it comes to using Linux with the amount of  \\n'\n",
      "              'configuration you can do it really becomes an os that you can '\n",
      "              \"customize for you one day I'll switch to native Linux and  \\n\"\n",
      "              \"the only reason I haven't already is because I had some audio \"\n",
      "              'trouble with some of my hardware I bet if I were running  \\n'\n",
      "              'native Linux for the last 10 years I would probably have 500 '\n",
      "              'aliases and custom scripts to help me in my day-to-day  \\n'\n",
      "              'anyways that is it for aliases the next part of my the next '\n",
      "              'part of my bashrc file is related to the prompt and we  \\n'\n",
      "              \"haven't really gone over that yet so let's quickly do that \"\n",
      "              \"before we see how it's configured my prompt doesn't look \"\n",
      "              'like  \\n'\n",
      "              'the bridge of the uss enterprise and this really comes down to '\n",
      "              'personal preference I very much prefer a minimal prompt  \\n'\n",
      "              \"that gets out of my way and lets me focus on the commands I'm \"\n",
      "              'running and the output of those commands but I do like to  \\n'\n",
      "              'see some information in my prompt for example as we saw earlier '\n",
      "              \"if you're in a git repo it will show the branch name  \\n\"\n",
      "              'which I find to be quite handy beyond that I also like to see '\n",
      "              \"the current directory of where I am since that's important  \\n\"\n",
      "              'information as well also if I ssh into one of my production '\n",
      "              'servers I like to configure the prompt there to be read to  \\n'\n",
      "              \"remind myself that I probably shouldn't copy paste commands \"\n",
      "              'from the internet here or start running a bunch of commands  \\n'\n",
      "              \"without thinking I don't drink much but if I did seeing this \"\n",
      "              'red prompt would be a last resort reminder to hopefully  \\n'\n",
      "              'prevent myself from deleting a production database in a drunken '\n",
      "              'rage one takeaway here is all of these command line  \\n'\n",
      "              \"tools are super configurable if you don't like my basic prompt \"\n",
      "              \"and you'd rather have a more busy prompt with emojis  \\n\"\n",
      "              'rainbows and seeing your battery life or hard drive space on '\n",
      "              \"every line that's totally cool and you can do that I've  \\n\"\n",
      "              'linked to a few resources in the get repo on projects that help '\n",
      "              'you customize your prompt in different ways with that  \\n'\n",
      "              \"said let's see how to configure your prompt aka your ps1 which \"\n",
      "              'stands for primary prompt string the syntax looks like  \\n'\n",
      "              'total insanity and it kinda is but there is a silver lining '\n",
      "              'once you have it set up you never have to worry about it  \\n'\n",
      "              'again until you get bored and want to change things around we '\n",
      "              \"can spend an hour here easy but let's go after the low  \\n\"\n",
      "              'hanging fruit the bash provides a bunch of special characters '\n",
      "              'such as backslash u and slash h these are special  \\n'\n",
      "              'variables that translate to specific things in the case of '\n",
      "              \"slash u that's your username and slash h is the host of the  \\n\"\n",
      "              'machine in my case my username is Nick and I named my computer '\n",
      "              \"kit if you're old enough to remember knight rider you'll  \\n\"\n",
      "              \"get that reference there's also this slash w hidden amongst a \"\n",
      "              \"sea of backslashes that's the current working directory  \\n\"\n",
      "              \"which is slash temp in this case there's a number of other \"\n",
      "              'special variables that you can use too and a lot of them are  \\n'\n",
      "              \"related to dates and times and I've dropped a link to the bash \"\n",
      "              'manual in the references if you want to check that out  \\n'\n",
      "              'now besides those special variables most of the other '\n",
      "              'characters in this ps1 are related to outputting color these '\n",
      "              'are  \\n'\n",
      "              \"called antsy escape codes and it's kind of where the insanity \"\n",
      "              \"begins it's not that it's a terrible system it's just that  \\n\"\n",
      "              'it requires a lot of escaping and also resetting colors back to '\n",
      "              \"the default color if you've picked a different color  \\n\"\n",
      "              \"that's why you see a few zero zero entries that's the result \"\n",
      "              'code as for picking colors if you Google around for an ansi  \\n'\n",
      "              \"color chart you'll find a bunch of examples I usually go to \"\n",
      "              'Wikipedia for it but that has too much to show in one slide  \\n'\n",
      "              'so I made a more readable chart here we can see a list of valid '\n",
      "              'colors fg is foreground and bg is background if you  \\n'\n",
      "              'remember my username and host was green so that would be '\n",
      "              'foreground color 32 and if you go back to the previous slide  \\n'\n",
      "              'here we can see 32 being set for the username and host we can '\n",
      "              'also see that 34 are set for the working directory  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:40:02,560::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              \"which is blue that's the slash temp and if we jump back to the \"\n",
      "              'chart again here we can see that 34 is blue now there is  \\n'\n",
      "              'a variation of these colors called bright mode as far as I know '\n",
      "              \"there's more than one way to set them honestly a lot of  \\n\"\n",
      "              'this stuff is black magic to me and I just copy paste things '\n",
      "              \"and tweak it until I'm happy but as for bright mode notice  \\n\"\n",
      "              'how there is a zero one before the 32 and 34. to my knowledge '\n",
      "              'that flips on bright mode for the color being set if you  \\n'\n",
      "              'set it to zero zero it would be the regular version of the '\n",
      "              'color an alternative way to set that is to add 60 to the  \\n'\n",
      "              'value so 32 becomes 92 and 34 becomes 94. keep in mind not all '\n",
      "              'terminals and themes are created equal so you may see  \\n'\n",
      "              \"different results depending on what you use but here's a \"\n",
      "              'modified chart to show how bright colors look using xterm '\n",
      "              'with  \\n'\n",
      "              'the default theme also my prompt only modifies the foreground '\n",
      "              \"color of text but you can set the background color too I'm  \\n\"\n",
      "              'going to leave that one up to you if you want to research it '\n",
      "              \"you'll be able to find lots of copy-pastable examples if  \\n\"\n",
      "              \"you Google around as for the command separator it's pretty \"\n",
      "              'standard to end your prompt with a dollar sign this will end  \\n'\n",
      "              'up being the character that separates your prompt with whatever '\n",
      "              \"command you're running some folks like to use unicode  \\n\"\n",
      "              'characters or an emoji years ago I used a lightning bolt but '\n",
      "              \"eventually grew bored of it but don't let me talk you out  \\n\"\n",
      "              'of using something else I really do encourage you to mold your '\n",
      "              'prompt and dev environment to whatever you like if you  \\n'\n",
      "              \"did want to swap out the dollar sign it's a matter of replacing \"\n",
      "              'it here at the end as for this prompt the last component  \\n'\n",
      "              'is getting the get branch wedged in near the end the basic idea '\n",
      "              'is we execute a function here and the output of that  \\n'\n",
      "              'function ends up in its place that function is defined in the '\n",
      "              'middle here and all it does is grab the current branch  \\n'\n",
      "              'then it uses said to replace some characters that get usually '\n",
      "              'outputs when you want to run the get branch command also  \\n'\n",
      "              'if the command fails we redirect all errors to dev null which '\n",
      "              \"means if we're not get repo we end up with no output which  \\n\"\n",
      "              \"is exactly what we want honestly it's not too important to go \"\n",
      "              \"through this function in detail I'm pretty sure I grabbed  \\n\"\n",
      "              'it from stack overflow like seven years ago and it still works '\n",
      "              \"great there's other more fancy integrations that you can  \\n\"\n",
      "              'add too such as outputting specific characters depending on the '\n",
      "              'state of staged files or if a file is different than  \\n'\n",
      "              \"what's been commit but in practice I don't really find that to \"\n",
      "              'be that useful typically that information could be  \\n'\n",
      "              \"obtained in your code editor or using get commands when you're \"\n",
      "              'ready to commit something but the branch is very nice to  \\n'\n",
      "              'see because being on the wrong branch can cause all sorts of '\n",
      "              \"confusion and typically you're launching vim or another  \\n\"\n",
      "              \"text editor while you're actively looking at your prompt so the \"\n",
      "              \"branch is nice to see there OK so that's the prompt the  \\n\"\n",
      "              'next bit here updates your terminals window title depending on '\n",
      "              \"which directory you're in this only works if your  \\n\"\n",
      "              \"terminal supports it and it's part of the default bashrc file \"\n",
      "              \"so it's not super important to go over next up we have  \\n\"\n",
      "              'asdf which is a tool that helps you manage programming language '\n",
      "              \"versions in a consistent way it's not important to cover  \\n\"\n",
      "              'now since this mainly relates to installing specific '\n",
      "              'programming languages although the vim plugin I use for '\n",
      "              'previewing  \\n'\n",
      "              'Markdown files does require having node installed and I do use '\n",
      "              \"asdf to install node we'll talk more about this later  \\n\"\n",
      "              \"next up there's fcf which is a command line fuzzy finder we saw \"\n",
      "              'this in action during the demo when searching through a  \\n'\n",
      "              'project in vim as well as the command history this tool '\n",
      "              'replaces the behavior for reverse searching your history and  \\n'\n",
      "              'makes it a lot better searching your history is an important '\n",
      "              \"part of using the command line so let's spend a minute or  \\n\"\n",
      "              'two going over how to do this and how fcf improves the '\n",
      "              \"experience when you hit CTRL r in bash without fcf you'll get \"\n",
      "              'a  \\n'\n",
      "              'menu that looks like this the basic idea is you can start '\n",
      "              \"typing the first few characters of any command and it'll \"\n",
      "              'start  \\n'\n",
      "              \"showing you matching commands you've run in the past  \\n\"\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:44:00,319::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              \"in this case I'm searching for the characters bs if there's \"\n",
      "              'multiple commands that start with these characters then you  \\n'\n",
      "              'can cycle through them by hitting control r as many times as '\n",
      "              \"you want it's called a reverse search because it returns  \\n\"\n",
      "              'the last command that matches in other words it searches your '\n",
      "              'history in reverse this is one of the most useful things  \\n'\n",
      "              'ever it sure beats hitting the up arrow 42 times in a row to '\n",
      "              'find some command you ran three days ago I probably use  \\n'\n",
      "              'this feature 100 times a day you can pick the command you want '\n",
      "              \"to run by hitting enter and it'll populate the command  \\n\"\n",
      "              'for you so in the above case all I did was hit CTRL r searched '\n",
      "              'for the bs characters and then hit enter all in all  \\n'\n",
      "              \"something like this takes a second or two to use it's really \"\n",
      "              'practical practical by the way you can also cancel your  \\n'\n",
      "              \"reverse search by hitting CTRL c if you don't want to pick \"\n",
      "              'anything now when you use fcf control r is still available '\n",
      "              'to  \\n'\n",
      "              'run except now it looks like this then as you start typing it '\n",
      "              'narrows down the search in real time in this case only  \\n'\n",
      "              \"commands that contain the c character are returned it's also a \"\n",
      "              'fuzzy search meaning if instead I searched for the l  \\n'\n",
      "              'character it would have found the second match since clear has '\n",
      "              'an l in it it ranks then based on what it thinks is the  \\n'\n",
      "              \"best match and it's very good I would say probably 99 of the \"\n",
      "              'time it picks the command they want after typing a few  \\n'\n",
      "              \"characters and and that's even going through thousands of files \"\n",
      "              \"in the one percent case where it fails it's no biggie I  \\n\"\n",
      "              'just type one or two more characters and it always finds it '\n",
      "              \"after hitting enter to pick the item it'll inject the  \\n\"\n",
      "              'command into your prompt and then you can choose to run it you '\n",
      "              'can also hit control c to cancel the selection just like  \\n'\n",
      "              'the regular control r fcf can do a lot more than search your '\n",
      "              'history too you can use it to search your process list and  \\n'\n",
      "              \"more to be honest it deserves its own 45 minute talk it's a \"\n",
      "              'really really useful tool but now that we know how to search  \\n'\n",
      "              \"our history efficiently let's go back to the bashrc file we saw \"\n",
      "              'these lines a few minutes ago this controls enabling and  \\n'\n",
      "              'configuring fcf the bottom line enables fcf and the two '\n",
      "              'optional lines above it configure it fcf supports both dark '\n",
      "              'and  \\n'\n",
      "              'light themes and you can also customize the colors however you '\n",
      "              'see fit but the line above that is a bit more interesting  \\n'\n",
      "              'to talk about it configures fcf to use a tool called rip grep '\n",
      "              'instead of the grep tool when it searches for matches rip  \\n'\n",
      "              'rep is a much faster version of grep although in practice I do '\n",
      "              \"use grep on the command line because it's fast enough but  \\n\"\n",
      "              'fuzzy searching is pretty demanding and I find rip grip speed '\n",
      "              'boost to be worth it it narrows down search results pretty  \\n'\n",
      "              'much as fast as you can type even with thousands of files this '\n",
      "              'is important because the even plugin for fc up will use  \\n'\n",
      "              'rip rep when searching for files to open or searching your '\n",
      "              'project for specific text during the demo I ran the rg  \\n'\n",
      "              'command which uses rip grip under the hood and fed the results '\n",
      "              'into fcf using a vim plugin installing rickrep is easy  \\n'\n",
      "              'because every major operating system has a version of it in its '\n",
      "              'package manager but I recommend holding off on  \\n'\n",
      "              \"installing it right now because there's a couple of other tools \"\n",
      "              \"covered in my rc file that we'll want to install and  \\n\"\n",
      "              \"it's going to be a lot easier to install them after we talk \"\n",
      "              \"about dot files since that's all documented in one spot and  \\n\"\n",
      "              \"finally there's a bit of configuration in my bashrc file \"\n",
      "              'related to wsl2 and wsl1 this mainly sets up an x server  \\n'\n",
      "              'display so you can share your clipboard or run graphical apps '\n",
      "              \"between wsl and regular Windows I don't want to spend a  \\n\"\n",
      "              \"ton of time on this one but if you're using wsl the reference \"\n",
      "              'has a link to a video that goes over this in more detail  \\n'\n",
      "              \"so that's how I have bash configured I know it was a lot to \"\n",
      "              'take in but we just condensed multiple months of research in  \\n'\n",
      "              'about 10 minutes or however long that took now before we get '\n",
      "              \"into using tmox I do want to cover one last thing that's  \\n\"\n",
      "              'important when writing your own scripts when writing shell '\n",
      "              'scripts you might have seen something like what we see here  \\n'\n",
      "              \"this is called a shebang and it's the first line in the file \"\n",
      "              'that starts with pound exclamation point followed by a path  \\n'\n",
      "              'to some type of binary such as  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:48:00,559::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              'sh bash or any programming runtime that can be called from the '\n",
      "              'command line you may have also seen this style of a  \\n'\n",
      "              \"shebang which technically works on most systems but it's \"\n",
      "              \"considered to be less portable it's less portable because \"\n",
      "              'the  \\n'\n",
      "              'bash binary might not exist in slash bin on some systems '\n",
      "              'whereas user bin env exists almost everywhere I tend to use  \\n'\n",
      "              'this style unless I made a mistake and use the less portable '\n",
      "              'version by accident speaking of portability remember when  \\n'\n",
      "              'we talked about using sh instead of bash for scripting well the '\n",
      "              'barn shell aka sh is posix compliant this is a standard  \\n'\n",
      "              'that was created to ensure compatibility between different '\n",
      "              'operating systems to write a posix compliant shell script '\n",
      "              'you  \\n'\n",
      "              'would reference sh instead of bash in practice I try to do that '\n",
      "              'but sometimes you really need the extra features  \\n'\n",
      "              'provided by bash in which case I would use bash without giving '\n",
      "              'it a second thought honestly we could spend two hours  \\n'\n",
      "              \"going over the subtle differences between the two so let's just \"\n",
      "              'leave it at that you can always Google for the  \\n'\n",
      "              'differences later with that said I think one of the best ways '\n",
      "              'to learn something is to see fully working examples so  \\n'\n",
      "              \"I've dropped a bunch of links in the references to a number of \"\n",
      "              \"scripts I've open sourced over the years to help me in my  \\n\"\n",
      "              \"day-to-day now I'm not saying it's the best code ever written \"\n",
      "              'but it might get your noodle cooking to see what types of  \\n'\n",
      "              'programs or problems you can solve on the command line all of '\n",
      "              \"the code is MIT licensed too so that's shells and  \\n\"\n",
      "              \"scripting in a nutshell on that note we covered a lot but don't \"\n",
      "              'feel compelled to learn everything upfront before you do  \\n'\n",
      "              'anything you can slowly introduce things on a need to know '\n",
      "              'basis what I mean by that is like any programming language '\n",
      "              'or  \\n'\n",
      "              'environment you can still be very productive without '\n",
      "              'understanding the entire language and ecosystem in detail next '\n",
      "              'up  \\n'\n",
      "              \"let's go over using tmux unlike the shell we'll spend the \"\n",
      "              'majority of our time using tmox instead of configuring it '\n",
      "              'but  \\n'\n",
      "              'we will take a look at some config settings too first up what '\n",
      "              \"is tmox good question I'm glad you asked tmux is a  \\n\"\n",
      "              'terminal multiplexer and what that means is it lets you create '\n",
      "              'and control multiple terminals from a single screen we  \\n'\n",
      "              'saw that before in the demo when creating Windows and splits it '\n",
      "              'also lets you create sessions that you can attach and  \\n'\n",
      "              'detach from which is amazing because it means if you close your '\n",
      "              'terminal everything will still be running in the  \\n'\n",
      "              'background and you can connect to it or connect to any session '\n",
      "              \"at any time and it'll just resume where you left off  \\n\"\n",
      "              \"before you'll find tmox in whatever package manager your \"\n",
      "              \"operating system uses so it's going to be no problem to \"\n",
      "              'install  \\n'\n",
      "              \"it but like before let's not worry about installing it until we \"\n",
      "              \"talk about dot files near the end for now let's just  \\n\"\n",
      "              'focus on using it one core feature of tmux is its ability to '\n",
      "              \"control most things with hotkeys but don't worry you can  \\n\"\n",
      "              'usually use the mouse for a lot of things too like selecting '\n",
      "              'text or copying text or clicking different Windows or split  \\n'\n",
      "              'panes but since tmox is so high key driven they introduce the '\n",
      "              'idea of a thing called a prefix key the basic idea is  \\n'\n",
      "              \"you'll hit this key along with some other key afterwards to \"\n",
      "              'perform a specific action this helps namespace all of your  \\n'\n",
      "              \"tmx hotkeys so you don't end up overriding hotkeys from other \"\n",
      "              'tools by the way I also tend to call this a leader key  \\n'\n",
      "              \"because that's what vim defines it as so you may hear me use \"\n",
      "              \"both terms interchangeably by default it's bound to control  \\n\"\n",
      "              \"b which in my opinion is a bit funky to hit so I've remapped my \"\n",
      "              \"prefix key to be the back tick that's usually under the  \\n\"\n",
      "              'escape key or to the left of the one on a standard keyboard in '\n",
      "              \"practice you'll find a lot of folks use this key it's  \\n\"\n",
      "              'pretty easy to access and you really type it in your day to day '\n",
      "              'and when you do need to insert a backtick somewhere then  \\n'\n",
      "              \"you can just hit the key twice it's not really a big deal \"\n",
      "              \"throughout the rest of this section I'll always reference  \\n\"\n",
      "              'hotkeys with the word prefix instead of using the backtick '\n",
      "              'directly since you could in theory be using a different key  \\n'\n",
      "              'by default when you open a terminal tmox is not going to be '\n",
      "              'running we need to run it manually by running the tmux  \\n'\n",
      "              \"command after running it you'll know you're inside of tmux \"\n",
      "              \"because you'll see a status bar on the bottom of your  \\n\"\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:52:00,800::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              'terminal out of the box the status bar is going to look '\n",
      "              \"different than what we see here but we'll go over how to  \\n\"\n",
      "              'configure that a bit later on I made mine pretty minimal to '\n",
      "              'maximize usable space while giving me just enough  \\n'\n",
      "              'information I care about from here we have access to a bunch of '\n",
      "              'tmox goodies for example if you hit prefix c you can  \\n'\n",
      "              'create a new window notice on the bottom we have two Windows '\n",
      "              'now instead of one and both of them are named bash because  \\n'\n",
      "              \"that's the shell I'm running so let's say we opened a couple of \"\n",
      "              'Windows like we see here and it starts to get confusing  \\n'\n",
      "              \"because we're not sure what's running in each window well if \"\n",
      "              'you hit prefix comma tmux will bring up its command mode  \\n'\n",
      "              'and now we can rename the window by typing in whatever we want '\n",
      "              'with a new name you can also switch to a different window  \\n'\n",
      "              'by hitting leader 2 or whatever number you want to switch to '\n",
      "              \"the window you're currently looking at will have a star  \\n\"\n",
      "              'next to it which makes it easy to identify at a glance by '\n",
      "              'default tmux will order Windows starting at zero but in my  \\n'\n",
      "              'tmux config I changed it to start at one because in my opinion '\n",
      "              \"it's a lot easier to hit backtick one rather than  \\n\"\n",
      "              'backtick zero also if you have mouse mode enabled you can click '\n",
      "              'any one of those Windows on the bottom to switch it and  \\n'\n",
      "              'yep I do have mouse mode enabled in my config can everyone read '\n",
      "              \"that yeah just kidding so as an aside I'm not trying to  \\n\"\n",
      "              \"cover every tmux feature here there's other ways to switch \"\n",
      "              \"between Windows 2 but I'm trying to focus on what you might  \\n\"\n",
      "              'use in your day-to-day tmux has this help menu that you can '\n",
      "              'bring up with prefix question Mark to learn all about its  \\n'\n",
      "              \"different binds and that's kind of what we see here the funny \"\n",
      "              'thing is I zoomed out to try to fit all of this in one  \\n'\n",
      "              \"screenshot but there's still five lines above this there's also \"\n",
      "              'the man pages too and plenty of cheat sheets available  \\n'\n",
      "              'if you Google around with that said I included a link to a '\n",
      "              'cheat sheet if you want to check that out in the references  \\n'\n",
      "              'so going back to our example before if you want to split a '\n",
      "              'window in half you can use prefix b to open a split below '\n",
      "              'you  \\n'\n",
      "              \"but that's not the default bind instead I made it match the \"\n",
      "              \"same bindings as my vim config and there's really no limit  \\n\"\n",
      "              \"to how many times you can split a window you're basically \"\n",
      "              'limited by your monitor screen resolution in practice I use  \\n'\n",
      "              'split panes all the time and you can switch between them using '\n",
      "              'the mouse or if you use my tmx config you can also use  \\n'\n",
      "              'alt and the upper down arrow depending on which direction you '\n",
      "              'want to go alternatively you can also split your Windows  \\n'\n",
      "              'vertically this time around you can use prefix v which also '\n",
      "              \"matches my vim keys binds or bim key bindings also while I'm  \\n\"\n",
      "              'not showing it here you can combine horizontal and vertical '\n",
      "              'splits together however you see fit you can even drag the  \\n'\n",
      "              \"bar around with your mouse to resize the panes there's resize \"\n",
      "              \"hotkeys for that too but personally I don't even know what  \\n\"\n",
      "              \"they are off the top of my head because I just don't use them \"\n",
      "              'at all next up if you do find yourself having a bunch of  \\n'\n",
      "              'splits open but you want to temporarily zoom into one of them '\n",
      "              'without closing the rest of your splits then you can do  \\n'\n",
      "              \"that with prefix z you can tell if you're zoomed in because \"\n",
      "              \"it'll show a z next to the window name on the bottom this is  \\n\"\n",
      "              \"something I do all the time as well it's especially handy if \"\n",
      "              \"you're creating tutorial videos with a large font size and  \\n\"\n",
      "              'you just want to zoom into something to explain one window but '\n",
      "              'I still do use this in my day to day with a regular font  \\n'\n",
      "              \"size too now there's a lot more we can do with Windows and \"\n",
      "              'paints but this goes back to the 80 20 rule which is  \\n'\n",
      "              'basically focusing on the 80 which is going to give you the '\n",
      "              'most benefits versus how much time you spend in other words  \\n'\n",
      "              'what we just covered goes a really really long ways for dealing '\n",
      "              \"with Windows and split panes next up let's go over  \\n\"\n",
      "              'another killer feature of tmux which are sessions if you hit '\n",
      "              'prefix d that is going to detach you from your current  \\n'\n",
      "              \"session we didn't go over this yet but when we ran tmux before \"\n",
      "              'it started a session for us behind the scenes and it  \\n'\n",
      "              \"named that session zero since we didn't define a name that \"\n",
      "              'means if we close our terminal we could reattach to it and  \\n'\n",
      "              'things will be back as if we never left if we ever forget '\n",
      "              \"what's running we can also use the tmux ls command  \\n\"\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 00:56:01,119::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              'to get a list of sessions in our case we have just the one here '\n",
      "              'then if we run the attach command along with a specific  \\n'\n",
      "              'session name we can attach to a specific session right now our '\n",
      "              'session is named zero but we can choose to rename it to  \\n'\n",
      "              \"something else which we'll do in a few it's also worth pointing \"\n",
      "              \"out that if you run tmux attach with no arguments it'll  \\n\"\n",
      "              'attach you to the last used session which is something I do all '\n",
      "              \"the time after running that attach command we're back to  \\n\"\n",
      "              \"our session from before and it's like we never left what's \"\n",
      "              'really useful is that you can have multiple sessions running  \\n'\n",
      "              \"at once and switch between them at will that's a game changer \"\n",
      "              'if you have a terminal focused development environment  \\n'\n",
      "              \"speaking of multiple sessions if you're already in a team up \"\n",
      "              'session you can create a new session by running prefix  \\n'\n",
      "              'colon to bring up command mode and then you can run the new '\n",
      "              \"session command I'm passing in a custom session name of cool  \\n\"\n",
      "              'here but you can name it whatever you want technically you can '\n",
      "              'make a new session without naming it but I find in  \\n'\n",
      "              'practice that naming your sessions is totally worth it '\n",
      "              \"especially when you're dealing with a few sessions as for \"\n",
      "              'this  \\n'\n",
      "              'animation right after the session is created we can see that '\n",
      "              \"we're put into a new session with only one window but our  \\n\"\n",
      "              \"old session is still running in the background also if you're \"\n",
      "              'curious most commands that you can run in command mode can  \\n'\n",
      "              'also be run directly on the command line although in this case '\n",
      "              'if you try to create a new session like this when already  \\n'\n",
      "              'in a session tmox is going to prevent you from doing it by '\n",
      "              'default otherwise it gets too confusing with nested sessions  \\n'\n",
      "              \"however when you run new session in command mode that doesn't \"\n",
      "              'nest the new session with an existing session it creates a  \\n'\n",
      "              \"completely new one I know that's a little confusing but that's \"\n",
      "              'how it works as far as I know the takeaway here is you  \\n'\n",
      "              \"should always spawn new sessions from command mode if you're \"\n",
      "              'already inside a session or you can just detach from your  \\n'\n",
      "              'current session and make a new session from the command line as '\n",
      "              'for switching between sessions you can hit prefix s this  \\n'\n",
      "              'gives you a list of all of your sessions and then you can use '\n",
      "              'the arrow keys to pick the one that you want and switch to  \\n'\n",
      "              \"it by hitting enter upon doing so you'll be put into that \"\n",
      "              \"session immediately what's really cool is as you cycle \"\n",
      "              'through  \\n'\n",
      "              \"the list of sessions you'll get to see a preview of what's \"\n",
      "              'running in the lower half of the window that could be very  \\n'\n",
      "              \"handy to see what's running in a session you can even click \"\n",
      "              'directly into one of those boxes and jump straight into that  \\n'\n",
      "              \"sessions window next up let's say you want to search up and \"\n",
      "              'down your buffer with the way I have things configured you  \\n'\n",
      "              'can use the mouse wheel to scroll up in your buffer and that '\n",
      "              \"puts you into tmox's copy mode you can identify that by the  \\n\"\n",
      "              'orange label in the top right which shows the number of lines '\n",
      "              'in the buffer then you can hit the forward slash key to  \\n'\n",
      "              'begin searching type in your term and hit enter from here tmux '\n",
      "              'will highlight all the matches and you can press m n as a  \\n'\n",
      "              'Nick to jump to the next match or shift end to go in reverse '\n",
      "              'this is what I meant by tmok super charging your terminal  \\n'\n",
      "              'suddenly we can do all of this with any terminal now with the '\n",
      "              'way I have things configured if you select any text with  \\n'\n",
      "              \"the mouse and let go with the mouse button it'll copy that text \"\n",
      "              \"to your clipboard that's due to using a tmux plugin  \\n\"\n",
      "              'called yank you can even use the mouse wheel or the page or up '\n",
      "              'down keys to quickly select a bunch of text that is  \\n'\n",
      "              'multiple pages long copy pasting text is a pretty common thing '\n",
      "              \"to do so I'm sure you'll be using that one once in a  \\n\"\n",
      "              \"while I know I do next up it's worth pointing out that if you \"\n",
      "              'open two separate terminals you can connect both of them  \\n'\n",
      "              'to the same session when you do this your actions are going to '\n",
      "              'be mirrored in both terminals that includes both typing  \\n'\n",
      "              'and switching Windows by default tmux will also resize the '\n",
      "              'viewport of both terminals to be whatever the smaller one is  \\n'\n",
      "              'to be honest I am not sure why it does this by default but you '\n",
      "              'can change this behavior for example if you attach to the  \\n'\n",
      "              'session like this along with setting one config option then you '\n",
      "              'can connect to the same session but each terminal will  \\n'\n",
      "              'have its own independent view of a specific window this is very '\n",
      "              'very handy if you use multiple monitors it means you can  \\n'\n",
      "              'have a second terminal open on your other monitor and look at a '\n",
      "              \"different window on the same session that's running on  \\n\"\n",
      "              \"your main monitor I do this quite often when I'm working on \"\n",
      "              'projects that require seeing many things at once lastly  \\n'\n",
      "              \"let's say you're detached from tmux and you want to stop all of \"\n",
      "              'your sessions you can do that by running tmux kill  \\n'\n",
      "              \"server that's going to completely kill tmux destroy all of your \"\n",
      "              'sessions and stop any processes that were running in the  \\n'\n",
      "              'foreground inside of tmox that could be handy if you get busy '\n",
      "              'with not naming your sessions and after a few weeks of  \\n'\n",
      "              'using tmux without rebooting you wind up with having 20 unnamed '\n",
      "              'sessions and 50 copies of him running been there done  \\n'\n",
      "              'that although as for rebooting by default you will lose all of '\n",
      "              'your sessions after your reboot on Linux this might not  \\n'\n",
      "              'be too bad because you know you might not reboot for six months '\n",
      "              'but on Windows this is painful because Windows will  \\n'\n",
      "              'reboot your machine whenever it feels like it but there is a '\n",
      "              \"silver lining there's a plugin called tmux resurrect that  \\n\"\n",
      "              'will let you save and restore your sessions after a reboot even '\n",
      "              \"if the tmx server is killed with that said let's quickly  \\n\"\n",
      "              'go over a few interesting parts of my tmox config for the sake '\n",
      "              \"of time we're not going to spend as much time as we did  \\n\"\n",
      "              'going over the shell related configs my config is pretty well '\n",
      "              'commented so it should be fairly easy for you to take a  \\n'\n",
      "              'look at it in more detail on your own like the other config '\n",
      "              'files will show a little bit at a time and this is how you  \\n'\n",
      "              \"can customize your prefix key there's not much to it if you \"\n",
      "              'wanted to use something other than the backtick then you  \\n'\n",
      "              'would change it here next up this one is super interesting this '\n",
      "              'is the setting that allows you to get independent  \\n'\n",
      "              'Windows when connecting to the same session multiple times '\n",
      "              \"apparently resizing isn't enough you need to aggressively  \\n\"\n",
      "              'resize to get your independence these settings control the '\n",
      "              'status bar I explicitly removed everything except showing '\n",
      "              'the  \\n'\n",
      "              'window number and labels by default tmx shows a clock on the '\n",
      "              'bottom right side of the status bar I have that disabled  \\n'\n",
      "              'with comments if you Google around for tmok status bar '\n",
      "              \"configuration you'll find a bunch of examples on what you can \"\n",
      "              'do  \\n'\n",
      "              'for example you can add things like your network ip address and '\n",
      "              \"more that could be handy if you're connecting to  \\n\"\n",
      "              'multiple remote machines these settings work together to make '\n",
      "              'tmox a bit more human friendly for example if you had four  \\n'\n",
      "              'Windows and you closed window 2 then tmux by default will leave '\n",
      "              'a numbered gap what I mean by that is you would end up  \\n'\n",
      "              'with Windows 1 3 and 4 being available the first setting '\n",
      "              'removes the gaps so you would end up with Windows 1 2 and 3.  \\n'\n",
      "              'the other settings on the bottom make tmux start numbering '\n",
      "              'Windows at 1 instead of 0. next up you can quickly reload  \\n'\n",
      "              \"your tmux config by hitting prefix r that's handy to do if \"\n",
      "              \"you're tweaking your tmux config and you want to see the  \\n\"\n",
      "              'changes without having to kill your tmux server this is similar '\n",
      "              'to sourcing your bashrc file to see the changes without  \\n'\n",
      "              'opening a new terminal lastly tmux has plugins and the tool to '\n",
      "              'manage them is called tpm that stands for tmux plugin  \\n'\n",
      "              \"manager in case you're wondering it's something you'll need to \"\n",
      "              \"install and it's included in my dot files documentation  \\n\"\n",
      "              'the yank plugin copies selected text to your clipboard and the '\n",
      "              'resurrect plugin lets you save and restore tmok sessions  \\n'\n",
      "              'even if your tmux server dies you can do that by hitting prefix '\n",
      "              'control s to save your sessions and prefix control r to  \\n'\n",
      "              \"restore them once you're inside any existing tmx session and \"\n",
      "              \"that's all you have to do this plugin is awesome especially  \\n\"\n",
      "              \"if you're on Windows now let's talk a little bit about them and \"\n",
      "              'unlike the tmx session this is going to be more like a  \\n'\n",
      "              'rapid-fire description of a bunch of vin plug-ins that I use in '\n",
      "              \"my day-to-day that's because to really really really get  \\n\"\n",
      "              'into vim would totally require its own dedicated talk instead '\n",
      "              \"of a few minutes if you're curious I do have over 15  \\n\"\n",
      "              \"videos on YouTube covering a bunch of vim topics I've linked to \"\n",
      "              'them in the reference notes lastly I do run regular vim  \\n'\n",
      "              \"not neovim but all the plugins that we're about to go over will \"\n",
      "              'work with vim 8.1 or above and neovim as well speaking  \\n'\n",
      "              'of plugins  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 01:04:00,559::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              \"we'll need to install a tool to use them then vim has many many \"\n",
      "              \"different choices for how to install plugins since it's  \\n\"\n",
      "              \"been around for so long the one I use is called plug and you'll \"\n",
      "              'see how to install it once we get into the dot files  \\n'\n",
      "              'section none of these plugins are in any specific order but '\n",
      "              \"let's start with the ones that we've seen during this talk  \\n\"\n",
      "              'also the code snippets you see here are straight from my vim '\n",
      "              'config file as for fcf it allows us to fuzzy search files  \\n'\n",
      "              \"git commits and more it's one of my favorite plugins fern \"\n",
      "              \"allows you to quickly visualize your project's directory  \\n\"\n",
      "              'structure and it makes it really easy to rename copy or delete '\n",
      "              'files and directories it even supports bulk renaming  \\n'\n",
      "              'files and even buffer for doing complex renaming this could '\n",
      "              'come in really handy if you want to re-reorganize how you  \\n'\n",
      "              'label media files or whatever you might be doing next up '\n",
      "              'signify is a super efficient plugin for displaying get '\n",
      "              'changes  \\n'\n",
      "              \"like what we saw during the Markdown preview demo that's where \"\n",
      "              'we saw the orange exclamation point it also uses a green  \\n'\n",
      "              'plus symbol for new lines and a red dash for deleted lines then '\n",
      "              \"for previewing Markdown I use this plugin it's labeled  \\n\"\n",
      "              'as being for neovim but it still works with the vim 8.1 or '\n",
      "              'above this is the plugin that requires having node installed  \\n'\n",
      "              'also we saw this plug-in display Markdown that looks like '\n",
      "              'GitHub styles but you can very easily change that by '\n",
      "              'swapping  \\n'\n",
      "              'a CSS file in case you happen to be using gitlab bitbucket or '\n",
      "              'you just want your Markdown style differently for csv  \\n'\n",
      "              \"files there's not much to say about this one other than it \"\n",
      "              'works really well by the way I have about 20 other plugins  \\n'\n",
      "              'related to syntax highlighting for all the different tools and '\n",
      "              \"languages they use on a regular basis that's one of the  \\n\"\n",
      "              'reasons why I really like vim it lets me use the same tool for '\n",
      "              'everything multi-snips and vim snippets really help with  \\n'\n",
      "              'providing shortcuts for inserting code most editors have some '\n",
      "              'type of snippet support and these work really nicely in  \\n'\n",
      "              'vim as for this other plugin it tells vim to automatically pop '\n",
      "              \"up its code complete window while you type now if you've  \\n\"\n",
      "              'ever used sublime text that makes it very similar to how you '\n",
      "              \"can autocomplete anything you've typed in any open buffer  \\n\"\n",
      "              'the cool thing about this one is none of these plugins require '\n",
      "              'running a language server if you happen to know what that  \\n'\n",
      "              'is this is more for a lightweight approach to get useful but '\n",
      "              \"not hyper intelligent autocomplete that's context aware  \\n\"\n",
      "              'moving on this one lets you zoom in and out of splits and vim '\n",
      "              'just like you can with tmox this comes in very handy if  \\n'\n",
      "              \"you're someone who likes to open a bunch of splits now all of \"\n",
      "              'these plugins work together to serve the end goal of  \\n'\n",
      "              'making it easier for you to find search and replace text I use '\n",
      "              'these all the time especially the visual star and vim  \\n'\n",
      "              \"gripper plugins there's a 30 minute video linked in the \"\n",
      "              'references that goes over how to do various find and replace  \\n'\n",
      "              'workflows in vim it ranges from searching for a character in '\n",
      "              'one line to doing a regular expression based find and  \\n'\n",
      "              \"replace across multiple files lastly there's two themes that I \"\n",
      "              'really enjoy using both of them have very good syntax  \\n'\n",
      "              'highlighting for many popular languages including Python and r '\n",
      "              'I used one dark for all the screenshots in this talk and  \\n'\n",
      "              'I typically use it when recording videos and then I switch over '\n",
      "              'to one light for personal use prior to that I used  \\n'\n",
      "              'drevbox for almost two years and I still really like that theme '\n",
      "              'but like most developers we get bored of themes over  \\n'\n",
      "              'time so I like to mix it up but I could see myself switching '\n",
      "              'back to it in the future besides plugin plugins related to  \\n'\n",
      "              \"syntax highlighting there's still about 15 or so plugins that I \"\n",
      "              \"didn't mention which are still worth using for very  \\n\"\n",
      "              \"specific things now I don't quite have 200 plugins but I would \"\n",
      "              'say I have about 45 or 50 and I do use them on a regular  \\n'\n",
      "              \"basis I'm very protective of which plugins I add and I also \"\n",
      "              'focus on keeping things as efficient as possible that means  \\n'\n",
      "              'not using plugins that introduce typing delays or other hitches '\n",
      "              'with that said all the plugins I use are listed in my  \\n'\n",
      "              'vmrc file in my dot files with a one line comment explaining '\n",
      "              \"which each one does and that's going to wrap things up for  \\n\"\n",
      "              \"vim so now let's talk a little bit about dot  \\n\"\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 01:08:01,359::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              \"files we've already seen a few such as the profile bashrc file \"\n",
      "              'and tmux config files when it comes to configuring user  \\n'\n",
      "              'settings for command line tools it is very common to put them '\n",
      "              \"into various dot files it's not a guaranteed role but most  \\n\"\n",
      "              'command line tools will use files that begin with a dot for its '\n",
      "              \"configuration and there's three main places where they  \\n\"\n",
      "              'might exist some tools will write out their config files into '\n",
      "              \"your user's home directory which is what we saw before  \\n\"\n",
      "              \"with a bashrc file tmux config and a few others it's also quite \"\n",
      "              'popular for tools to create a dot directory in your home  \\n'\n",
      "              'directory named after that tool for example if we take a look '\n",
      "              \"at my home directory there's a few tools that do that this  \\n\"\n",
      "              'usually happens when a tool has one or more config file but not '\n",
      "              'always sometimes a tool will place a bunch of config  \\n'\n",
      "              'files unrelated to its configuration along with its '\n",
      "              \"configuration all in one spot it's also pretty popular for \"\n",
      "              'tools to  \\n'\n",
      "              'place their config files within their own directory within the '\n",
      "              \"dot config directory inside of your home directory that's  \\n\"\n",
      "              'a lot of directories this is a newer standard based on '\n",
      "              'something called the xdg-based directory specification which '\n",
      "              'is  \\n'\n",
      "              'linked to in the reference notes if you want to learn more '\n",
      "              'about conventions around where certain files should live and  \\n'\n",
      "              \"that's what we can see here if I happen to write any scripts \"\n",
      "              'that need one or more config files I always use the config  \\n'\n",
      "              \"directory since it's a standard so that's how configuration is \"\n",
      "              'mainly handled but I sort of simplified where I store my  \\n'\n",
      "              'individual dot files to make the previous slides easier to read '\n",
      "              'in reality most of my dot files are sim-linked to a  \\n'\n",
      "              \"separate directory that's contained within a get repo you can \"\n",
      "              'see that here with the arrow pointer which is a sim link  \\n'\n",
      "              'sim links are references to another file or directory you can '\n",
      "              \"sort of think of them as shortcuts if you're coming from  \\n\"\n",
      "              \"the Windows world now they're not specific to dot files but \"\n",
      "              \"they're often used together with that file so that you can  \\n\"\n",
      "              'save your dat files in one centralized location and then sim '\n",
      "              'link them to wherever they need to go in your home  \\n'\n",
      "              'directory and that leads us into one way of managing your dot '\n",
      "              \"files I'm a fan of having a single directory somewhere on  \\n\"\n",
      "              'my system such as this dot files repo in my GitHub folder and '\n",
      "              'then I can sim link the config files to where they need to  \\n'\n",
      "              'go this keeps things simple and it makes it easy to replicate '\n",
      "              'my setup on another machine or share it with others all  \\n'\n",
      "              'you would have to do is clone down this repo and then set up '\n",
      "              'the sim links if all you wanted to do is replicate my  \\n'\n",
      "              'configs but sim links are not the only way to manage your dot '\n",
      "              \"files there's dozens of tools focused on dot files  \\n\"\n",
      "              'management but honestly I never felt the need to go that far a '\n",
      "              'really popular one is called yet another dot files  \\n'\n",
      "              \"manager and I've dropped a link to that one in the references \"\n",
      "              \"it's basically an abstraction on top of git and it comes  \\n\"\n",
      "              'with its own custom command line tool to manage your dat files '\n",
      "              'if you find yourself gravitating towards wanting to use a  \\n'\n",
      "              \"dot files manager I would start with this one I haven't used it \"\n",
      "              'personally but I do know someone who is happily using it  \\n'\n",
      "              'that someone has been working on the command line for 15 plus '\n",
      "              'years and he looked at a lot of other tools so I trust his  \\n'\n",
      "              \"recommendation with that said let's take a look at a few key \"\n",
      "              'parts of my dot files repo on GitHub in addition to the  \\n'\n",
      "              'config files themselves I included screenshots and '\n",
      "              'documentation related to installing the tools I use on a few '\n",
      "              'popular  \\n'\n",
      "              \"Linux distros and Mac OS there's not really an unwritten rule \"\n",
      "              'to include installation instructions in your dot files  \\n'\n",
      "              'repo but I decided to go the extra mile I included them mainly '\n",
      "              'because I have a few programming courses and I wanted a  \\n'\n",
      "              'single place where I could point folks too in case they wanted '\n",
      "              'to replicate or cherry pick a few things for my setup for  \\n'\n",
      "              'example if you wanted to install tmux vim and another a number '\n",
      "              'of other tools you could copy paste one of the package  \\n'\n",
      "              \"installation commands depending on what os you're using in this \"\n",
      "              \"case it's the debian and ubuntu installation  \\n\"\n",
      "              \"instructions but in the readme file there's copy pasteable \"\n",
      "              \"commands for brew in case you're using Mac OS you can also  \\n\"\n",
      "              'choose to add or remove anything you see fit for example jq is '\n",
      "              \"a command line tool that lets you parse json if you don't  \\n\"\n",
      "              'care about that then you  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 01:12:00,159::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              'can easily remove it then for installing the dot files '\n",
      "              \"themselves it's a matter of cloning this repo and then setting \"\n",
      "              'up  \\n'\n",
      "              'the sim links you could choose to grab all of my configs or '\n",
      "              'just the ones that you care about for example if you find  \\n'\n",
      "              'yourself using z shell instead of bash you could remove my '\n",
      "              \"bashrc file or if you're not using wsl you can remove the  \\n\"\n",
      "              'last sim link since that only applies to wsl also if you want '\n",
      "              'more control over your setup and want a very personalized  \\n'\n",
      "              'solution you could always skim through my individual configs '\n",
      "              'and copy paste whatever you want directly into your config  \\n'\n",
      "              \"files beyond that there's a bunch of commands like this where \"\n",
      "              'you can pick and choose to install whatever tools you want  \\n'\n",
      "              \"this quite a bit more than what's shown here but the rest are \"\n",
      "              'documented in the same way and all of these commands will  \\n'\n",
      "              \"work on Linux and Mac OS since it's mainly running curl get and \"\n",
      "              \"pip commands there's even reminders like this in the  \\n\"\n",
      "              \"readme file for installing plugins for vim and tmox there's \"\n",
      "              'also a section up top to help verify everything is set up  \\n'\n",
      "              \"correctly lastly it's worth mentioning that these dot dot files \"\n",
      "              \"are going to evolve over time in the references I've  \\n\"\n",
      "              'linked to the exact commits for all the files that we went over '\n",
      "              'but I do recommend checking out the master branch on  \\n'\n",
      "              \"your own because if you're watching this talk in the future \"\n",
      "              \"chances are I've updated a number of things which are  \\n\"\n",
      "              \"probably improvements in some way or another so that's the tldr \"\n",
      "              'on dot files if you Google around for fails GitHub  \\n'\n",
      "              \"you'll find thousands of other examples I'm not sure how many \"\n",
      "              'of them will have tons of documentation but at least now  \\n'\n",
      "              \"you're familiar with the concept and that's kind of the main \"\n",
      "              'takeaway of this talk I tried my best to showcase and go  \\n'\n",
      "              \"over a bunch of tools but ultimately it's up to you to apply as \"\n",
      "              'much or as little as you want in your daily workflow  \\n'\n",
      "              'like anything that takes time to build up muscle memory '\n",
      "              'especially for hitting hotkeys that might be foreign to you '\n",
      "              'I  \\n'\n",
      "              'know it took me a long time to get used to them and I still '\n",
      "              \"feel like I have a lot to learn in other words don't get  \\n\"\n",
      "              'discouraged if you struggle for a while it definitely gets '\n",
      "              'better over time and on that note hopefully at the very '\n",
      "              'least  \\n'\n",
      "              'a couple of unknown unknowns were discovered today usually when '\n",
      "              \"I watch a talk or read a book I'm hoping to walk away  \\n\"\n",
      "              'with at least one nugget of information then I can apply it '\n",
      "              'back to my day-to-day and that is all I have thanks a lot  \\n'\n",
      "              'for having me and if anyone is still awake and we still have '\n",
      "              \"some time I'm more than happy to answer any questions cue  \\n\"\n",
      "              'awkward silence not really sure what I need to do beyond this '\n",
      "              'point oh can you hear me now OK if anybody has any  \\n'\n",
      "              'questions please feel free to post in the chat or in the Q&A I '\n",
      "              'want to thank Nick for his presentation yeah that was a  \\n'\n",
      "              'lot of great information I did have a question for you there '\n",
      "              'are a number of different these are called vim is called  \\n'\n",
      "              \"the terminal editor right there's a number of different ones \"\n",
      "              'what are your thoughts on some of the other ones OK by the  \\n'\n",
      "              \"way this is a little bit weird you're actually coming through \"\n",
      "              'on headphones sitting on my desk instead of the one in my  \\n'\n",
      "              \"ear so I can barely barely hear you but I'm pretty sure you \"\n",
      "              'asked me like how to compare different terminal emulators  \\n'\n",
      "              \"like what's the difference between using next term and alacrity \"\n",
      "              'or kitty or like 50 other ones it really comes down to  \\n'\n",
      "              'preference right like tmux really helps you out when it comes '\n",
      "              'to using different features like splitting Windows and  \\n'\n",
      "              'panes and searching so I just try to really go for the terminal '\n",
      "              'that that is basically the fastest and the ones listed  \\n'\n",
      "              'in that other slide were kind of the ones that I prefer but you '\n",
      "              'can always just Google around for you know like alacrity  \\n'\n",
      "              'versus kitty versus hyper or whatever terminal that you want '\n",
      "              'and check out its features OK great  \\n'\n",
      "              ' \\n'\n",
      "              '\\n'\n",
      "              '#### 01:16:00,080::\\t\\t4 minutes mark -> new paragraph \\n'\n",
      "              ' \\n'\n",
      "              'somebody asked is asking a question on check can we review the '\n",
      "              \"video if you're referring to this video yes I will post  \\n\"\n",
      "              'it pretty soon on the Data Umbrella YouTube sorry to interrupt '\n",
      "              \"this is crazy like I can't hear you at all so I'm going  \\n\"\n",
      "              'to switch over to different headphones OK yeah these are also a '\n",
      "              'lot uglier and bigger I use these to edit my videos but  \\n'\n",
      "              'they work very nicely can you hear me now yeah much better OK '\n",
      "              'all right yeah so reviewing the video the video will be  \\n'\n",
      "              \"posted pretty soon and we'll send an announcement somebody else \"\n",
      "              'says I really enjoyed learning about tmux as an unknown  \\n'\n",
      "              'unknown thanks for the informative talk yes and actually you '\n",
      "              'know I for tmux I wrote up a I have like a written up thing  \\n'\n",
      "              'that I did when I was taking the fastai deep learning best day '\n",
      "              'deep learning course and so I can share that as well but  \\n'\n",
      "              \"yeah tmux is very very it's really really useful I'd say yeah \"\n",
      "              \"yeah it's one of those tools where it's like until you  \\n\"\n",
      "              'realize that it exists like how to live live my life without '\n",
      "              \"that yeah it is I mean I'm gonna let me just see if I can  \\n\"\n",
      "              'find it pretty quickly it was it certainly was very very '\n",
      "              'popular yeah if anybody has any other questions feel free to  \\n'\n",
      "              'ask now while Nick is available and yeah yeah do you remember '\n",
      "              \"one question that popped up last time was like well let's  \\n\"\n",
      "              \"say I don't want to use vim like let's say I want to use vs \"\n",
      "              'code instead like is the command line still useful and like  \\n'\n",
      "              'totally for sure like vs code if you wanted to jump between '\n",
      "              'different projects you can totally do that as a built-in  \\n'\n",
      "              'terminal like you know the real takeaway is just using the '\n",
      "              \"command line in general right it's not tied to specifically  \\n\"\n",
      "              \"using vim right OK that's great and I guess the other thing is \"\n",
      "              'maybe this is a good time to share your YouTube channel  \\n'\n",
      "              \"do you want to share that in the link Nick ah I don't even have \"\n",
      "              \"that chat open I can't even bring it up otherwise it's  \\n\"\n",
      "              'going to like interfere with some things here but I can search '\n",
      "              'YouTube is it yeah right if you just search for my name  \\n'\n",
      "              \"it's there well I hope there might be other Nick gen attackers \"\n",
      "              \"out there but no it's true you have 10 and a half  \\n\"\n",
      "              'thousand subscribers is that right OK do you have a custom URL '\n",
      "              \"for your YouTube funny enough I don't remember it but I'm  \\n\"\n",
      "              \"pretty sure it's just YouTube.com c nicktoon attackers OK all \"\n",
      "              \"right cool also if you went to my home page here then it's  \\n\"\n",
      "              \"in the about page on the bottom ah OK cool yeah there's a lot \"\n",
      "              'of a lot of helpful stuff there OK so if there are no  \\n'\n",
      "              \"other questions I am going to we're going to complete the \"\n",
      "              'webinar once again thank you so much return no problem yeah  \\n'\n",
      "              'also one heads up about that YouTube channel there is a vim and '\n",
      "              'tmux playlist so if all you care about is that then I  \\n'\n",
      "              'would check that out ah wonderful cool all right thank you yeah '\n",
      "              'no problem  \\n'\n",
      "              ' '),\n",
      "             ('year', '2021'),\n",
      "             ('idn', '01'),\n",
      "             ('video_url', 'https://youtu.be/y4fYxmE0HZM'),\n",
      "             ('title_kw', 'command'),\n",
      "             ('transcript_md', '01-nick-janetakis-command.md'),\n",
      "             ('audio_track',\n",
      "              WindowsPath('C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/resources/EventManagement/data/meta/2021_01_y4fYxmE0HZM.mp4')),\n",
      "             ('audio_text',\n",
      "              WindowsPath('C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/resources/EventManagement/data/meta/2021_01_y4fYxmE0HZM.txt')),\n",
      "             ('has_transcript', True),\n",
      "             ('trans_idx', 1760),\n",
      "             ('status', 'Partial (new editor requested)'),\n",
      "             ('notes', ''),\n",
      "             ('video_embed',\n",
      "              '\\n'\n",
      "              '<iframe width=\"560\" height=\"315\" \\n'\n",
      "              '        '\n",
      "              'src=\"https://www.youtube-nocookie.com/embed/y4fYxmE0HZM?cc_load_policy=1&autoplay=0\" \\n'\n",
      "              '        frameborder=\"0\">\\n'\n",
      "              '</iframe>\\n')])\n"
     ]
    }
   ],
   "source": [
    "pp(first_yr_21.event_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Test: text wrap fn as externally defined TW obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08-cat-reshama-audio-demo.md\n"
     ]
    }
   ],
   "source": [
    "# Instantiate existing event\n",
    "idn, year = 8, 2021\n",
    "\n",
    "tm = Meta.TranscriptMeta(idn, year)\n",
    "print(tm.event_dict['transcript_md'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4131"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_trx = tm.event_dict['formatted_transcript']\n",
    "new_txt = tm.redo_transcript_cleanup(formatted_trx)\n",
    "\n",
    "i = new_txt.find('Bayesian')\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<!-- Editing Guide: The pipe (|) position in this comment is 120:                                                       | -->\n",
       "### Introduction\n",
       "\n",
       "Okay hello and welcome to Data Umbrella's webinar for October so I'm just going to go over the agenda I'm going to do a  \n",
       "brief introduction then there will be the workshop by Hugo and James and you can ask questions along the way in the chat  \n",
       "or actually the best place to ask questions is the Q&A and there's an option to upvote as well so yet asking the Q&A if  \n",
       "you happen to post it on the chat by mistake I can also transfer it over to Q&A so that would be fine too and this  \n",
       "webinar is being recorded briefly about me I am a statistician and data scientist and I am the founder of Data Umbrella  \n",
       "I am on a lot of platforms as Reshama so feel free to follow me on Twitter and LinkedIn we have a code of conduct we're  \n",
       "dedicated to providing harassment free experience for everyone thank you for helping to make this a welcoming friendly  \n",
       "professional community for all "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/markdown": [
       "<!-- Editing Guide: The pipe (|) position in this comment is 120:                                                       | -->\n",
       "### Introduction\n",
       "\n",
       "Okay hello and welcome to Data Umbrella's webinar for October so I'm just going to go over the agenda I'm going to do a  \n",
       "brief introduction then there will be the workshop by Hugo and James and you can ask questions along the way in the chat  \n",
       "or actually the best place to ask questions is the Q&A and there's an option to upvote as well so yet asking the Q&A if  \n",
       "you happen to post it on the chat by mistake I can also transfer it over to Q&A so that would be fine too and this  \n",
       "webinar is being recorded briefly about me I am a statistician and data scientist and I am the founder of Data Umbrella  \n",
       "I am on a lot of platforms as Reshama so feel free to follow me on Twitter and LinkedIn we have a code of conduct we're  \n",
       "dedicated to providing harassment free experience for everyone thank you for helping to make this a welcoming friendly  \n",
       "professional community for all "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(formatted_trx[:1024])\n",
    "Markdown(new_txt[:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"\"\"I gave either at the end of February or early March was data umbrella's inaugural  \n",
    "tutorial and Meetup if I recall correctly on bayesian thinking and hacker statistics and simulation and  \n",
    "that type of stuff so it's just wonderful to be back particularly with my colleague and friend James we're  \n",
    "building really cool distributed data science products at coiled we'll say a bit about that but we'll do some  \n",
    "introductions in a bit I just wanted to get you all accustomed to it was February thank you Reshama we're working \"\"\"\n",
    "\n",
    "formatted_trx == new_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update text_processing files (see How_Tos.ipynb)\n",
    "Do these with gui:\n",
    "```\n",
    "# For the 'upper' file, textbox entry is list of lower-case names/acronyms to upper-case\n",
    "upper\tnyc\n",
    "# For the 'corrections' file, textbox entry is a list of tuples:\n",
    "corrections\t [('ipad', iPad'),('iphone', iPhone'),('coyle','coil'),('job lib', 'joblib'),('dars', 'Dask')]\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_people = ['theodore', 'alex']\n",
    "TRX.update_conversions('people', new_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "uppercase_list = TRX.readcsv(TRX.upper_file).upper.tolist()\n",
    "titlecase_list = (TRX.readcsv(TRX.people_file).people.tolist()\n",
    "                + TRX.readcsv(TRX.names_file).names.tolist()\n",
    "                + TRX.readcsv(TRX.places_file).places.tolist())\n",
    "corrections = TRX.get_corrections_dict()\n",
    "\n",
    "current_txt = tm.event_dict['formatted_transcript']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Test: New transcript\n",
    "## 5 steps to new transcript:\n",
    "```\n",
    "# 0. Instantiate new event\n",
    "tm = Meta.TranscriptMeta()\n",
    "\n",
    "# 1. Get dummy or actual data:\n",
    "# Load the starter kv pairs\n",
    "demo_kvs = Meta.dummy_kv_pairs.copy()\n",
    "# Modify at will\n",
    "demo_kvs.append(('transcriber', 'cat chenal'))\n",
    "d = Meta.get_dummy_data(new_kv_pairs=demo_kvs)\n",
    "\n",
    "# 2. update starter event_dict:\n",
    "tm.update_dict(d)\n",
    "# updated:\n",
    "#tm.event_dict\n",
    "\n",
    "# 3. update_readme()\n",
    "tm.update_readme()\n",
    "\n",
    "# 4. save_transcript_md()\n",
    "tm.save_transcript_md()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK: New event in current year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Instantiate new event\n",
    "tm = Meta.TranscriptMeta()\n",
    "\n",
    "# 1. Get dummy or actual data:\n",
    "# Load the starter kv pairs\n",
    "demo_kvs = Meta.dummy_kv_pairs.copy()\n",
    "# modify\n",
    "demo_kvs.append(('transcriber', 'max chenal'))\n",
    "d = Meta.get_dummy_data(new_kv_pairs=demo_kvs)\n",
    "\n",
    "# 2. update starter event_dict:\n",
    "tm.update_dict(d)\n",
    "# updated:\n",
    "#tm.event_dict\n",
    "\n",
    "# 3. update_readme()\n",
    "tm.update_readme()\n",
    "\n",
    "# 4. save_transcript_md()\n",
    "tm.save_transcript_md()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in Meta:\n",
    "dummy_kv_pairs = [('presenter', 'cat Chenal, Reshama Shaikh'),\n",
    "                  ('title','Automating Audio tanscription.'),\n",
    "                  ('title_kw','audio demo'),\n",
    "                  ('video_url','https://youtu.be/MHAjCcBfT_A'),\n",
    "                  ('extra_references', \n",
    "                  \"\"\"## Other References\\n\n",
    "                  - Binder:  <url>\\n- Paper:  <Paper url or citation>  \\n\n",
    "                  - Wiki:  This is an excellent [wiki]  \\n\n",
    "                  (http://en.wikipedia.org/wiki/Main_Page)  \\n\"\"\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK: New event in other year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2020', '21-cat-chenal-foo-demo.md')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Talk Transcript</th>\n",
       "      <th>Transcriber</th>\n",
       "      <th>Status</th>\n",
       "      <th>Notes</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>Hugo Bowne-Anderson</td>\n",
       "      <td>Bayesian Data Science</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>Not recorded</td>\n",
       "      <td></td>\n",
       "      <td>N.A.</td>\n",
       "      <td>N.A.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02</td>\n",
       "      <td>Bruno Goncalves</td>\n",
       "      <td>Time Series Modeling</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>Not recorded</td>\n",
       "      <td></td>\n",
       "      <td>N.A.</td>\n",
       "      <td>N.A.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03</td>\n",
       "      <td>Ty Shaikh</td>\n",
       "      <td>[Webscraping Poshmark](2020/03-ty-shaikh-websc...</td>\n",
       "      <td>Ty Shaikh</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>03-ty-shaikh-webscraping.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04</td>\n",
       "      <td>Ali Spittel</td>\n",
       "      <td>[Navigating Your Tech Career](2020/04-ali-spit...</td>\n",
       "      <td>Janine</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>04-ali-spittel-career.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05</td>\n",
       "      <td>Andreas Mueller</td>\n",
       "      <td>[Crash Course in Contributing to Scikit-learn]...</td>\n",
       "      <td>Reshama Shaikh</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>05-andreas-mueller-contributing.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>06</td>\n",
       "      <td>Reshama Shaikh</td>\n",
       "      <td>[Example PR for Scikit-learn](2020/06-reshama-...</td>\n",
       "      <td>Reshama Shaikh, Mark</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>06-reshama-shaikh-sklearn-pr.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>07</td>\n",
       "      <td>Shailvi Wakhlu</td>\n",
       "      <td>[Fixing Bad Data (Using SQL)](2020/07-shailvi-...</td>\n",
       "      <td>Juanita</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>07-shailvi-wakhlu-fixing-data.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08</td>\n",
       "      <td>Matt Brems</td>\n",
       "      <td>[Data Science with Missing Data](2020/08-matt-...</td>\n",
       "      <td>Barbara Graniello Batlle</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>08-matt-brems-missing-data.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>09</td>\n",
       "      <td>Sam Bail</td>\n",
       "      <td>[Intro to Terminal](2020/09-sam-bail-terminal.md)</td>\n",
       "      <td>Isaack</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>09-sam-bail-terminal.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Emily Robinson</td>\n",
       "      <td>[Build a Career in Data Science](2020/10-emily...</td>\n",
       "      <td>Kevin</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>10-emily-robinson-career.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Rebecca Kelly</td>\n",
       "      <td>[Kdb Time Series Database](2020/11-rebecca-kel...</td>\n",
       "      <td>Coretta</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td>Paragraphs are too long</td>\n",
       "      <td>2020</td>\n",
       "      <td>11-rebecca-kelly-kdb.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Mridu Bhatnagar</td>\n",
       "      <td>[Build a Bot](2020/12-mridu-bhatnagar-bot.md)</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>12-mridu-bhatnagar-bot.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Liz DiLuzio</td>\n",
       "      <td>[Creating Nimble Data Processes](2020/13-liz-d...</td>\n",
       "      <td>Lily</td>\n",
       "      <td>Complete</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>13-liz-diluzio-data-process.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Megan Robertson</td>\n",
       "      <td>[3 Lessons From 3 Years of Data Science](2020/...</td>\n",
       "      <td>Sethupathy</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td>Headers should not be in capital letters, etc</td>\n",
       "      <td>2020</td>\n",
       "      <td>14-megan-robertson-career.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Emma Gouillart</td>\n",
       "      <td>[Data Visualization with Plotly](2020/15-emma-...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>15-emma-gouillart-plotly.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Hugo Bowne-Anderson, James Bourbeau</td>\n",
       "      <td>[Data Science and Machine Learning at Scale](2...</td>\n",
       "      <td>Cynthia</td>\n",
       "      <td>Needs reviewer</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>16-hugo-james-dask.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Carol Willing</td>\n",
       "      <td>[Contributing to Core Python](2020/17-carol-wi...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>17-carol-willing-python.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Thomas Fan</td>\n",
       "      <td>[Streamlit for Data Science](2020/18-thomas-fa...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>18-thomas-fan-streamlit.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Matti Picus</td>\n",
       "      <td>[Contributing to NumPy](2020/19-matti-picus-nu...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>19-matti-picus-numpy.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Marco Gorelli</td>\n",
       "      <td>[Contributing to pandas](2020/20-marco-gorelli...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>20-marco-gorelli-pandas.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Cat Chenal</td>\n",
       "      <td>[Automating Audio Tanscription.](2020/21-cat-c...</td>\n",
       "      <td>Billy Bop</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>21-cat-chenal-foo-demo.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Nick Janetakis</td>\n",
       "      <td>[Creating a Command Line Focused Development E...</td>\n",
       "      <td>?</td>\n",
       "      <td>Partial (new editor requested)</td>\n",
       "      <td></td>\n",
       "      <td>2021</td>\n",
       "      <td>21-nick-janetakis-command.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Cat Chenal, Reshama Shaikh</td>\n",
       "      <td>[Automating Audio Transcription.](2021/22-cat-...</td>\n",
       "      <td>?</td>\n",
       "      <td>Not yet processed (editor needed)</td>\n",
       "      <td>Dummy entry for demo.</td>\n",
       "      <td>2021</td>\n",
       "      <td>22-cat-reshama-audio-foo.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     N                              Speaker  \\\n",
       "0   01                  Hugo Bowne-Anderson   \n",
       "1   02                      Bruno Goncalves   \n",
       "2   03                            Ty Shaikh   \n",
       "3   04                          Ali Spittel   \n",
       "4   05                      Andreas Mueller   \n",
       "5   06                       Reshama Shaikh   \n",
       "6   07                       Shailvi Wakhlu   \n",
       "7   08                           Matt Brems   \n",
       "8   09                             Sam Bail   \n",
       "9   10                       Emily Robinson   \n",
       "10  11                        Rebecca Kelly   \n",
       "11  12                      Mridu Bhatnagar   \n",
       "12  13                          Liz DiLuzio   \n",
       "13  14                      Megan Robertson   \n",
       "14  15                       Emma Gouillart   \n",
       "15  16  Hugo Bowne-Anderson, James Bourbeau   \n",
       "16  17                        Carol Willing   \n",
       "17  18                           Thomas Fan   \n",
       "18  19                          Matti Picus   \n",
       "19  20                        Marco Gorelli   \n",
       "20  21                           Cat Chenal   \n",
       "21  21                       Nick Janetakis   \n",
       "22  22           Cat Chenal, Reshama Shaikh   \n",
       "\n",
       "                                      Talk Transcript  \\\n",
       "0                               Bayesian Data Science   \n",
       "1                                Time Series Modeling   \n",
       "2   [Webscraping Poshmark](2020/03-ty-shaikh-websc...   \n",
       "3   [Navigating Your Tech Career](2020/04-ali-spit...   \n",
       "4   [Crash Course in Contributing to Scikit-learn]...   \n",
       "5   [Example PR for Scikit-learn](2020/06-reshama-...   \n",
       "6   [Fixing Bad Data (Using SQL)](2020/07-shailvi-...   \n",
       "7   [Data Science with Missing Data](2020/08-matt-...   \n",
       "8   [Intro to Terminal](2020/09-sam-bail-terminal.md)   \n",
       "9   [Build a Career in Data Science](2020/10-emily...   \n",
       "10  [Kdb Time Series Database](2020/11-rebecca-kel...   \n",
       "11      [Build a Bot](2020/12-mridu-bhatnagar-bot.md)   \n",
       "12  [Creating Nimble Data Processes](2020/13-liz-d...   \n",
       "13  [3 Lessons From 3 Years of Data Science](2020/...   \n",
       "14  [Data Visualization with Plotly](2020/15-emma-...   \n",
       "15  [Data Science and Machine Learning at Scale](2...   \n",
       "16  [Contributing to Core Python](2020/17-carol-wi...   \n",
       "17  [Streamlit for Data Science](2020/18-thomas-fa...   \n",
       "18  [Contributing to NumPy](2020/19-matti-picus-nu...   \n",
       "19  [Contributing to pandas](2020/20-marco-gorelli...   \n",
       "20  [Automating Audio Tanscription.](2020/21-cat-c...   \n",
       "21  [Creating a Command Line Focused Development E...   \n",
       "22  [Automating Audio Transcription.](2021/22-cat-...   \n",
       "\n",
       "                 Transcriber                             Status  \\\n",
       "0                       N.A.                       Not recorded   \n",
       "1                       N.A.                       Not recorded   \n",
       "2                  Ty Shaikh                     Needs reviewer   \n",
       "3                     Janine                     Needs reviewer   \n",
       "4             Reshama Shaikh                           Complete   \n",
       "5       Reshama Shaikh, Mark                           Complete   \n",
       "6                    Juanita                           Complete   \n",
       "7   Barbara Graniello Batlle                     Needs reviewer   \n",
       "8                     Isaack                           Complete   \n",
       "9                      Kevin                           Complete   \n",
       "10                   Coretta                     Needs reviewer   \n",
       "11                         ?  Not yet processed (editor needed)   \n",
       "12                      Lily                           Complete   \n",
       "13                Sethupathy                     Needs reviewer   \n",
       "14                         ?  Not yet processed (editor needed)   \n",
       "15                   Cynthia                     Needs reviewer   \n",
       "16                         ?  Not yet processed (editor needed)   \n",
       "17                         ?  Not yet processed (editor needed)   \n",
       "18                         ?  Not yet processed (editor needed)   \n",
       "19                         ?  Not yet processed (editor needed)   \n",
       "20                 Billy Bop  Not yet processed (editor needed)   \n",
       "21                         ?     Partial (new editor requested)   \n",
       "22                         ?  Not yet processed (editor needed)   \n",
       "\n",
       "                                            Notes  year  \\\n",
       "0                                                  N.A.   \n",
       "1                                                  N.A.   \n",
       "2                                                  2020   \n",
       "3                                                  2020   \n",
       "4                                                  2020   \n",
       "5                                                  2020   \n",
       "6                                                  2020   \n",
       "7                                                  2020   \n",
       "8                                                  2020   \n",
       "9                                                  2020   \n",
       "10                        Paragraphs are too long  2020   \n",
       "11                                                 2020   \n",
       "12                                                 2020   \n",
       "13  Headers should not be in capital letters, etc  2020   \n",
       "14                                                 2020   \n",
       "15                                                 2020   \n",
       "16                                                 2020   \n",
       "17                                                 2020   \n",
       "18                                                 2020   \n",
       "19                                                 2020   \n",
       "20                                                 2020   \n",
       "21                                                 2021   \n",
       "22                          Dummy entry for demo.  2021   \n",
       "\n",
       "                                  name  \n",
       "0                                 N.A.  \n",
       "1                                 N.A.  \n",
       "2          03-ty-shaikh-webscraping.md  \n",
       "3             04-ali-spittel-career.md  \n",
       "4   05-andreas-mueller-contributing.md  \n",
       "5      06-reshama-shaikh-sklearn-pr.md  \n",
       "6     07-shailvi-wakhlu-fixing-data.md  \n",
       "7        08-matt-brems-missing-data.md  \n",
       "8              09-sam-bail-terminal.md  \n",
       "9          10-emily-robinson-career.md  \n",
       "10             11-rebecca-kelly-kdb.md  \n",
       "11           12-mridu-bhatnagar-bot.md  \n",
       "12      13-liz-diluzio-data-process.md  \n",
       "13        14-megan-robertson-career.md  \n",
       "14         15-emma-gouillart-plotly.md  \n",
       "15               16-hugo-james-dask.md  \n",
       "16          17-carol-willing-python.md  \n",
       "17          18-thomas-fan-streamlit.md  \n",
       "18             19-matti-picus-numpy.md  \n",
       "19          20-marco-gorelli-pandas.md  \n",
       "20           21-cat-chenal-foo-demo.md  \n",
       "21        21-nick-janetakis-command.md  \n",
       "22         22-cat-reshama-audio-foo.md  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0. Instantiate new event\n",
    "tm = Meta.TranscriptMeta()\n",
    "\n",
    "# 1. Get dummy or actual data:\n",
    "# Load the starter kv pairs\n",
    "demo_kvs = Meta.dummy_kv_pairs.copy()\n",
    "# modify\n",
    "demo_kvs[0] = (demo_kvs[0][0], 'cat chenal')\n",
    "demo_kvs[2] = (demo_kvs[2][0], 'foo demo')\n",
    "demo_kvs.append(('transcriber', 'Billy Bop'))\n",
    "\n",
    "d = Meta.get_dummy_data(year=2020,\n",
    "                        new_kv_pairs=demo_kvs)\n",
    "\n",
    "# 2. update starter event_dict:\n",
    "tm.update_dict(d)\n",
    "# updated:\n",
    "tm.event_dict['year'], tm.event_dict['transcript_md']\n",
    "\n",
    "# 3. update_readme()\n",
    "tm.update_readme()\n",
    "\n",
    "# 4. save_transcript_md()\n",
    "tm.save_transcript_md()\n",
    "\n",
    "tm.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, tbl_delims = Meta.df_from_readme_tbl()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test: update of existing transcript: need to keep same year & idn\n",
    "\n",
    "##  steps to update transcript:\n",
    "```\n",
    "# 0. Instantiate existing event\n",
    "year = 2021\n",
    "idn = '06'  # or 6\n",
    "tm = Meta.TranscriptMeta(idn, year)\n",
    "\n",
    "# 1. Get dummy or actual data, keeping idn:\n",
    "# Load the starter kv pairs\n",
    "demo_kvs = Meta.dummy_kv_pairs.copy()\n",
    "demo_kvs.append(('idn', idn))\n",
    "\n",
    "# Modify at will\n",
    "demo_kvs.append(('transcriber', 'sing song'))\n",
    "# change kw to get new file:\n",
    "demo_kvs[2] = (demo_kvs[2][0], 'better demo')\n",
    "\n",
    "d = Meta.get_dummy_data(new_kv_pairs=demo_kvs)\n",
    "\n",
    "# 2. update starter event_dict:\n",
    "tm.update_dict(d)\n",
    "tm.to_delete  # should not be None\n",
    "\n",
    "# 3. update_readme()\n",
    "tm.update_readme()\n",
    "\n",
    "# 4. save_transcript_md()\n",
    "tm.save_transcript_md()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2021', '06')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0. Instantiate existing event\n",
    "year = 2021\n",
    "idn = '06'  # or 6\n",
    "tm = Meta.TranscriptMeta(idn, year)\n",
    "\n",
    "\n",
    "# 1. Get dummy or actual data, keeping idn:\n",
    "\n",
    "# Load the starter kv pairs\n",
    "demo_kvs = Meta.dummy_kv_pairs.copy()\n",
    "demo_kvs.append(('idn', idn))\n",
    "# Modify at will\n",
    "demo_kvs.append(('transcriber', 'mae song'))\n",
    "# change kw to get new file:\n",
    "demo_kvs[2] = (demo_kvs[2][0], 'better demo')\n",
    "\n",
    "d = Meta.get_dummy_data(new_kv_pairs=demo_kvs)\n",
    "d['year'], d['idn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/2021/06-cat-reshama-new-demo.md')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. update starter event_dict:\n",
    "tm.update_dict(d)\n",
    "tm.to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. update_readme()\n",
    "tm.update_readme()\n",
    "\n",
    "# 4. save_transcript_md()\n",
    "tm.save_transcript_md()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# DONE: Test: Trap missing tbl delimiters in README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Audit: coverage of split_url\n",
    "RE: regex in get_id_from_YT_url not working for all md files\n",
    "## Q1: Is `parse_href` working? Yes\n",
    "## Q2: Is video_url working? Yes\n",
    "# Conclusion: Updated (fixed) regex in get_id_from_YT_url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test meetup...\n",
      "Test youtube...\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "UTL.test_split_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Audit: which xml files are not lowercase?\n",
    "- Answer by testing 1st paragraph => Modified `xml_caption_to_text` to obtain `Audit.audit_xml_captions`\n",
    "\n",
    "## Audit conclusion:\n",
    "The xml files have consistently been lowercase since event 12, hence this does not warrant implementing\n",
    "of a by-pass to text cleaning if they are not (the corrections would still need applying but they would not\n",
    "be optimal without adding special cases if text is not lowercase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUDIT ALL EVENTS\n",
      "* Replacement of xml files selected.\n",
      "* Captions case check (on 1st P with minutes_mark= 1):\n",
      "03, 2020:: Lower= False\n",
      "everyone I am doing a recording of the scraping presentation and the original recording from the webinar didn't come out well so this is just a recording in today's presentation I'm going to talk about web scraping we're going to look at the website Poshmark comm and we're going to use Python and some additional packages to gather the data so agenda I'm gonna give a quick introduction about myself and the group then we're going to talk about web scraping and high level then we'll walk through a code example I'm going to share the code files so you can walk through it on your own as well and then during the webinar there was obviously QA it's a little bit about me I'm a product manager with General Assembly I used to run operations at an online data science bootcamp and that's kind of where I picked up everything I know about Python and programming and data science and \n",
      ")\n",
      "04, 2020:: Lower= False\n",
      "all right it's 7:05 so I'll do my intra bit and we can happen to yours hey everyone my name is ty I will be doing an intro quick and true and then Ali will be giving her presentation if you have questions there's a QA function and zoom so you can ask the quench questions there if I can answer it I'll help answer if not we will run through them at the end okay so I'm just gonna give a quick introduction then I will talk and then we'll do QA this will be recorded so if something happens you have to leave or someone you know wasn't able to attend the recording will automatically be sent out by General Assembly usually I think it's within a day but it might be two days so just an intro on how a she teaches people how to code she loves Python JavaScript and talking about code and she's always been interested in programming art and education she's on the distinguished \n",
      ")\n",
      "05, 2020:: Lower= False\n",
      "hey everybody so this is an instruction video on how to contribute to source projects in particular to cycle learn I'm andreas Miller one of the core developers of recycle or project thanks for Reshma and the data umbrella for organizing this sprint so I really want to give you just a very brief overview of the technology behind contributing to open source and the steps of getting your first cohesions in so first off a great way to communicate with the developers is the getter channel you can find that at get heard I am slashed so I could learn for a spread there is this channel called sprint and there's also a cycle to learn channel that's just a general channel that you can find that Gator that I am slash scikit-learn slash I could learn \n",
      ")\n",
      "06, 2020:: Lower= False\n",
      "hello my name is Rashmi and I'm going to go through an example of submitting a pull request or a PR I participated in my first scikit-learn sprint about a year and a half ago and I'm happy to share an example once you learn this example is gonna be for the SK learn repo but once you've learned how to do it for this repository you can do it for any repository on github so the first thing that I'm going to do is I'm just gonna make sure that I have my I have some things set up and the first thing is I'm working out of my home directory guys I just want to make sure and see which which Python I am using with the system is of anaconda I am using the anaconda version which is good I also want to just confirm what version of Python I'm using and it's version 3 6 8 and I just want to check one more thing which is do I have get installed and I do that's great so the next thing that \n",
      ")\n",
      "07, 2020:: Lower= False\n",
      "okay so it's it's really great my name is Rashmi Shaikh and I am the founder of data umbrella I am also an organizer for NYC hi ladies and I am based out of New York City welcome to our welcome to our session the mission of data umbrella is to provide a welcoming and educational space for underrepresented persons in data science we welcome allies to help us in our mission and we welcome all different skill levels we are on twitter at theta umbrella so feel free to tweet and share about the event and follow us also there's a LinkedIn page for us where I usually share job posting so you're welcome to join there and there's information on the website about a discord for community I just want to go over the code of conduct that you know we're dedicated to providing harassment free experience for everyone be kind to others be professional and respectful we really want to build a friendly \n",
      ")\n",
      "08, 2020:: Lower= False\n",
      "Oh you you you you if you have a you can use the QA function so if you're on zoom and you hover over the video it should be on the bottom below the video it looks like QA and you can ask questions there and then we'll spend some time to answer them we do a break in the middle of the talk and then at the end of the talk as well you \n",
      ")\n",
      "09, 2020:: Lower= False\n",
      "there you go okay everybody welcome to data umbrella and PI ladies online event just sort of let you know how well did it how the event is gonna go I'm gonna do an introduction about the meetup group Sam Bell is going to give a talk and we'll have Q&A will also sort of watch the Q&A every 10 minutes or so and answer questions along the way just to let you know this event is being recorded about me I'm a statistician data scientist I founded data umbrella and I'm also a hi ladies organizer a New York City chapter and I am on twitter at reached is the mission of data umbrella is to provide a welcoming and educational space for underrepresented persons in the fields of data science our website has information we're on twitter if you'd like to share about this event and just know that we are a \n",
      ")\n",
      "10, 2020:: Lower= True\n",
      "hi everyone welcome to data umbrella webinar um also co-promoted with nyc pi ladies i'm going to do a brief introduction and then turn it over to emily for her presentation um and then after that what you could do is you can place any questions in the q a and um at the end uh emily will answer questions from them but just and just a reminder to reiterate this talk is being recorded about me um i'm a statistician and data scientist um i'm the founder of data brella and i'm also an nyc pie ladies organizer the mission of data umbrella is to provide a welcoming and educational space for underrepresented persons in the field of data science and machine learning we welcome allies who support our cause our home page is dataumbrella.org check it out we're also on twitter and we are a volunteer run organization \n",
      ")\n",
      "11, 2020:: Lower= False\n",
      "everybody welcome to data umbrellas webinar I'm just gonna go over a brief introduction so I'm gonna introduce the umbrella of Rebecca Kelly's going to do her talk and then you can ask questions in the Q&A tab or you can ask in the chat and I'll just move the questions over to the Q&A tab and depending on how the questions come in we can sort of I made into a home turf Rebecca if it's a good time to interrupt her but we'll get the questions answered and this webinar is being recorded about me I am the founder of data umbrella I'm a statistician by training and a data scientist and I also organize for the New York City chapter of Pi ladies and I'm on Twitter evasion is the mission of data umbrella is to provide a welcoming education inclusive space for underrepresented persons in data science and we are a volunteer run organization \n",
      ")\n",
      "12, 2020:: Lower= True\n",
      "hey everybody welcome to a data umbrella webinar um i am joining um from new york and uh i'm just gonna do a a brief introduction about data umbrella and then we'll get started on the um webinar um so it'll be it'll sort of go like i'll do the introduction um we'll do the talk and then you can post any questions in the chat or in the q a and we'll sort of answer those as they pile up over time and just to let people know this will be recorded and will be available on our youtube i posted a link to youtube in the chat if you're not able to see it just let me know and i can share it again about me i'm the founder of data umbrella i'm a statistician data scientist and i am also an organizer for the new york city chapter of \n",
      ")\n",
      "13, 2020:: Lower= True\n",
      "hey everybody welcome to data umbrella's webinar um thanks for joining us um i'm going to go over um a brief introduction about data umbrella and then liz is going to do her talk and then um for the q a for the questions you can feel free to post them in chat or in the q a tab and i'll sort of moderate the session and ask the questions as i can you know find a good place to interrupt liz as she's presenting now this webinar will be is being recorded actually okay a little bit about me i am a statistician and data scientist um i am based in new york city i am the founder of data umbrella and i'm also an organizer for the new york city chapter of pi ladies you can find me on twitter with at reishmaest and i'm also on github and linkedin with um with the same username data umbrella our mission is to provide a welcoming and educational space for \n",
      ")\n",
      "14, 2020:: Lower= True\n",
      "hi everyone welcome to data umbrella's webinar uh my name is reshma and i'm just going to go over some some housekeeping um the way that the webinar will work tonight is that i do a brief introduction megan is going to be doing um her talk and then we'll um what you can do is there's a tab on the webinar platform for q a so feel free to post any questions there and when you know when it's a good breaking point megan will answer any questions that you have this webinar is being recorded about me uh i am a statistician slash data scientist i am the founder of data umbrella and i'm also an organizer for the new york city chapter of pi ladies and you can find me on twitter at reshma s i'm also on linkedin and github with the same username \n",
      ")\n",
      "15, 2020:: Lower= True\n",
      "hi everybody welcome to data umbrella um i'm going to just go over the agenda of how the webinar is going to go i'm going to do a brief introduction emmanuelle um will do her presentation on plotly and you can ask questions on the q a tab and so we'll sort of check questions when it's a good time to stop um but not to worry your questions will get answered some of them might be at the end but we will answer all questions and this webinar is being recorded a little bit about myself i'm a statistician data scientist i'm the founder of data umbrella and i'm also an organizer for the new york city chapter of pi ladies you can find me on twitter github and linkedin as reishmas data umbrella is our mission is to provide inclusive and welcoming space for underrepresented persons in data science \n",
      ")\n",
      "16, 2020:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "17, 2020:: Lower= True\n",
      "hello everyone thank you for joining our webinar for today uh thanks for joining data umbrella i'm gonna do a quick introduction uh carol willing is going to do her talk and we'll have a q a session at the end and and this webinar is being recorded a little bit about me i'm a statistician data scientist i'm the founder of data umbrella and i am on twitter linkedin github has raised my s feel free to follow me we have a code of conduct we're dedicated to providing harassment free professional respectful experience for everyone this applies to the chat as well um thank you for helping make this a welcoming and friendly community for all of us about data umbrella we're an inclusive community for underrepresented persons in data science we welcome allies to join us and we are a volunteer run organization so how can you support data umbrella the \n",
      ")\n",
      "18, 2020:: Lower= True\n",
      "hello everyone and welcome to data umbrella's webinar i'm going to do a brief presentation um i'm going to do a quick introduction um the talk is going to be there and we are going to have a q a at the end this talk is being recorded if you have any questions there's a q a tab on this platform so if you could post questions there that's a good place to aggregate them if you happen to place them in the chat i can also easily you know transfer them over to q a but it is easier if you post them in the q a tab this webinar is being recorded a little bit about me briefly i'm a statistician data scientist i'm a founder i am the founder of data umbrella and i am on twitter linkedin and github as reishima so feel free to follow me um if you would like we have a code of conduct uh we're quite strict with our code of conduct because one of the reasons that this community was created is to provide a safe and inclusive environment for people from underrepresented groups \n",
      ")\n",
      "19, 2020:: Lower= True\n",
      "hello everybody welcome to data umbrella's webinar so um our our processes usually i do a quick introduction um maddie will do his talk and you know her q a um if you have questions you can on chat or you can post them in the q a tab i can easily move questions from the chat over to the q a so that's fine um and this webinar is being recorded and will be available on youtube um usually within a couple of days but sometimes a week depending on how much editing has to be done a quick introduction about myself i i'm the founder of data umbrella i'm a statistician data scientist by training and i am available on twitter linkedin and github at rachmas so feel free to follow me if you'd like we have a code of conduct here uh we're dedicated to providing harassment free experience for everyone this applies to \n",
      ")\n",
      "20, 2020:: Lower= True\n",
      "hello everyone and welcome to data umbrella's webinar our our agenda is going to be i'm going to do a quick introduction and then marco will be doing his talk and we have q a at the end this is actually a special webinar because what we will do after marco's presentation is we're going to go over to discord and if anybody wants to set up their environment we have somebody to help people who are viewing this reporting after today is tuesday december 15th a little bit about data umbrella we are an inclusive community for underrepresented persons in data science and we are a volunteer run organization a little bit about me i'm a statistician data scientist i have a master's in statistics and um let me just get my slides back i'm going to uh okay \n",
      ")\n",
      "21, 2020:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "22, 2020:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "23, 2020:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "24, 2020:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "01, 2021:: Lower= True\n",
      "hello everyone and welcome to our data umbrella webinar this is our first webinar of 2021. i'm going to do a quick introduction and then nick will do his talk and we'll open it up for q a you can also ask questions in the question tab throughout the presentation and we'll sort of break whenever it's a good time to break and this webinar is being recorded data umbrella is an inclusive community for underrepresented persons in data science and we are volunteer run a brief couple of things about me i'm a statistician data scientist and i have an ms in masters in statistics and an mba from nyu and i am the founder of data umbrella um if you are interested in learning more about what i'm doing you can follow me on twitter linkedin or github i have the same handle raishma s we have a code of conduct here and we're \n",
      ")\n",
      "02, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "03, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "04, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "05, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "06, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "07, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "08, 2021:: Lower= True\n",
      "okay hello and welcome to data umbrella's webinar for um october so i'm just going to go over the agenda i'm going to do a brief introduction then there will be the workshop by hugo and james and you can ask questions along the way in the chat or actually the best place to ask questions is the q a and there's an option to upvote as well um so yet um asking the q a if you happen to post it on the chat by mistake i can also transfer it over to q a so that would be fine too and this webinar is being recorded uh briefly about me i am a statistician and data scientist and i am the founder of data umbrella um i am on a lot of platforms as rachmas so feel free to follow me on twitter and linkedin we have a code of conduct we're dedicated to providing harassment free experience for everyone \n",
      ")\n",
      "get_all_transcripts [meta_only=False, audit_captions=True, replace_xml=True, replace_trx=False]:: done!\n"
     ]
    }
   ],
   "source": [
    "AUD.audit_all_events(audit_captions=True,\n",
    "                     replace_xml=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# DONE: Test: Change GrispecLayout (\"a regulary-spaced grid\": missed that!) to GridBox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# DONE: Incorporate modification to propercasing files in Edit page + reprocess\n",
    "**Note**: This _might_ disappear once punctuation is restored with an NLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# DONE: Added NullHandler: if not DEBUG_MODE, no Output widget created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f526e41be1477f908141d1cb7fc084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black', height='160px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55a1b87660e4a329d434d5452f70a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AppLayout(children=(Accordion(children=(VBox(children=(ToggleButtons(button_style='info', options=('Enter Info"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AC = CTR.AppControls()  # class, GUI controls instantiation\n",
    "gui = AC.app            # AppLayout method\n",
    "gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2126ff8c5eda45168e9a20a5d245ae8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'name': 'stdout', 'text': 'Update dict: OK!\\nUpdate readme: OK!\\nSave: Done!\\n', 'output_type"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gui.center.children[1].children[0].clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': '2021',\n",
       " 'presenter': 'Cat Chenal, Reshama Shaikh',\n",
       " 'title': 'Automating Audio Tanscription.',\n",
       " 'title_kw': 'audio demo',\n",
       " 'video_url': 'https://youtu.be/MHAjCcBfT_A',\n",
       " 'video_href': 'http://www.youtube.com/watch?feature=player_embedded&v=MHAjCcBfT_A',\n",
       " 'video_href_src': 'http://img.youtube.com/vi/MHAjCcBfT_A/0.jpg',\n",
       " 'video_href_alt': 'Automating Audio Tanscription.',\n",
       " 'event_url': 'N.A.',\n",
       " 'slides_url': 'N.A.',\n",
       " 'repo_url': 'N.A.',\n",
       " 'notebook_url': 'N.A.',\n",
       " 'transcriber': 'Mama Chenal',\n",
       " 'status': 'Not yet processed (editor needed)',\n",
       " 'notes': 'N.A.',\n",
       " 'extra_references': '## Other References\\n- Paper:  <Paper url or citation> \\n'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_form = AC.PC.page.children[1].children[0]\n",
    "        \n",
    "d = CTR.get_accordion_entries(input_form)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('presenter', 'Cat Chenal, Reshama Shaikh'),\n",
       "             ('title', 'Automating Audio Tanscription.'),\n",
       "             ('event_url', 'N.A.'),\n",
       "             ('yt_video_id', 'MHAjCcBfT_A'),\n",
       "             ('slides_url', 'https://www.example.com'),\n",
       "             ('repo_url', 'https://www.example.com'),\n",
       "             ('notebook_url', 'N.A.'),\n",
       "             ('transcriber', 'Bibi Chenal'),\n",
       "             ('extra_references',\n",
       "              '## Other References\\n- Binder:  url \\n- Paper:  Paper url or citation \\n- Wiki:  This is an excellent [wiki](http://en.wikipedia.org/wiki/Main_Page) \\n'),\n",
       "             ('video_href',\n",
       "              'http://www.youtube.com/watch?feature=player_embedded&v=MHAjCcBfT_A'),\n",
       "             ('video_href_src', 'http://img.youtube.com/vi/MHAjCcBfT_A/0.jpg'),\n",
       "             ('video_href_alt', 'Automating Audio Tanscription.'),\n",
       "             ('video_href_w', '25%'),\n",
       "             ('formatted_transcript',\n",
       "              \"<!-- Editing Guide: The pipe (|) position in this comment is 120:                                                       | -->\\n### Introduction\\n\\nOkay hello and welcome to Data Umbrella's webinar for October so I'm just going to go over the agenda I'm going to do a  \\nbrief introduction then there will be the workshop by Hugo and James and you can ask questions along the way in the chat  \\nor actually the best place to ask questions is the Q&A and there's an option to upvote as well so yet asking the Q&A if  \\nyou happen to post it on the chat by mistake I can also transfer it over to Q&A so that would be fine too and this  \\nwebinar is being recorded briefly about me I am a statistician and data scientist and I am the founder of Data Umbrella  \\nI am on a lot of platforms as Reshama so feel free to follow me on Twitter and LinkedIn we have a code of conduct we're  \\ndedicated to providing harassment free experience for everyone thank you for helping to make this a welcoming friendly  \\nprofessional community for all and this code of conduct applies to the chat as well so our mission is to provide an  \\ninclusive community for underrepresented persons in data science and we are an all volunteer run organization you can  \\nsupport Data Umbrella by doing the following things you can follow our code of conduct and keep our community a place  \\nwhere everybody wants to keep coming to you can donate to our open collective and that helps to pay meet-up dues and  \\nother operational costs and you can check out this link here on GitHub we have this new initiative where all the videos  \\nare being transcribed and so is to make them more accessible so we take the YouTube videos and we put the raw there and  \\nso we've had a number of volunteers help us transcribe it so feel free to check out this link and maybe if you do this  \\nvideo maybe the two speakers will follow you on Twitter I can't promise anything but it's possible Data Umbrella has a  \\njob board and it's at jobs.org and once this gets started I'll put some links in the chat the job that we are  \\nhighlighting today is is the machine learning engineer job by development seed and development seat is based in  \\nWashington DC and Lisbon Portugal and they do I'm going to go to the next slide what they do is they're doing social  \\ngood work and so they're doing for instance mapping elections from Afghanistan to the US analyzing public health and  \\neconomic data from Palestine to Illinois and leading the strategy and development behind data world bank and some other  \\norganizations and I will share a link to their job posting in the chat as well as soon as I finish this brief  \\nintroduction check out our website for resources there's a lot of resources on learning Python and r also for  \\ncontributing to open source also for guides on accessibility and responsibility and allyship we have a monthly  \\nnewsletter that goes out towards the end of the month and it has information on our upcoming events we have two great  \\nevents coming up in November and December on open source so subscribe to our newsletter to be in the know we are on all  \\nsocial media platforms as Data Umbrella Meetup is the best place to join to find out about upcoming events our website  \\nhas resources follow us on Twitter we also share a lot of information on LinkedIn and if you want to subscribe to our  \\nYouTube channel we record all of our talks and post them there within about a week of the talk so it's a good way to get  \\ninformation OK and now we are ready to get started  \\n \\n\\n#### 00:04:03,120::\\t\\t4 minutes mark -> new paragraph \\n \\nso I will hand it over to put myself on mute and I will hand it over to Hugo and James and let you take over but thank  \\nyou all for joining I just want to thank Reshama Christina and and everyone else who tied all the tireless effort that  \\nthat goes into putting these meet-ups and these online sessions together I I think one thing I want to say is actually  \\nthe the last in-person workshop I gave either at the end of February or early March was data umbrellas in inaugural  \\ntutorial and Meetup if I if I recall correctly on on bayesian bayesian thinking and hacker statistics and simulation and  \\nthat type of stuff so it's it's just wonderful to be back particularly with my colleague and friend friend James we're  \\nbuilding really cool distributed data science products at coiled we'll say a bit about that but we'll do some  \\nintroductions in in a bit I just wanted to get you all accustomed to it was February thank you Reshama we're working  \\nwith Jupyter notebooks in a GitHub repository the repository is pinned to the top of the chat this is what it looks like  \\nthese are all the files this is the file system now we use something called Binder which is a project out of and related  \\nto project project Jupyter which provides infrastructure to run notebooks without any local installs so there are two  \\nways you can you can code along on this tutorial the first is and I won't get you to do this yet is to launch Binder the  \\nreason I won't get you to do that yet is because once you launch it we have 10 minutes to start coding or the Binder  \\nsession times out I've been burnt by that before actually several times I'm surprised I even remembered it this time the  \\nother thing you can do is install everything locally by cloning the repository downloading anaconda creating a conda  \\nenvironment if you haven't done that I suggest you do not do that now and you launch launch the Binder James is going to  \\nstart by telling us a few a few things about about GAs and distributed compute in general my question for you James is  \\nif we get people to launch this now will we get to execute a cell code cell in 10 minutes I would let's hold off for now  \\nmaybe yep maybe I'll indicate when we should launch Binder OK fantastic cool and just what I'm looking at right now is  \\nthe GitHub repository on your browser OK exactly so I will not launch Binder now I will not get you to now I've I'm  \\ndoing this locally and we see that I'm in notebook zero and if you want to actually have a look at this notebook before  \\nlaunching Binder it's in the notebooks Data Umbrella subdirectory and its notebook zero and we're going to hopefully  \\nmake it through the overview then chatting about Dask Dask delayed and and data framing and machine learning great so we  \\nhave Hashim has said you could open in VSCode as well you could I mean that would require all your local installs and  \\nthat that type of stuff as well but we're to introduce me and James we we work at coiled where we build products for  \\ndistributed compute in infrastructure as we'll see one of the big problems with like bursting to the cloud is all the  \\nlike Kubernetes AWS docker stuff so we build a one-click host of deployments for das but for data science and machine  \\nlearning in general James maintains task along with Matt Matt Rocklin who created Dask with a team people who was  \\nworking with Continuum Anaconda at the time and James is a software engineer at called and I run data science evangelism  \\nmarketing work on a bunch of product product stuff as well wear a bunch of different different hats  \\n \\n\\n#### 00:08:01,680::\\t\\t4 minutes mark -> new paragraph \\n \\noccasionally there are many ways to think about distributed compute and how to do it in in Python we're going to present  \\nhey James you're muted I'm taking it I went away based on what I see in the chat you did you did but now we're back I've  \\nintroduced you I've introduced me I've mentioned that there are many ways to do distributed compute in the Python  \\necosystem and we'll be chatting about one called Dask and maybe I'll pass you in a second but I'll say one thing that I  \\nreally like about my background isn't in distributed compute my background's in pythonic data science when thinking  \\nabout bursting to larger data sets and larger models there are a variety of options the thing that took me attracted me  \\nto desk originally I saw Cameron's note the ghost in the machine aren't playing nice tonight I think that ain't that the  \\ntruth is that dark plays so nicely with the entire PyData ecosystem so as we'll see if you want to write dash code for  \\ndata frames dash data frames it really mimics your Pandas code same with numpy same with scikit-learn OK and the other  \\nthing is dark essentially runs the Python code under the hood so your mental model of what's happening is actually  \\ncorresponds to the code being being executed OK now I'd like to pass over to James but it looks like he's disappeared  \\nagain I'm still here if you can hear me I've just turned my camera off oh yeah OK great I'm gonna turn my camera  \\nhopefully that will help yeah and I might do do the same for bandwidth bandwidth issues so if if you want to jump in and  \\nand talk about dark at a high level I'm sharing my screen and we can scroll through yeah that sounds great so that's  \\nsort of a nutshell you can think of it as being composed of two main well components the first we call collections these  \\nare the user interfaces that you use to actually construct a computation you would like to compute in parallel or on  \\ndistributed hardware there are a few different interfaces that desk implements for instance there's Dask array for doing  \\nnd array computations there's das data frame for working with tabular data you can think of those as like GAsk array as  \\na parallel version of numpy das data frame has a parallel version of Pandas and so on there are also a couple other  \\ninterfaces that we'll be talking about das delayed for instance we'll talk about that today we'll also talk about the  \\nfutures API those are sort of for lower level custom algorithms in sort of paralyzing existing existing code the main  \\ntakeaway is that there are several sort of familiar APIs that desk implements and that will use today to actually  \\nconstruct your computation so that's the first part of desk it is these dash collections you then take these collections  \\nset up your steps for your computation and then pass them off to the second component which are desk schedulers and  \\nthese will actually go through and execute your computation potentially in parallel there are two flavors of schedulers  \\nthat desk offers the first is a are called single machine schedulers and these just take advantage of your local  \\nhardware they will spin up a a local thread or process pool and start submitting tasks in your computation to to be  \\nexecuted in parallel either on multiple threads or multiple processes there's also a distributed scheduler or maybe a  \\nbetter term for would actually be called the advanced scheduler because it works well on a single machine but it also  \\nscales out to multiple machines so for instance as you'll see later we will actually spin up a distributed scheduler  \\nthat has workers on remote  \\n \\n\\n#### 00:12:00,160::\\t\\t4 minutes mark -> new paragraph \\n \\nmachines on AWS so you can actually scale out beyond your local resources like say what's on your laptop kind of  \\nscrolling down then to the image of the cluster we can see the main components of the distributed scheduler and James I  \\nmight get people to spin up the Binder now because we're going to execute codes now is a good point yep so just here's a  \\nquick break point before you know a teaser for schedulers and what's happening there I'll ask you to in the repository  \\nthere's also the link to the Binder click on launch Binder I'm going to open it in a new tab and what this will create  \\nis an environment in which you can just execute the code in in the notebooks OK so hopefully by the time we've gotten  \\ngone through this section this will be ready to start executing code so if everyone wants to do that to code along  \\notherwise just watch or if you're running things locally also cool thanks James yeah yeah no problem thank you so so  \\nyeah looking at the image for the distributed scheduler we're not gonna have time to go into the a lot of detail about  \\nthe distributed scheduler in this workshop so but we do want to provide at least a high level overview of the the  \\ndifferent parts and components of the distributed scheduler so the first part I want to talk about is in the diagram  \\nwhat's labeled as a client so this is the user facing entry point to a cluster so wherever you are running your Python  \\nsession that could be in a Jupyter lab session like we are here that could be in a Python script somewhere you will  \\ncreate and instantiate a client object that connects to the second component which is the das scheduler so each desk  \\ncluster has a single scheduler in it that sort of keeps track of all of the state for all of the the state of your  \\ncluster and all the tasks you'd like to compute so from your client you might start submitting tasks to the cluster the  \\nschedule will receive those tasks and compute things like all the dependencies needed for that task like say you're  \\nimplementing you say you want to compute task c but that actually requires first you have to compute task b and task a  \\nlike there are some dependency structures there it'll compute those dependencies as well as keep track of them it'll  \\nalso communicate with all the workers to understand what worker is working on which task and as space frees up on the  \\nworkers it will start farming out new tasks to compute to the workers so in this particular diagram there are three das  \\ndistributed workers here however you can have as you can have thousands of workers if you'd like so the workers are the  \\nthings that actually compute the tasks they also store the results of your tasks and then serve them back to you and the  \\nclient the scheduler basically manages all the state needed to perform the computations and you submit tasks from the  \\nclient so that's sort of a quick whirlwind tour of the different components for the distributed scheduler and at this  \\npoint I think it'd be great to actually see see some of this in action Hugo would like to take over absolutely thank you  \\nfor that wonderful introduction to darsk and and the schedulers in particular and we are going to see that with dark in  \\naction I'll just note that this tab in which I launched the Binder is up and running if you're going to execute code  \\nhere click on notebooks click on Data Umbrella oop and then go to the overview notebook and you can drag around we'll  \\nsee the utility of these these dashboards in a second but you can you know drag your stuff around to to make you know  \\nhowever you want to want to structure it and then you can execute code in here I'm not going to do that I'm going to do  \\nthis locally at the moment but just to see dust in action to begin with I'm going to I'm actually going to  \\n \\n\\n#### 00:16:02,720::\\t\\t4 minutes mark -> new paragraph \\n \\nrestart kernel and clear my outputs so I'm going to import from dash distributed the client the sorry the other thing I  \\nwanted to mention is we made a decision around content for this we do have a notebook that we we love to teach on  \\nschedulers but we decided to switch it out for machine learning for this workshop in particular we are teaching a  \\nsimilar although distinct workshop at PyData global so we may see some of you there in which we'll be going more in  \\ndepth into schedulers as well so if you want to check that out definitely do so we instantiate the client which as James  \\nmentioned is kind of what we work with as the user to submit our code so that will take take a few seconds OK it's got a  \\nport in you so it's going going elsewhere what I'll just first get you to notice is that it tells us where our dashboard  \\nis and we'll see those tools in a second tells us about our cluster that we have four workers eight cores between eight  \\nand nine gigs of of ram OK now this is something I really love about Dask all the diagnostic tools if I click on the  \\nlittle desk thing here and we've modified the Binder so that that exists there as well we can see I'll hit search and it  \\nshould that now corresponds to the the scheduler now I want to look at the task stream which will tell us in real time  \\nwhat's happening I also want to look at the cluster map so we see here this is already really cool we've got all of our  \\nworkers around here and our scheduler scheduler there and when we start doing some compute we'll actually see  \\ninformation flowing between these and the other thing maybe I'll yeah I'll include a little progress and that can be an  \\nalternate tab to ask I'm wondering perhaps I also want to include something about the workers yeah OK great so we've got  \\na bunch of stuff that's that's pretty interesting there and so the next thing I'm going to do we've got a little utility  \\nfile which downloads some of the data and this is what it does is if you're in Binder it downloads a subset of the data  \\nif you're anywhere else it loads a larger set for this particular example we're dealing with a small data set you see  \\nthe utility of dark and distributed compute when it generalizes to larger data sets but for pedagogical purposes we're  \\ngoing to sit with a smaller data set so that we can actually run run the code there's a trade-off there so actually that  \\nwas already downloaded it seems but you should all see it download I'm actually going to run that in the Binder just to  \\nyou should start seeing downloading nyc flights data set done extracting creating json data etc OK now what we're going  \\nto do is we're going to read in this data as a Dask data frame and what I want you to notice is that it really the das  \\ncode mimics Pandas code so instead of pd read csv we've got dd read csv we've got you know this is the file path the  \\nfirst argument we're doing some parse date setting some data types OK we've got a little wild card regular expression  \\nthere to to join to do a bunch of them and then we're performing a group by OK so we're grouping by the origin of these  \\nflight flight data we're looking at the the mean departure delay group by origin the the one difference I want to make  \\nclear is that in das we need a compute method that's because das performs lazy computation it won't actually do anything  \\nbecause you don't want it to do anything on really large data sets until you explicitly tell it tell it to compute so  \\nI'm going to execute this now and we should see some information  \\n \\n\\n#### 00:20:01,520::\\t\\t4 minutes mark -> new paragraph \\n \\ntransfer between the scheduler and the workers and we should see tasks starting starting to be done OK so moment of  \\ntruth fantastic so we call this a pew pew plot because we see pew pew pew we saw a bunch of data transfer happening  \\nbetween them these are all our cause and we can see tasks happening it tells us what tasks there are we can see that  \\nmost of the time was spent reading reading csvs then we have some group bias on chunks and and that type of stuff so  \\nthat's a really nice diagnostic tool to see what most of your work is is actually doing under dark work as you can see  \\nmemory used CPU use more fine-grained examples there so I I'd love to know if in the Q&A I'm going to ask were you able  \\nto execute this code and if you were in Binder just a thumb up a vote would be no would be fantastic much appreciated so  \\nas we've mentioned I just wanted to say a few things about tutorial goals the goal is to cover the basics of dark and  \\ndistributed compute we'd love for you to walk away with an understanding of when to use it when to not what it has to  \\noffer we're going to be covering the basics of Dask delayed which although not immediately applicable to data science  \\nprovides a wonderful framework for thinking about Dask how dark works and understanding how it works under the hood then  \\nwe're going to go into dark data frames and then machine learning hopefully due to the technical considerations with  \\nwe've got less time than than we thought we would but we'll definitely do the best we can we may have less time to do  \\nexercises so we've had two people who are able to execute this code if you if you tried to execute it in Binder and were  \\nnot able to perhaps post that in the Q&A but we also have several exercises and I'd like you to take a minute just to do  \\nthis exercise the I I'm not asking you to do this because I want to know if you're able to print hello world I'm  \\nessentially asking you to do it so you get a sense of how these exercises work so if you can take 30 seconds to print  \\nhello world then we'll we'll move on after that so just take 30 seconds now and it seems like we have a few more people  \\nwho are able to execute code which which was great OK fantastic so you will put your solution there for some reason I  \\nhave an extra cell here so I'm just going to clip that and to see a solution I'll just get you to execute this cell and  \\nit provides the solution and then we can execute it and compare it to the the output of what you had OK hello world so  \\nas as we saw I've done all this locally you may have done it on Binder there is an option to work directly from the  \\ncloud and I'll I'll take you through this there are many ways to do this as I mentioned we're working on one way with  \\ncoil and I'll explain the rationale behind that in in a second but I'll show you how easy it is to get a cluster up and  \\nrunning on on AWS without even interacting with AWS for free for example you can follow along by signing into coiled  \\ncloud to be clear this is not a necessity and it does involve you signing up to our product so I just wanted to be  \\nabsolutely transparent about that it does not involve any credit card information or anything  \\n \\n\\n#### 00:24:01,520::\\t\\t4 minutes mark -> new paragraph \\n \\nalong those lines and in my opinion it does give a really nice example of how to run stuff on the cloud to do so you can  \\nsign in at cloud dot coiled dot io you can also pip install coiled and then do authentication you can also spin up this  \\nthis hosted coiled notebook so I'm going to spin that up now and I'm going to post that here actually yep I'm gonna post  \\nthat in the ch chat if you let me get this right if you've if you've never logged in to code before it'll ask you to  \\nsign up using gmail or GitHub so feel free to do that if you'd like if not that's also also cool but I just wanted to be  \\nexplicit about that the reason I want to do this is to show how dars can be leveraged to do work on really large data  \\nsets so you will recall that I had between eight and nine gigs of ram on my local system oh wow Anthony says on ipad  \\nunable to execute on Binder incredible I don't have a strong sense of how Binder works on ipad I do know that I was able  \\nto to check to use a Binder on my iphone several years ago on my way to scipy doing code review for someone for Eric  \\nMaher I think for what that that's worth but back to this we have this nyc taxi data set which is over 10 gigs it won't  \\neven I can't even store that in local memory I don't have enough ram to store that so we do need either to do it locally  \\nin an out of core mode of some sort or we can we can burst to the cloud and we're actually going to burst to the cloud  \\nusing using coiled so the notebook is running here for me and but I'm actually gonna do it from my local local notebook  \\nbut you'll see and once again feel free to code along here it's spinning up a notebook and James who is is my co-  \\ninstructor here is to be I'm I'm so grateful all the work is done on our notebooks in coiled you can launch the cluster  \\nhere and then analyze the entire over 10 gigs of data there I'm going to do it here so to do that I import coiled and  \\nthen I import the dash distributed stuff and then I can create my own software environment cluster configuration I'm not  \\ngoing to do that because the standard coiled cluster configuration software environment works now I'm going to spin up a  \\ncluster and instantiate a client now because we're spinning up a cluster in in the cloud it'll take it'll take a minute  \\na minute or two enough time to make a cup of coffee but it's also enough time for me to just talk a bit about why this  \\nis important and there are a lot of a lot of good good people working on on similar things but part of the motivation  \\nhere is that if you want to you don't always want to do distributed data science OK first I'd ask you to look at instead  \\nof using dark if you can optimize your Pandas code right second I'd ask if you've got big data sets it's a good question  \\ndo you actually need all the data so I would if you're doing machine learning plot your learning curve see how accurate  \\nsee how your accuracy or whatever your metric of interest is improves as you increase the amount of data right and if it  \\nplateaus before you get to a large data size then you may as well most of the time use your small data see if sub  \\nsampling can actually give you the results you need so you can get a bigger bigger access to a bigger machine so you  \\ndon't have to burst to the cloud but after all these things if you do need to boast burst to the cloud until recently  \\nyou've had to get an AWS account you've had to you know set up containers with docker and or Kubernetes and do all of  \\nthese kind of  \\n \\n\\n#### 00:28:00,640::\\t\\t4 minutes mark -> new paragraph \\n \\nI suppose devopsy software engineering foo stuff which which if you're into that I I absolutely encourage you encourage  \\nyou to do that but a lot of working data scientists aren't paid to do that and I don't necessarily want to so that's  \\nsomething we're working on is thinking about these kind of one-click hosted deployments so you don't have to do all of  \\nthat having said that I very much encourage you to try doing that stuff if if you're interested we'll see that the the  \\ncluster has just been created and what I'm going to do we see that oh I'm sorry I've done something funny here I'm I'm  \\nreferencing the previous client anna James yeah it looks like you should go ahead and connect a new client to the coil  \\ncluster and making sure not to re-execute the cluster creation exactly so would that be how would I what's the call here  \\nI would just open up a new cell and say client equals capital client and then pass in the cluster like open parentheses  \\ncluster yeah great OK fantastic and what we're seeing is a slight version this we don't need to worry about this this is  \\nessentially saying that the environment on the cloud mis is there's a slight mismatch with my with my local environment  \\nwe're fine with that I'm going to look here for a certain reason the the dashboard isn't quite working here at the  \\nmoment James would you suggest I just click on this and open a new yeah click on the ecs dashboard link oh yes fantastic  \\nso yep there's some bug with the local dashboards that we're we're currently currently working on but what we'll see now  \\njust a SEC I'm going to remove all of this we'll see now that I have access to 10 workers I have access to 40 cores and  \\nI have access to over 170 gigs of memory OK so now I'm actually going to import this data set and it's the entire year  \\nof data from 2019 and we'll start seeing on on the diagnostics all the all the processing happening OK so oh actually  \\nnot yet because we haven't called compute OK so it's done this lazily we've imported it it shows kind of like Pandas  \\nwhen you show a data frame the column names and data types but it doesn't show the data because we haven't loaded it yet  \\nit does tell you how many partitions it is so essentially and we'll see this soon das data frames correspond to  \\ncollections of Pandas data frames so they're really 127 Pandas data frames underlying this task data frame so now I'm  \\ngoing to do the compute well I'm going to set myself up for the computation to do a group by passenger gown and look at  \\nthe main tip now that took a very small amount of time we see the IPython magic timing there because we haven't computed  \\nit now we're actually going to compute and James if you'll see in the chat Eliana said her coil coiled authentication  \\nfailed I don't know if you're able to to help with that but if you are that would be great and it may be difficult to  \\ndebug in but look as we see we have the task stream now and we see how many you know we've got 40 cores working together  \\nwe saw the processing we saw the bytes stored it's over 10 gigs as I said and we see we were able  \\n \\n\\n#### 00:32:01,519::\\t\\t4 minutes mark -> new paragraph \\n \\nto do our basic analytics we were able to do it on a 10 plus gig data set in in 21.3 seconds which is pretty pretty  \\nexceptional if any any code based issues come up and they're correlated in particular so if you have questions about the  \\ncode execution please ask in the Q&A not in the chat because others cannot vote it and I will definitively prioritize  \\nquestions on technical stuff particularly ones that up that are upvoted but yeah I totally agree thanks thanks very much  \\nso yeah let's jump into into data frames so of course we write here that in the last exercise we used ask delayed to  \\nparallelize loading multiple csv files into a Pandas data frame we're not we we haven't done that but you can definitely  \\ngo through and have a look at that but I think perhaps even more immediately relevant for a data science crowd and an  \\nanalytics crowd is which is what I see here from the reasons people people have joined is jumping into Dask data frames  \\nand as I said before adas data frame really feels like a Pandas data frame but internally it's composed of many  \\ndifferent different data frames this is one one way to think about it that we have all these Pandas data frames and the  \\ncollection of them is a dark data frame and as we saw before they're partitioned we saw when we loaded the taxi data set  \\nin the dash data frame was 127 partitions right where each partition was a normal panda Pandas data frame and they can  \\nlive on disk as they did early in the first example dark in action or they can live on other machines as when I spun up  \\na coiled cluster and and did it on on AWS something I love about darth's data frames I mean I ran about this all the  \\ntime it's how it's the Pandas API and and Matt Matt Rocklin actually has a post on on the blog called a brief history of  \\nDask in which he talks about the technical goals of us but also talks about a social goal of task which in Matt's words  \\nis to invent nothing he wanted and the team wanted the Dask API to be as comfortable and familiar for users as possible  \\nand that's something I really appreciate about it so we see we have element element wires on operations we have the our  \\nfavorite row eyes selections we have loc we have the common aggregations we saw group buyers before we have is-ins we  \\nhave date time string accessors oh James we forgot to I forgot to edit this and I it should be grouped by I don't know  \\nwhat what a fruit buy is but that's something we'll make sure the next iteration to to get right at least we've got it  \\nright there and in the code but have a look at the dash data frame API docs to check out what's happening and a lot of  \\nthe time dash data frames can serve as drop in replacements for Pandas data frames the one thing that I just want to  \\nmake clear as I did before is that you need to call compute because of the lazy laser compute property of das so this is  \\nwonderful to talk about when to use data frames so if your data fits in memory use Pandas if your data fits in memory  \\nand your code doesn't run super quickly I wouldn't go to Dask I'd try to I'd do my best to optimize my Pandas code  \\nbefore trying to get gains gains and efficiency but dark itself becomes useful when the data set you want to analyze is  \\nlarger than your machine's ram where you normally run into memory errors and that's what we saw  \\n \\n\\n#### 00:36:01,520::\\t\\t4 minutes mark -> new paragraph \\n \\nwith the taxicab example the other example that we'll see when we get to [music] machine learning is you can do machine  \\nlearning on a small data set that fits in memory but if you're building big models or training over like a lot of  \\ndifferent hyper parameters or different types of models you can you can parallelize that using using dark so there is  \\nyou know you want to use dash perhaps in the big data or medium to big data limit as we see here or in the medium to big  \\nmodel limit where training for example takes and takes a lot of time OK so without further ado let's get started with  \\ndas data frames you likely ran this preparation file to get the data in the previous notebook but if you didn't execute  \\nthat now we're going to get our file names by doing doing a few joins and we see our file is a string data nyc flights a  \\nwildcard to access all of them dot dot csv and we're going to import our Dask dust.dataframe and read in our dataframe  \\nparsing some dates setting some sending some data types OK I'll execute that we'll see we have 10 partitions as we noted  \\nbefore if this was a Pandas data frame we'd see a bunch of entries here we don't we see only the column names and the  \\ndata types of the columns and the reason is as we've said it explicitly here is the representation of the data frame  \\nobject contains no data it's done Dask has done enough work to read the start of the file so that we know a bit about it  \\nsome of the important stuff and then further column types and column names and data types OK but we don't once again we  \\ndon't let's say we've got 100 gigs of data we don't want to like do this call and suddenly it's reading all that stuff  \\nin and doing a whole bunch of compute until we explicitly tell it to OK now this is really cool if you know a bit of  \\nPandas you'll know that you can there's an attribute columns which prints out it's well it's actually the columns form  \\nan index right the Pandas index object and we get the we get the column names there cool Pandas in dark form we can  \\ncheck out the data types as well as we would in Pandas we see we've got some ins for the day of the week we've got some  \\nfloats for departure time maybe we'd actually prefer that to be you know a date time at some point we've got some  \\nobjects which generally are the most general on objects so generally strings so that's all pandasey type stuff in  \\naddition das data frames have an attribute n partitions which tells us the number of partitions and we saw before that  \\nthat's 10 so I'd expect to see 10 here hey look at that now this is something that we talk about a lot in the delayed  \\nnotebook is really the task graph and I don't want to say too much about that but really it's a visual schematic of of  \\nthe order in which different types of compute happen OK and so the task graph for read csv tells us what happens when we  \\ncall compute and essentially it reads csv 10 ten times zero indexed of course because Python it reads csv ten different  \\ntimes into these ten different Pandas Pandas data frames and if there were group buys or stuff after that we'd see them  \\nhappen in in the in the graph there and we may see an example of this in a second so once again as with Pandas we're  \\ngoing to view the the head of the data frame great and we see a bunch of stuff you know we we see the first first five  \\nrows I'm actually also gonna gonna have a look at the  \\n \\n\\n#### 00:40:02,240::\\t\\t4 minutes mark -> new paragraph \\n \\nthe tail the final five rows that may take longer because it's accessing the the final I I there's a joke and it may not  \\neven be a joke how much data analytics is actually biased by people looking at the first five rows before actually you  \\nknow interrogating the data more more seriously so how would all of our results look different if if our files were  \\nordered in in a different way that's another conversation for a more philosophical conversation for another time so now  \\nI want to show you some computations with dark data frames OK so since dash data frames implement a Pandas like API we  \\ncan just write our familiar Pandas codes so I want to look at the column departure delay and look at the maximum of that  \\ncolumn I'm going to call that max delay so you can see we're selecting the column and then applying the max method as we  \\nwould with Pandas oh what happened there gives us some da scala series and what's happened is we haven't called compute  \\nright so it hasn't actually done the compute yet we're going to do compute but first we're going to visualize the task  \\ngraph like we did here and let's try to reason what the task graph would look like right so the task graph first it's  \\ngoing to read in all of these things and then it'll probably perform this selector on each of these different Pandas  \\ndata frames comprising the dash data frame and then it will compute the max of each of those and then do a max on all  \\nthose maxes I think that's what I would assume is happening here great so that's what we're what we're doing we're  \\nreading this so we read the first perform the first read csv get this das data frame get item I think is that selection  \\nthen we're taking the max we're doing the same for all of them then we take all of these max's and aggregate them and  \\nthen take the max of that OK so that that's essentially what's happening when I call compute which I'm going to do now  \\nmoment of truth OK so that took around eight seconds and it tells us the max and I I'm sorry let's let's just get out  \\nsome of our dashboards up as well huh I think in this notebook we are using the single machine scheduler Hugo so I don't  \\nthink there is a dashboard to be seen exactly yeah thank you for that that that catch James great is even better James  \\nwe have a question around using dark for reinforcement learning can you can you speak to that yeah so it depends on this  \\nI mean yeah short answer yes you can use GAs to train reinforcement learning models so there's a package that Hugo will  \\ntalk about called desk ML that we'll see in the next notebook for distributing machine learning that paralyzes and and  \\ndistributes some existing models using desks so for instance things like random forces forest inside kit learn so so yes  \\nyou can use das to do distributed training for models I'm not actually sure if GAskml implements any reinforcement  \\nlearning models in particular but that is certainly something that that can be done yeah and I'll I'll build on that by  \\nsaying we are about to jump into machine  \\n \\n\\n#### 00:44:00,000::\\t\\t4 minutes mark -> new paragraph \\n \\nlearning I don't think as James said I don't think there's reinforcement learning explicitly that that one can do but  \\nyou of course can use the das scheduler yourself to you know to distribute any reinforcement learning stuff you you have  \\nas well and that's actually another another point to make that maybe James can speak to a bit more is that the dark team  \\nof course built all of these high-level collections and task arrays and dust data frames and were pleasantly surprised  \\nwhen you know maybe even up to half the people using dust came in all like we love all that but we're going to use the  \\nscheduler for our own bespoke use cases right yeah exactly yeah the original intention was to like make basically a num  \\nlike a parallel numpy so that was like the desk array stuff like run run numpy and parallel on your laptop and and yeah  \\nso in order to do that we ended up building a distributed scheduler which sort of does arbitrary task computations so  \\nnot just things like you know parallel numpy but really whatever you'd like to throw at it and it turns out that ended  \\nup being really useful for people and so yeah now people use that sort of on their own just using the distributed  \\nscheduler to do totally custom algorithms in parallel in addition to these like nice collections like you saw Hugo  \\npresents the dash data frame API is you know the same as the panda's API so there is this like familiar space you can  \\nuse things like the high-level collections but you can also run whatever custom like Hugo said bespoke computations you  \\nmight have exactly and it's it's been wonderful to see so many people so many people do that and the first thing as  \\nwe'll see here the first thing to think about is if if you're doing lifestyle compute if there's anything you can you  \\nknow parallelize embarrassingly as they say right so just if you're doing a hyper parameter search you just run some on  \\none worker and some on the other and there there's no interaction effect so you don't need to worry about that as  \\nopposed to if you're trying to do you know train on streaming data where you may require it all to happen on on on the  \\nsame worker OK yeah so even think about trying to compute the standard deviation of a of a a univariate data set right  \\nin in that case you can't just send you can't just compute the standard deviation on two workers and then combine the  \\nresult in some some way you need to do something slightly slightly more nuanced and slightly slightly clever more clever  \\nI mean you still can actually in in that case but you can't just do it as naively as that but so now we're talking about  \\nparallel and distributed machine learning we have 20 minutes left so this is kind of going to be a whirlwind tour but  \\nyou know whirlwinds when safe exciting and informative I just want to make clear the material in this notebook is based  \\non the open source content from darsk's tutorial repository as there's a bunch of stuff we've shown you today the reason  \\nwe've done that is because they did it so well so I just want to give a shout out to all the das contributors OK so what  \\nwe're going to do now is just break down machine learning scaling problems into two categories just review a bit of  \\npsychic learn in passing solve a machine learning problem with single Michelle single Michelle I don't know who she is  \\nbut single Michelle wow single machine and parallelism with psychic learning job lib then solve an l problem with an ML  \\nproblem with multiple machines and parallelism using dark as well and we won't have time to burst for the cloud I don't  \\nthink but you can also play play around with that OK so as I mentioned before when thinking about distributed compute a  \\nlot of people do it when they have large data they don't necessarily think about the large model limit and this  \\nschematic kind of speaks to that if you've got a small model that fits in ram you don't need to think about  \\n \\n\\n#### 00:48:00,480::\\t\\t4 minutes mark -> new paragraph \\n \\ndistributed compute if your data size if your data is larger than your ram so your computer's ram bound then you want to  \\nstart going to a distributed setting or if your model is big and CPU bound such as like large-scale hyper-parameter  \\nsearches or like ensembl blended models of like machine learning algorithms whatever it is and then of course we have  \\nthe you know big data big model limit where distributed computer desk is incredibly handy as I'm sure you could imagine  \\nOK and that's really what I've what I've gone through here a bird's-eye view of the strategies we think about if it's in  \\nmemory in the bottom left quadrant just use scikit-learn or your favorite ML library otherwise known as psychic learn  \\nfor me anyway I was going to make a note about xg boost but I but I won't for large models you can use joblib and your  \\nfavorite circuit learn estimator for large data sets use our dark ML estimators so we're gonna do a whirlwind tour of  \\npsychic learn in in five minutes we're going to load in some data so we'll actually generate it we'll import scikit-  \\nlearn for our ML algorithm create an estimator and then check the accuracy of the model OK so once again I'm actually  \\ngoing to clear all outputs after restarting the kernel OK so this is a utility function of psychic learn to create some  \\ndata sets so I'm going to make a classification data set with four features and 10 000 samples and just have a quick  \\nview of some of it so just a reminder on ML x is the samples matrix the size of x is the number of samples in terms of  \\nrows number of features as columns and then a feature or an attribute is what we're trying to predict essentially OK so  \\nwhy is the predictor variable which we're where which we're or the target variable which we're trying to predict so  \\nlet's have a quick view of why it's zeros and ones in in this case OK so yep that's what I've said here why are the  \\ntargets which are real numbers for regression tasks or integers for classification or any other discrete sets of values  \\nno words about unsupervised learning at the moment we're just going to support we're going to fit a support vector  \\nclassifier for this example so let's just load the appropriate scikit-learn module we don't really need to discuss what  \\nsupport vector classifiers are at the moment now this is one of the very beautiful things about the scikit-learn API in  \\nterms of fitting the the model we instantiate a classifier and we want to fit it to the features with respect to the  \\ntarget OK so the first argument is the features second argument is the target variable so we've done that now I'm not  \\ngoing to worry about inspecting the learn features I just want to see how accurate it was OK and once we see how  \\naccurate it was I'm not gonna do this but then we can make a prediction right using estimator dot predict on a new a new  \\ndata set so this estimator will tell us so this score will tell us the accuracy and essentially that's the proportion or  \\npercentage a fraction of the results that were that the estimator got correct and we're doing this on the training data  \\nset we've just trained the model on this so this is telling us the accuracy on the on the training data set OK so it's  \\n90  \\n \\n\\n#### 00:52:01,760::\\t\\t4 minutes mark -> new paragraph \\n \\naccurate on the training data set if you dive into this a bit more you'll recognize that if we we really want to know  \\nthe accuracy on a holdout set or a test set and it should be probably a bit lower because this is what we use to fit it  \\nOK but all that having been said I expect you know if if this is all resonating with you it means we can really move on  \\nto the distributed stuff in in a second but the other thing that that's important to note is that we've trained it but a  \\nlot of model a lot of estimators and models have hyper parameters that affect the fit but you that we need to specify up  \\nfront instead of being learned during training so you know there's a c parameter here there's a are we using shrinking  \\nor not so we specify those we didn't need to specify them because there are default values but here we specify them OK  \\nand then we're going to look at the score now OK this is amazing we've got 50 accuracy which is the worst score possible  \\njust think about this if if you've got binary classification task and you've got 40 accuracy then you just flip the  \\nlabels and that changes to 60 accuracy so it's amazing that we've actually hit 50 accuracy we're to be congratulated on  \\nthat and what I want to note here is that we have two sets of hyper parameters we've used one's created 90 actual model  \\nwith 90 accuracy another one one with 50 accuracy so we want to find the best hyper parameters essentially and that's  \\nwhy hyper parameter optimization is is so important there are several ways to do hyper parameter optimization one is  \\ncalled grid search cross validation I won't talk about cross validation it's essentially a more robust analogue of train  \\ntest split where you train on a subset of your data and compute the accuracy on a test on a holdout set or a test set  \\ncross validation is a as I said a slightly more robust analog of this it's called grid search because we have a grid of  \\nhyper parameters so we have you know in this case we have a hyper parameter c we have a hyper parameter kernel and we  \\ncan imagine them in a in a grid and we're performing we're checking out the score over all this gr over this entire grid  \\nof hyper parameters OK so to do that I import grid search csv now I'm going to compute the estimator over over these  \\ntrain the estimator over over this grid and as you see this is taking time now OK and what I wanted to make clear and I  \\nthink should be becoming clearer now is that if we have a large hyper parameter sweep we want to do on a small data set  \\ndas can be useful for that OK because we can send some of the parameters to one worker some to another and they can  \\nperform them in parallel so that's embarrassingly parallel because you're you're doing the same work as you would  \\notherwise but sending it to a bunch of different workers we saw that took 30 seconds which is in my realm of comfort as  \\na data scientist I'm happy to wait 30 seconds if I had to wait much longer if this grid was bigger I'd start to get  \\nprobably a bit frustrated but we see that it computed it for c is equal to all combinations of these essentially OK so  \\nthat's really all I wanted to say there and then we can see the best parameters and the best score so the best score was  \\n0.098 and it was c10 and the kernel rbf a radial basis function it doesn't even matter what that is though for the  \\npurposes of this so we've got 10 minutes left we're going to we're going to make it I can feel it I have a good I have a  \\ngood sense  \\n \\n\\n#### 00:56:00,400::\\t\\t4 minutes mark -> new paragraph \\n \\n a good after the I mean this demo is actually going incredibly well given the initial technical hurdles so touchwood  \\nHugo OK so what we've done is we've really segmented ML scaling problems into two categories CPU bound and ram bound and  \\nI I really I can't emphasize that enough because I see so many people like jumping in to use new cool technologies  \\nwithout perhaps taking it being a bit mindful and intentional about it and reasoning about when things are useful and  \\nand when not I suppose the one point there is that sure data science is a technical discipline but there are a lot of  \\nother aspects to it involving this type of reasoning as well so we then carried out a typical sklearn workflow for ML  \\nproblems with small models and small data and we reviewed hyper parameters and hyper parameter optimization so in this  \\nsection we'll see how job lib which is a set of tools to provide lightweight pipelining in Python gives us parallelism  \\non our laptop and then we'll see how dark ML can give us awesome parallelism on on clusters OK so essentially what I'm  \\ndoing here is I'm doing exactly the same as above with a grid search but I'm using the quark the keyword argument n jobs  \\nwhich tells you how many tasks to run in parallel using the cause available on your local workstation and specifying  \\nminus one jobs means the it just runs them the maximum possible OK so I'm going to execute that great so we should be  \\ndone in a second feel free to ask any questions in the chat oh alex has a great question in the Q&A does das have see a  \\nsequel and query optimizer I'm actually so excited that [music] and James maybe you can provide a couple of links to  \\nthis we're really excited to have seen dark dust SQL developments there recently so that's dark hyphen hyphen SQL and  \\nwe're actually we're working on some some content and a blog post and maybe a live live coding session about that in in  \\nthe near future so if anyone if you want updates from from coyle feel free to go to our website and sign up for our  \\nmailing list and we'll let you know about all of this type of stuff but the short answer is yes alex and it's getting  \\nbetter and if James is able to post post a link there that would be that would be fantastic so we've done link in the  \\nchat fantastic [music] and so we've we've seen how we have [music] single machine parallelism here using the using the  \\nend jobs quark and in the final minutes let's see multiple multi-machine parallelism with Dask OK so what I'm going to  \\ndo is I'm going to do my imports and create my client incentive my client and check it out OK so once again I'm working  \\nlocally I hit search and that'll task is pretty smart in terms of like knowing which which client I want to check out do  \\nthe tasks stream because it's my favorite I'll do the cluster map otherwise known as the pew pew map and then I want  \\nsome progress we all we all crave progress don't we and maybe my workers tab OK great so we've got that up and running  \\nnow I'm going to do a slightly larger hyper parameter search OK so remember we had just a couple for c a couple for  \\nkernel we're going to do more we have some for shrinking now I'm actually going to comment that out because I don't know  \\nhow long that's going to take if you're coding them on Binder now this May actually take far far too long for you but  \\nwe'll we'll see so I'll execute this code and we should see just sick no we shouldn't see any work happening yet but  \\nwhat I'm doing here is oh looks like OK my clusters back up great we're doing our grid search but we're going to use  \\nDask as as the back end right and this is a context manager where we're asserting that and and we can just discuss the  \\nthe syntax there but it's not particularly important currently I'm going to execute this now and let's see fantastic  \\nwe'll see all this data transfer happening here we'll see our tasks happening here we can see these big batches of fit  \\nand score fit so fitting fitting the models then finding how well they perform via this k-fold cross validation which is  \\nreally cool and let's just yep we can see what's happening here we can see we currently have 12 processing we've got  \\nseven in memory and we have several more that we need to do our desk workers we can see us oh we can see our CPU usage  \\nwe can see how we can see CPU usage across all the workers which is which is pretty cool seeing that distribution is is  \\nreally nice whenever some form of b swarm plot if you have enough would would be useful there or even some form of  \\ncumulative distribution function or something like that not a histogram people OK you can go to my bayesian tutorial  \\nthat I've taught here before to hear me rave about the the horrors of histograms so we saw that talk a minute which is  \\ngreat and we split it across you know eight cores or whatever it is and now we'll have a look once again we get the same  \\nbest performer which is which is a sanity check and that's pretty cool I think we have a we actually have a few minutes  \\nleft so I am gonna just see if I can oh let me think yeah I will see if I can burst burst to the cloud and and and do  \\nthis that will take a minute a minute or two to create the cluster again but while we're while we're doing that I'm  \\nwondering if we have any any questions or if anyone has any feedback on on this workshop I very much welcome welcome  \\nthat perhaps if there are any final messages you'd you'd like to say James while we're spinning this up you can you can  \\nlet me know yeah sure I just also first off wanted to say thanks everyone for attending and like bearing  \\n \\n\\n#### 01:04:01,119::\\t\\t4 minutes mark -> new paragraph \\n \\nbearing with us with the technical difficulties really appreciate that real quick I'm just yeah so if you have if you  \\nhave questions please post in the Q&A section while the cold cluster's spinning up theodore posted in the last largest  \\nexample of grid search how much performance gain did we get from using das and not just in jobs hmm that's a great  \\nquestion and we actually didn't see let's see so it took 80 seconds ah let me get this they're actually not comparable  \\nbecause I did the grid search over a different set of hyper parameters I did it over a larger set of hyper parameters  \\nright so when I did end jobs I did it there were only it was a two by two grid of hyper parameters whereas when I did it  \\nwith with Dask it was a one two three four five six six by three so let's just reason about that this one was eighteen  \\nsix by three is eighteen which took eighty seconds and this one was two by two so it was four and it took 26 seconds so  \\na minor gain I think with this hyper parameter search if you multiply that by by four you'll well 4.2 4.5 you'll need  \\nthat would have taken maybe two minutes or something something like that so we saw some increase in efficiency not a  \\ngreat deal but James maybe you can say more to this part of the reason for that is that we're doing it on kind of a very  \\nsmall example so we won't necessarily see the gains in efficiency with a data set this size and with a small hyper  \\nparameter suite like this is that right yeah yeah and yeah exactly and I guess also this is more of an kind of an  \\nillustrative point here I guess so you're just using directly using in jobs with something like job lib by default we'll  \\nuse local threads and processes on like whatever machine you happen to be running on so like in this case on Hugo's  \\nlaptop one of the real advantages of using job lib with the das back in will actually dispatch back to to run tasks on a  \\ndash cluster is that your cluster can expand beyond what local resources you have so you can run you know you can  \\nbasically scale out like for instance using the coil cluster to have many many CPUs and a large amount of ram that you  \\nwouldn't have on your locally table to run and there you'll see both large performance gains as well as you'll be able  \\nto expand your the set of possible problems you can solve to larger than ram scenarios so you're out of out of core  \\ntraining exactly and thank you Jack this was absolutely unplanned and we didn't plan that question but that's a  \\nwonderful segue into me now performing exactly the same compute with the same code using the dasc as the parallel back  \\nend on a on a coiled cluster which is an AWS cluster right so we can I'm more currently anyway so I will execute this  \\ncode and it's exactly the same as we did whoa OK great so we see our tasks task stream here you see once again we see  \\nthe majority is being batch fit and and getting the scores out similarly we see the same result being the best I'll just  \\nnotice that for this for this small task doing it on the cloud took 20 seconds doing it locally for me took 80 seconds  \\nso that's a four-fold increase in performance on a very small task so imagine what that does if you can take the same  \\ncode as you've written  \\n \\n\\n#### 01:08:00,240::\\t\\t4 minutes mark -> new paragraph \\n \\nhere and burst to the cloud with with one click or however however you do it I think that that's incredibly powerful and  \\nthat the fact that your code and what's happening in the back end with Dask generalizes immediately to the new setting  \\nof working on a cluster I personally find very exciting and if you work with larger data sets or building larger models  \\nor big hyper parameter sweeps I'm pretty sure it's an exciting option for all of you also so on that note I'd like to  \\nreiterate James what James said and thanking you all so much for joining us for asking great questions and for bearing  \\nwith us through some some technical technical hurdles but it made it even even funner when when we got up and running  \\nonce again I'd love to thank Mark Christina and and the rest of the organizers for doing such a wonderful job and doing  \\nsuch a great service to the data science and machine learning community and ecosystem worldwide so thank you once again  \\nfor having us thank you Hugo and James I have to say like with all the technical difficulties I was actually giggling  \\nbecause it was kind of funny yeah but we're very sorry and we thank you for your patience and sticking through it and I  \\nwill be editing this video to you know make it as efficient as possible and have that available Tim supercard thank you  \\ngreat and I'll just ask you if you are interested in checking out coiled go to our website if you want to check out our  \\nproduct go to cloud.coil.io we started building this company in February we're really excited about building a new  \\nproduct so if you're interested reach out we'd love to chat with you about what we're doing and what we're up to and  \\nit's wonderful to be in the same community as you all so thanks  \\n \\n\"),\n",
       "             ('year', '2021'),\n",
       "             ('idn', '08'),\n",
       "             ('video_url', 'https://youtu.be/MHAjCcBfT_A'),\n",
       "             ('title_kw', 'audio-demo'),\n",
       "             ('transcript_md', '08-cat-reshama-audio-demo.md'),\n",
       "             ('audio_track',\n",
       "              WindowsPath('C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/resources/EventManagement/data/meta/2021_08_MHAjCcBfT_A.mp4')),\n",
       "             ('audio_text',\n",
       "              WindowsPath('C:/Users/catch/Documents/GitHub/DU-event-transcript-demo/resources/EventManagement/data/meta/2021_08_MHAjCcBfT_A.txt')),\n",
       "             ('has_transcript', True),\n",
       "             ('trans_idx', 684),\n",
       "             ('status', 'Partial (new editor requested)'),\n",
       "             ('notes', 'N.A.'),\n",
       "             ('video_embed',\n",
       "              '\\n<iframe width=\"560\" height=\"315\" \\n        src=\"https://www.youtube-nocookie.com/embed/MHAjCcBfT_A?cc_load_policy=1&autoplay=0\" \\n        frameborder=\"0\">\\n</iframe>\\n')])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gui.data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_idx = AC.app.left_sidebar.selected_index\n",
    "AC.app.left_sidebar.children[menu_idx].children[0].index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_string.Textarea"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "60133"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AC.PC.page.children[1].children[1].value\n",
    "txa = AC.PC.page.children[1].children[1]\n",
    "type(txa)\n",
    "len(txa.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = \"\"\"\n",
    "'<!-- Editing Guide: The pipe (|) position in this comment is 120:                                                       | -->\\n### Introduction\\n\\nOkay hello and welcome to Data Umbrella\\'s webinar for October so I\\'m just going to go over the agenda I\\'m going to do a  \\nbrief introduction then there will be the workshop by Hugo and James and you can ask questions along the way in the chat  \\nor actually the best place to ask questions is the Q&A and there\\'s an option to upvote as well so yet asking the Q&A if  \\nyou happen to post it on the chat by mistake I can also transfer it over to Q&A so that would be fine too and this  \\nwebinar is being recorded. Briefly about me. I am a statistician and data scientist and I am the founder of Data Umbrella.  \\nI am on a lot of platforms as Reshama so feel free to follow me on Twitter and LinkedIn. We have a code of conduct we\\'re  \\ndedicated to providing a harassment-free experience for everyone. Thank you for helping to make this a welcoming friendly  \\nprofessional community for all and this code of conduct applies to the chat as well. So our mission is to provide an  \\ninclusive community for underrepresented persons in data science and we are an all volunteer-run organization you can  \\nsupport Data Umbrella by doing the following things: You can follow our code of conduct and keep our community a place  \\nwhere everybody wants to keep coming to; You can donate to our open collective and that helps to pay meet-up dues and  \\nother operational costs and you can check out this link here on GitHub we have this new initiative where all the videos  \\nare being transcribed and so is to make them more accessible. So we take the YouTube videos and we put the raw there and  \\nso we\\'ve had a number of volunteers help us transcribe it so feel free to check out this link and maybe if you do this  \\nvideo maybe the two speakers will follow you on Twitter. I can\\'t promise anything, but it\\'s possible Data Umbrella has a  \\njob board and it\\'s at jobs.org and once this gets started I\\'ll put some links in the chat. The job that we are  \\nhighlighting today is the machine learning engineer job by Development Seed. Development Seed is based in  \\nWashington DC and Lisbon Portugal and they do I\\'m going to go to the next slide what they do is they\\'re doing social  \\ngood work and so they\\'re doing for instance mapping elections from Afghanistan to the US analyzing public health and  \\neconomic data from Palestine to Illinois and leading the strategy and development behind data world bank and some other  \\norganizations. I will share a link to their job posting in the chat as well as soon as I finish this brief  \\nintroduction. Check out our website for resources there\\'s a lot of resources on learning Python and R also for  \\ncontributing to open source also for guides on accessibility and responsibility and allyship. We have a monthly  \\nnewsletter that goes out towards the end of the month and it has information on our upcoming events. We have two great  \\nevents coming up in November and December on open source so subscribe to our newsletter to be in the know. We are on all  \\nsocial media platforms as Data Umbrella Meetup is the best place to join to find out about upcoming events our website  \\nhas resources follow us on Twitter we also share a lot of information on LinkedIn and if you want to subscribe to our  \\nYouTube channel we record all of our talks and post them there within about a week of the talk so it\\'s a good way to get  \\ninformation. OK and now we are ready to get started so I will put myself on mute and I will hand it over to Hugo and James  \\nand let you take over but thank you all for joining!   \\n \\n\\n#### 00:04:03,120::\\t\\t4 minutes Mark -> new paragraph \\n \\nI just want to thank Reshama Christina and everyone else who tied all the tireless effort that  \\nthat goes into putting these meet-ups and these online sessions together I think one thing I want to say is actually  \\nthe last in-person workshop I gave either at the end of February or early March was Data Umbrella\\'s inaugural  \\ntutorial and Meetup if I recall correctly on Bayesian thinking and hacker statistics and simulation and  \\nthat type of stuff so it\\'s just wonderful to be back particularly with my colleague and friend James we\\'re  \\nbuilding really cool distributed data science products at coiled we\\'ll say a bit about that but we\\'ll do some  \\nintroductions in a bit I just wanted to get you all accustomed to it was February thank you Reshama we\\'re working  \\nwith Jupyter notebooks in a GitHub repository the repository is pinned to the top of the chat this is what it looks like  \\nthese are all the files this is the file system now we use something called Binder which is a project out of and related  \\nto project Jupyter which provides infrastructure to run notebooks without any local installs so there are two  \\nways you can code along on this tutorial the first is and I won\\'t get you to do this yet is to launch Binder the  \\nreason I won\\'t get you to do that yet is because once you launch it we have 10 minutes to start coding or the Binder  \\nsession Times out I\\'ve been burnt by that before actually several Times I\\'m surprised I even remembered it this time the  \\nother thing you can do is install everything locally by cloning the repository downloading anaconda creating a conda  \\nenvironment if you haven\\'t done that I suggest you do not do that now and you launch the Binder James is going to  \\nstart by telling us a few things about GAs and distributed computing in general my question for you James is  \\nif we get people to launch this now will we get to execute a cell code cell in 10 minutes I would let\\'s hold off for now  \\nmaybe yep maybe I\\'ll indicate when we should launch Binder OK fantastic cool and just what I\\'m looking at right now is  \\nthe GitHub repository on your browser OK exactly so I will not launch Binder now I will not get you to now I\\'ve I\\'m  \\ndoing this locally and we see that I\\'m in notebook zero and if you want to actually have a look at this notebook before  \\nlaunching Binder, it\\'s in the notebooks Data Umbrella subdirectory and it\\'s notebook zero and we\\'re going to hopefully  \\nmake it through the overview then chatting about Dask, Dask delayed and data framing and machine learning great so we  \\nhave Hashim has said you could open in VSCode as well. You could. I mean that would require all your local installs and  \\nthat that type of stuff as well but we\\'re to introduce me and James we work at coiled where we build products for  \\ndistributed computing in infrastructure as we\\'ll see one of the big problems with like bursting to the cloud is all the  \\nlike Kubernetes AWS docker stuff so we build a one-click host of deployments for das but for data science and machine  \\nlearning in general James maintains task along with Matt Matt Rocklin who created Dask with a team of people and was  \\nworking with Continuum Anaconda at the Time and James is a software engineer at coiled and I run data science evangelism  \\nMarketing work on a bunch of product stuff as well wear a bunch of different hats occasionally.  \\n \\n\\n#### 00:08:01,680::\\t\\t4 minutes Mark -> new paragraph \\n \\nThere are many ways to think about distributed compute and how to do it in in Python we\\'re going to present  \\nhey James you\\'re muted I\\'m taking it I went away based on what I see in the chat you did you did but now we\\'re back I\\'ve  \\nintroduced you I\\'ve introduced me I\\'ve mentioned that there are many ways to do distributed compute in the Python  \\necosystem and we\\'ll be chatting about one called Dask and maybe I\\'ll pass you in a second but I\\'ll say one thing that I  \\nreally like about my background isn\\'t in distributed compute my background\\'s in Pythonic data science when thinking  \\nabout bursting to larger data sets and larger models there are a variety of options the thing that took me attracted me  \\nto desk originally I saw Cameron\\'s note the ghost in the machine aren\\'t playing nice tonight I think that ain\\'t that the  \\ntruth is that dark plays so nicely with the entire PyData ecosystem so as we\\'ll see if you want to write dash code for  \\ndata frames dash data frames it really mimics your Pandas code same with numpy same with scikit-learn OK and the other  \\nthing is dark essentially runs the Python code under the hood so your mental model of what\\'s happening is actually  \\ncorresponds to the code being being executed OK now I\\'d like to pass over to James but it looks like he\\'s disappeared  \\nagain I\\'m still here if you can hear me I\\'ve just turned my camera off oh yeah OK great I\\'m gonna turn my camera  \\nhopefully that will help yeah and I might do do the same for bandwidth bandwidth issues so if if you want to jump in and  \\nand talk about dark at a high level I\\'m sharing my screen and we can scroll through yeah that sounds great so that\\'s  \\nsort of a nutshell you can think of it as being composed of two main well components the first we call collections these  \\nare the user interfaces that you use to actually construct a computation you would like to compute in parallel or on  \\ndistributed hardware there are a few different interfaces that Dask implements. For instance, there\\'s Dask array for doing  \\nnd array computations there\\'s Dask dataframe for working with tabular data you can think of those as like Dask array as  \\na parallel version of numpy. Dask dataframe has a parallel version of Pandas and so on there are also a couple other  \\ninterfaces that we\\'ll be talking about das delayed for instance we\\'ll talk about that today we\\'ll also talk about the  \\nfutures API those are sort of for lower level custom algorithms in sort of paralyzing existing existing code the main  \\ntakeaway is that there are several sort of familiar APIs that desk implements and that will use today to actually  \\nconstruct your computation so that\\'s the first part of desk it is these dash collections you then take these collections  \\nset up your steps for your computation and then pass them off to the second component which are desk schedulers and  \\nthese will actually go through and execute your computation potentially in parallel there are two flavors of schedulers  \\nthat desk offers the first is a are called single machine schedulers and these just take advantage of your local  \\nhardware they will spin up a a local thread or process pool and start submitting tasks in your computation to to be  \\nexecuted in parallel either on multiple threads or multiple processes there\\'s also a distributed scheduler or maybe a  \\nbetter term for would actually be called the advanced scheduler because it works well on a single machine but it also  \\nscales out to multiple machines so for instance as you\\'ll see later we will actually spin up a distributed scheduler  \\nthat has workers on remote  \\n \\n\\n#### 00:12:00,160::\\t\\t4 minutes Mark -> new paragraph \\n \\nmachines on AWS so you can actually scale out beyond your local resources like say what\\'s on your laptop kind of  \\nscrolling down then to the image of the cluster we can see the main components of the distributed scheduler and James I  \\nmight get people to spin up the Binder now because we\\'re going to execute codes now is a good point yep so just here\\'s a  \\nquick break point before you know a teaser for schedulers and what\\'s happening there I\\'ll ask you to in the repository  \\nthere\\'s also the link to the Binder click on launch Binder I\\'m going to open it in a new tab and what this will create  \\nis an environment in which you can just execute the code in in the notebooks OK so hopefully by the Time we\\'ve gotten  \\ngone through this section this will be ready to start executing code so if everyone wants to do that to code along  \\notherwise just watch or if you\\'re running things locally also cool thanks James yeah yeah no problem thank you so so  \\nyeah looking at the image for the distributed scheduler we\\'re not gonna have Time to go into the a lot of detail about  \\nthe distributed scheduler in this workshop so but we do want to provide at least a high level overview of the the  \\ndifferent parts and components of the distributed scheduler so the first part I want to talk about is in the diagram  \\nwhat\\'s labeled as a client so this is the user facing entry point to a cluster so wherever you are running your Python  \\nsession that could be in a Jupyter lab session like we are here that could be in a Python script somewhere you will  \\ncreate and instantiate a client object that connects to the second component which is the das scheduler so each desk  \\ncluster has a single scheduler in it that sort of keeps track of all of the state for all of the the state of your  \\ncluster and all the tasks you\\'d like to compute so from your client you might start submitting tasks to the cluster the  \\nschedule will receive those tasks and compute things like all the dependencies needed for that task like say you\\'re  \\nimplementing you say you want to compute task c but that actually requires first you have to compute task b and task a  \\nlike there are some dependency structures there it\\'ll compute those dependencies as well as keep track of them it\\'ll  \\nalso communicate with all the workers to understand what worker is working on which task and as space frees up on the  \\nworkers it will start farming out new tasks to compute to the workers so in this particular diagram there are three das  \\ndistributed workers here however you can have as you can have thousands of workers if you\\'d like so the workers are the  \\nthings that actually compute the tasks they also store the results of your tasks and then serve them back to you and the  \\nclient the scheduler basically manages all the state needed to perform the computations and you submit tasks from the  \\nclient so that\\'s sort of a quick whirlwind tour of the different components for the distributed scheduler and at this  \\npoint I think it\\'d be great to actually see see some of this in action Hugo would like to take over absolutely thank you  \\nfor that wonderful introduction to Dask and and the schedulers in particular and we are going to see that with dark in  \\naction I\\'ll just note that this tab in which I launched the Binder is up and running if you\\'re going to execute code  \\nhere click on notebooks click on Data Umbrella oop and then go to the overview notebook and you can drag around we\\'ll  \\nsee the utility of these these dashboards in a second but you can you know drag your stuff around to to make you know  \\nhowever you want to want to structure it and then you can execute code in here I\\'m not going to do that I\\'m going to do  \\nthis locally at the moment but just to see dust in action to begin with I\\'m going to I\\'m actually going to  \\n \\n\\n#### 00:16:02,720::\\t\\t4 minutes Mark -> new paragraph \\n \\nrestart kernel and clear my outputs so I\\'m going to import from dash distributed the client the sorry the other thing I  \\nwanted to mention is we made a decision around content for this we do have a notebook that we we love to teach on  \\nschedulers but we decided to switch it out for machine learning for this workshop in particular we are teaching a  \\nsimilar although distinct workshop at PyData global so we may see some of you there in which we\\'ll be going more in  \\ndepth into schedulers as well so if you want to check that out definitely do so we instantiate the client which as James  \\nmentioned is kind of what we work with as the user to submit our code so that will take take a few seconds OK it\\'s got a  \\nport in you so it\\'s going going elsewhere what I\\'ll just first get you to notice is that it tells us where our dashboard  \\nis and we\\'ll see those tools in a second tells us about our cluster that we have four workers eight cores between eight  \\nand nine gigs of of ram OK now this is something I really love about Dask all the diagnostic tools if I click on the  \\nlittle desk thing here and we\\'ve modified the Binder so that that exists there as well we can see I\\'ll hit search and it  \\nshould that now corresponds to the the scheduler now I want to look at the task stream which will tell us in real Time  \\nwhat\\'s happening I also want to look at the cluster map so we see here this is already really cool we\\'ve got all of our  \\nworkers around here and our scheduler scheduler there and when we start doing some compute we\\'ll actually see  \\ninformation flowing between these and the other thing maybe I\\'ll yeah I\\'ll include a little progress and that can be an  \\nalternate tab to ask I\\'m wondering perhaps I also want to include something about the workers yeah OK great so we\\'ve got  \\na bunch of stuff that\\'s that\\'s pretty interesting there and so the next thing I\\'m going to do we\\'ve got a little utility  \\nfile which downloads some of the data and this is what it does is if you\\'re in Binder it downloads a subset of the data  \\nif you\\'re anywhere else it loads a larger set for this particular example we\\'re dealing with a small data set you see  \\nthe utility of dark and distributed compute when it generalizes to larger data sets but for pedagogical purposes we\\'re  \\ngoing to sit with a smaller data set so that we can actually run run the code there\\'s a trade-off there so actually that  \\nwas already downloaded it seems but you should all see it download I\\'m actually going to run that in the Binder just to  \\nyou should start seeing downloading NYC flights data set done extracting creating json data etc OK now what we\\'re going  \\nto do is we\\'re going to read in this data as a Dask data frame and what I want you to notice is that it really the das  \\ncode mimics Pandas code so instead of pd read csv we\\'ve got dd read csv we\\'ve got you know this is the file path the  \\nfirst argument we\\'re doing some parse date setting some data types OK we\\'ve got a little wild card regular expression  \\nthere to to join to do a bunch of them and then we\\'re performing a group by OK so we\\'re grouping by the origin of these  \\nflight flight data we\\'re looking at the the mean departure delay group by origin the the one difference I want to make  \\nclear is that in das we need a compute method that\\'s because das performs lazy computation it won\\'t actually do anything  \\nbecause you don\\'t want it to do anything on really large data sets until you explicitly tell it tell it to compute so  \\nI\\'m going to execute this now and we should see some information  \\n \\n\\n#### 00:20:01,520::\\t\\t4 minutes Mark -> new paragraph \\n \\ntransfer between the scheduler and the workers and we should see tasks starting starting to be done OK so moment of  \\ntruth fantastic so we call this a pew pew plot because we see pew pew pew we saw a bunch of data transfer happening  \\nbetween them these are all our cause and we can see tasks happening it tells us what tasks there are we can see that  \\nmost of the Time was spent reading csvs then we have some group bias on chunks and and that type of stuff so  \\nthat\\'s a really nice diagnostic tool to see what most of your work is is actually doing under dark work as you can see  \\nmemory used CPU use more fine-grained examples there so I I\\'d love to know if in the Q&A I\\'m going to ask were you able  \\nto execute this code and if you were in Binder just a thumb up a vote would be no would be fantastic much appreciated so  \\nas we\\'ve mentioned I just wanted to say a few things about tutorial goals the goal is to cover the basics of dark and  \\ndistributed compute we\\'d love for you to walk away with an understanding of when to use it when to not what it has to  \\noffer we\\'re going to be covering the basics of Dask delayed which although not immediately applicable to data science  \\nprovides a wonderful framework for thinking about Dask how dark works and understanding how it works under the hood then  \\nwe\\'re going to go into dark data frames and then machine learning hopefully due to the technical considerations with  \\nwe\\'ve got less Time than than we thought we would but we\\'ll definitely do the best we can we may have less Time to do  \\nexercises so we\\'ve had two people who are able to execute this code if you if you tried to execute it in Binder and were  \\nnot able to perhaps post that in the Q&A but we also have several exercises and I\\'d like you to take a minute just to do  \\nthis exercise the I I\\'m not asking you to do this because I want to know if you\\'re able to print hello world I\\'m  \\nessentially asking you to do it so you get a sense of how these exercises work so if you can take 30 seconds to print  \\nhello world then we\\'ll we\\'ll move on after that so just take 30 seconds now and it seems like we have a few more people  \\nwho are able to execute code which which was great OK fantastic so you will put your solution there for some reason I  \\nhave an extra cell here so I\\'m just going to clip that and to see a solution I\\'ll just get you to execute this cell and  \\nit provides the solution and then we can execute it and compare it to the the output of what you had OK hello world so  \\nas as we saw I\\'ve done all this locally you may have done it on Binder there is an option to work directly from the  \\ncloud and I\\'ll I\\'ll take you through this there are many ways to do this as I mentioned we\\'re working on one way with  \\ncoil and I\\'ll explain the rationale behind that in in a second but I\\'ll show you how easy it is to get a cluster up and  \\nrunning on on AWS without even interacting with AWS for free for example you can follow along by signing into coiled  \\ncloud to be clear this is not a necessity and it does involve you signing up to our product so I just wanted to be  \\nabsolutely transparent about that it does not involve any credit card information or anything  \\n \\n\\n#### 00:24:01,520::\\t\\t4 minutes Mark -> new paragraph \\n \\nalong those lines and in my opinion it does give a really nice example of how to run stuff on the cloud to do so you can  \\nsign in at cloud.coiled.io you can also pip install coiled and then do authentication you can also spin up this  \\nthis hosted coiled notebook so I\\'m going to spin that up now and I\\'m going to post that here actually yep I\\'m gonna post  \\nthat in the ch chat if you let me get this right if you\\'ve if you\\'ve never logged in to code before it\\'ll ask you to  \\nsign up using gmail or GitHub so feel free to do that if you\\'d like if not that\\'s also also cool but I just wanted to be  \\nexplicit about that the reason I want to do this is to show how Dask can be leveraged to do work on really large datasets  \\nso you will recall that I had between eight and nine gigs of ram on my local system. Oh wow! Anthony says on iPad  \\n\"unable to execute\" on Binder, incredible! I don\\'t have a strong sense of how Binder works on iPad. I do know that I was able  \\nto to check to use a Binder on my iPhone several years ago on my way to scipy doing code review for someone for Eric  \\nMaher I think for what that that\\'s worth but back to this we have this NYC taxi data set which is over 10 gigs it won\\'t  \\neven I can\\'t even store that in local memory I don\\'t have enough ram to store that so we do need either to do it locally  \\nin an out of core mode of some sort or we can we can burst to the cloud and we\\'re actually going to burst to the cloud  \\nusing using coiled so the notebook is running here for me and but I\\'m actually gonna do it from my local local notebook  \\nbut you\\'ll see and once again feel free to code along here it\\'s spinning up a notebook and James who is is my co-  \\ninstructor here is to be I\\'m I\\'m so grateful all the work is done on our notebooks in coiled you can launch the cluster  \\nhere and then analyze the entire over 10 gigs of data there I\\'m going to do it here so to do that I import coiled and  \\nthen I import the dash distributed stuff and then I can create my own software environment cluster configuration I\\'m not  \\ngoing to do that because the standard coiled cluster configuration software environment works now I\\'m going to spin up a  \\ncluster and instantiate a client now because we\\'re spinning up a cluster in in the cloud it\\'ll take it\\'ll take a minute  \\na minute or two enough Time to make a cup of coffee but it\\'s also enough Time for me to just talk a bit about why this  \\nis important and there are a lot of a lot of good good people working on on similar things but part of the motivation  \\nhere is that if you want to you don\\'t always want to do distributed data science OK first I\\'d ask you to look at instead  \\nof using dark if you can optimize your Pandas code right second I\\'d ask if you\\'ve got big data sets it\\'s a good question  \\ndo you actually need all the data so I would if you\\'re doing machine learning plot your learning curve see how accurate  \\nsee how your accuracy or whatever your metric of interest is improves as you increase the amount of data right and if it  \\nplateaus before you get to a large data size then you may as well most of the Time use your small data see if sub  \\nsampling can actually give you the results you need so you can get a bigger bigger access to a bigger machine so you  \\ndon\\'t have to burst to the cloud but after all these things if you do need to boast burst to the cloud until recently  \\nyou\\'ve had to get an AWS account you\\'ve had to you know set up containers with docker and or Kubernetes and do all of  \\nthese kind of  \\n \\n\\n#### 00:28:00,640::\\t\\t4 minutes Mark -> new paragraph \\n \\nI suppose devopsy software engineering foo stuff which which if you\\'re into that I I absolutely encourage you encourage  \\nyou to do that but a lot of working data scientists aren\\'t paid to do that and I don\\'t necessarily want to so that\\'s  \\nsomething we\\'re working on is thinking about these kind of one-click hosted deployments so you don\\'t have to do all of  \\nthat having said that I very much encourage you to try doing that stuff if if you\\'re interested we\\'ll see that the the  \\ncluster has just been created and what I\\'m going to do we see that oh I\\'m sorry I\\'ve done something funny here I\\'m I\\'m  \\nreferencing the previous client anna James yeah it looks like you should go ahead and connect a new client to the coil  \\ncluster and making sure not to re-execute the cluster creation exactly so would that be how would I what\\'s the call here  \\nI would just open up a new cell and say client equals capital client and then pass in the cluster like open parentheses  \\ncluster yeah great OK fantastic and what we\\'re seeing is a slight version this we don\\'t need to worry about this this is  \\nessentially saying that the environment on the cloud mis is there\\'s a slight mismatch with my with my local environment  \\nwe\\'re fine with that I\\'m going to look here for a certain reason the the dashboard isn\\'t quite working here at the  \\nmoment James would you suggest I just click on this and open a new yeah click on the ecs dashboard link oh yes fantastic  \\nso yep there\\'s some bug with the local dashboards that we\\'re we\\'re currently currently working on but what we\\'ll see now  \\njust a SEC I\\'m going to remove all of this we\\'ll see now that I have access to 10 workers I have access to 40 cores and  \\nI have access to over 170 gigs of memory OK so now I\\'m actually going to import this data set and it\\'s the entire year  \\nof data from 2019 and we\\'ll start seeing on on the diagnostics all the all the processing happening OK so oh actually  \\nnot yet because we haven\\'t called compute OK so it\\'s done this lazily we\\'ve imported it it shows kind of like Pandas  \\nwhen you show a data frame the column names and data types but it doesn\\'t show the data because we haven\\'t loaded it yet  \\nit does tell you how many partitions it is so essentially and we\\'ll see this soon das data frames correspond to  \\ncollections of Pandas data frames so they\\'re really 127 Pandas data frames underlying this task data frame so now I\\'m  \\ngoing to do the compute well I\\'m going to set myself up for the computation to do a group by passenger gown and look at  \\nthe main tip now that took a very small amount of Time we see the IPython magic Timing there because we haven\\'t computed  \\nit now we\\'re actually going to compute and James if you\\'ll see in the chat Eliana said her coil coiled authentication  \\nfailed I don\\'t know if you\\'re able to to help with that but if you are that would be great and it may be difficult to  \\ndebug in but look as we see we have the task stream now and we see how many you know we\\'ve got 40 cores working together  \\nwe saw the processing we saw the bytes stored it\\'s over 10 gigs as I said and we see we were able  \\n \\n\\n#### 00:32:01,519::\\t\\t4 minutes Mark -> new paragraph \\n \\nto do our basic analytics we were able to do it on a 10 plus gig data set in in 21.3 seconds which is pretty pretty  \\nexceptional if any any code based issues come up and they\\'re correlated in particular so if you have questions about the  \\ncode execution please ask in the Q&A not in the chat because others cannot vote it and I will definitively prioritize  \\nquestions on technical stuff particularly ones that up that are upvoted but yeah I totally agree thanks thanks very much  \\nso yeah let\\'s jump into into data frames so of course we write here that in the last exercise we used ask delayed to  \\nparallelize loading multiple csv files into a Pandas DataFrame we\\'re not we we haven\\'t done that but you can definitely  \\ngo through and have a look at that but I think perhaps even more immediately relevant for a data science crowd and an  \\nanalytics crowd is which is what I see here from the reasons people people have joined is jumping into Dask dataframes  \\nand as I said before, a Dask dataframe really feels like a Pandas data frame but internally it\\'s composed of many  \\ndifferent data frames this is one one way to think about it that we have all these Pandas data frames and the  \\ncollection of them is a dark data frame and as we saw before they\\'re partitioned we saw when we loaded the taxi data set  \\nin the dash data frame was 127 partitions right where each partition was a normal panda Pandas data frame and they can  \\nlive on disk as they did early in the first example dark in action or they can live on other machines as when I spun up  \\na coiled cluster and and did it on on AWS something I love about Dask data frames I mean I ran about this all the  \\ntime it\\'s how it\\'s the Pandas API and and Matt Matt Rocklin actually has a post on on the blog called a brief history of  \\nDask in which he talks about the technical goals of us but also talks about a social goal of task which in Matt\\'s words  \\nis to invent nothing he wanted and the team wanted the Dask API to be as comfortable and familiar for users as possible  \\nand that\\'s something I really appreciate about it so we see we have element element wires on operations we have the our  \\nfavorite row eyes selections we have loc we have the common aggregations we saw group buyers before we have is-ins we  \\nhave date Time string accessors oh James we forgot to I forgot to edit this and I it should be grouped by I don\\'t know  \\nwhat what a fruit buy is but that\\'s something we\\'ll make sure the next iteration to to get right at least we\\'ve got it  \\nright there and in the code but have a look at the dash data frame API docs to check out what\\'s happening and a lot of  \\nthe Time dash data frames can serve as drop in replacements for Pandas data frames the one thing that I just want to  \\nmake clear as I did before is that you need to call compute because of the lazy laser compute property of das so this is  \\nwonderful to talk about when to use data frames so if your data fits in memory use Pandas if your data fits in memory  \\nand your code doesn\\'t run super quickly I wouldn\\'t go to Dask I\\'d try to I\\'d do my best to optimize my Pandas code  \\nbefore trying to get gains gains and efficiency but dark itself becomes useful when the data set you want to analyze is  \\nlarger than your machine\\'s ram where you normally run into memory errors and that\\'s what we saw  \\n \\n\\n#### 00:36:01,520::\\t\\t4 minutes Mark -> new paragraph \\n \\nwith the taxicab example the other example that we\\'ll see when we get to [music] machine learning is you can do machine  \\nlearning on a small data set that fits in memory but if you\\'re building big models or training over like a lot of  \\ndifferent hyper parameters or different types of models you can you can parallelize that using using dark so there is  \\nyou know you want to use dash perhaps in the big data or medium to big data limit as we see here or in the medium to big  \\nmodel limit where training for example takes and takes a lot of Time OK so without further ado let\\'s get started with  \\ndas data frames you likely ran this preparation file to get the data in the previous notebook but if you didn\\'t execute  \\nthat now we\\'re going to get our file names by doing doing a few joins and we see our file is a string data NYC flights a  \\nwildcard to access all of them dot dot csv and we\\'re going to import our Dask.dataframe and read in our dataframe  \\nparsing some dates setting some sending some data types OK I\\'ll execute that we\\'ll see we have 10 partitions as we noted  \\nbefore if this was a Pandas data frame we\\'d see a bunch of entries here we don\\'t we see only the column names and the  \\ndata types of the columns and the reason is as we\\'ve said it explicitly here is the representation of the data frame  \\nobject contains no data it\\'s done Dask has done enough work to read the start of the file so that we know a bit about it  \\nsome of the important stuff and then further column types and column names and data types OK but we don\\'t once again we  \\ndon\\'t let\\'s say we\\'ve got 100 gigs of data we don\\'t want to like do this call and suddenly it\\'s reading all that stuff  \\nin and doing a whole bunch of compute until we explicitly tell it to OK now this is really cool if you know a bit of  \\nPandas you\\'ll know that you can there\\'s an attribute columns which prints out it\\'s well it\\'s actually the columns form  \\nan index right the Pandas index object and we get the we get the column names there cool Pandas in dark form we can  \\ncheck out the data types as well as we would in Pandas we see we\\'ve got some ins for the day of the week we\\'ve got some  \\nfloats for departure Time maybe we\\'d actually prefer that to be you know a date Time at some point we\\'ve got some  \\nobjects which generally are the most general on objects so generally strings so that\\'s all Pandasey type stuff in  \\naddition das data frames have an attribute n partitions which tells us the number of partitions and we saw before that  \\nthat\\'s 10 so I\\'d expect to see 10 here hey look at that now this is something that we talk about a lot in the delayed  \\nnotebook is really the task graph and I don\\'t want to say too much about that but really it\\'s a visual schematic of of  \\nthe order in which different types of compute happen OK and so the task graph for read csv tells us what happens when we  \\ncall compute and essentially it reads csv 10 ten Times zero indexed of course because Python it reads csv ten different  \\nTimes into these ten different Pandas Pandas data frames and if there were group buys or stuff after that we\\'d see them  \\nhappen in in the in the graph there and we may see an example of this in a second so once again as with Pandas we\\'re  \\ngoing to view the the head of the data frame great and we see a bunch of stuff you know we we see the first first five  \\nrows I\\'m actually also gonna gonna have a look at the  \\n \\n\\n#### 00:40:02,240::\\t\\t4 minutes Mark -> new paragraph \\n \\nthe tail the final five rows that may take longer because it\\'s accessing the the final I I there\\'s a joke and it may not  \\neven be a joke how much data analytics is actually biased by people looking at the first five rows before actually you  \\nknow interrogating the data more more seriously so how would all of our results look different if if our files were  \\nordered in in a different way that\\'s another conversation for a more philosophical conversation for another Time so now  \\nI want to show you some computations with dark data frames OK so since dash data frames implement a Pandas like API we  \\ncan just write our familiar Pandas codes so I want to look at the column departure delay and look at the maximum of that  \\ncolumn I\\'m going to call that max delay so you can see we\\'re selecting the column and then applying the max method as we  \\nwould with Pandas. Oh what happened there gives us some Dask scalar series and what\\'s happened is we haven\\'t called compute  \\nright so it hasn\\'t actually done the compute yet we\\'re going to do compute but first we\\'re going to visualize the task  \\ngraph like we did here and let\\'s try to reason what the task graph would look like right so the task graph first it\\'s  \\ngoing to read in all of these things and then it\\'ll probably perform this selector on each of these different Pandas  \\ndata frames comprising the dash data frame and then it will compute the max of each of those and then do a max on all  \\nthose maxes I think that\\'s what I would assume is happening here great so that\\'s what we\\'re what we\\'re doing we\\'re  \\nreading this so we read the first perform the first read csv get this das data frame get item I think is that selection  \\nthen we\\'re taking the max we\\'re doing the same for all of them then we take all of these max\\'s and aggregate them and  \\nthen take the max of that OK so that that\\'s essentially what\\'s happening when I call compute which I\\'m going to do now  \\nmoment of truth OK so that took around eight seconds and it tells us the max and I I\\'m sorry let\\'s let\\'s just get out  \\nsome of our dashboards up as well huh I think in this notebook we are using the single machine scheduler Hugo so I don\\'t  \\nthink there is a dashboard to be seen exactly yeah thank you for that that that catch James great is even better James  \\nwe have a question around using dark for reinforcement learning can you can you speak to that yeah so it depends on this  \\nI mean yeah short answer yes you can use GAs to train reinforcement learning models so there\\'s a package that Hugo will  \\ntalk about called Dask ML that we\\'ll see in the next notebook for distributing machine learning that paralyzes and and  \\ndistributes some existing models using desks so for instance things like random forces forest inside kit learn so so yes  \\nyou can use das to do distributed training for models I\\'m not actually sure if Dask ML implements any reinforcement  \\nlearning models in particular but that is certainly something that that can be done yeah and I\\'ll I\\'ll build on that by  \\nsaying we are about to jump into machine  \\n \\n\\n#### 00:44:00,000::\\t\\t4 minutes Mark -> new paragraph \\n \\nlearning I don\\'t think as James said I don\\'t think there\\'s reinforcement learning explicitly that that one can do but  \\nyou of course can use the das scheduler yourself to you know to distribute any reinforcement learning stuff you you have  \\nas well and that\\'s actually another another point to make that maybe James can speak to a bit more is that the dark team  \\nof course built all of these high-level collections and task arrays and dust data frames and were pleasantly surprised  \\nwhen you know maybe even up to half the people using dust came in all like we love all that but we\\'re going to use the  \\nscheduler for our own bespoke use cases right yeah exactly yeah the original intention was to like make basically a num  \\nlike a parallel numpy so that was like the desk array stuff like run run numpy and parallel on your laptop and and yeah  \\nso in order to do that we ended up building a distributed scheduler which sort of does arbitrary task computations so  \\nnot just things like you know parallel numpy but really whatever you\\'d like to throw at it and it turns out that ended  \\nup being really useful for people and so yeah now people use that sort of on their own just using the distributed  \\nscheduler to do totally custom algorithms in parallel in addition to these like nice collections like you saw Hugo  \\npresents the dash data frame API is you know the same as the panda\\'s API so there is this like familiar space you can  \\nuse things like the high-level collections but you can also run whatever custom like Hugo said bespoke computations you  \\nmight have exactly and it\\'s it\\'s been wonderful to see so many people so many people do that and the first thing as  \\nwe\\'ll see here the first thing to think about is if if you\\'re doing lifestyle compute if there\\'s anything you can you  \\nknow parallelize embarrassingly as they say right so just if you\\'re doing a hyper parameter search you just run some on  \\none worker and some on the other and there there\\'s no interaction effect so you don\\'t need to worry about that as  \\nopposed to if you\\'re trying to do you know train on streaming data where you may require it all to happen on on on the  \\nsame worker OK yeah so even think about trying to compute the standard deviation of a of a a univariate data set right  \\nin in that case you can\\'t just send you can\\'t just compute the standard deviation on two workers and then combine the  \\nresult in some some way you need to do something slightly slightly more nuanced and slightly slightly clever more clever  \\nI mean you still can actually in in that case but you can\\'t just do it as naively as that but so now we\\'re talking about  \\nparallel and distributed machine learning we have 20 minutes left so this is kind of going to be a whirlwind tour but  \\nyou know whirlwinds when safe exciting and informative I just want to make clear the material in this notebook is based  \\non the open source content from Dask\\'s tutorial repository as there\\'s a bunch of stuff we\\'ve shown you today the reason  \\nwe\\'ve done that is because they did it so well so I just want to give a shout out to all the das contributors OK so what  \\nwe\\'re going to do now is just break down machine learning scaling problems into two categories just review a bit of  \\nscikit-learn in passing solve a machine learning problem with single Michelle single Michelle I don\\'t know who she is  \\nbut single Michelle wow single machine and parallelism with scikit-learning joblib then solve an l problem with an ML  \\nproblem with multiple machines and parallelism using dark as well and we won\\'t have Time to burst for the cloud I don\\'t  \\nthink but you can also play play around with that OK so as I mentioned before when thinking about distributed compute a  \\nlot of people do it when they have large data they don\\'t necessarily think about the large model limit and this  \\nschematic kind of speaks to that if you\\'ve got a small model that fits in ram you don\\'t need to think about  \\n \\n\\n#### 00:48:00,480::\\t\\t4 minutes Mark -> new paragraph \\n \\ndistributed compute if your data size if your data is larger than your ram so your computer\\'s ram bound then you want to  \\nstart going to a distributed setting or if your model is big and CPU bound such as like large-scale hyper-parameter  \\nsearches or like ensemble blended models of like machine learning algorithms whatever it is and then of course we have  \\nthe you know big data big model limit where distributed computer desk is incredibly handy as I\\'m sure you could imagine  \\nOK and that\\'s really what I\\'ve what I\\'ve gone through here a bird\\'s-eye view of the strategies we think about if it\\'s in  \\nmemory in the bottom left quadrant just use scikit-learn or your favorite ML library otherwise known as scikit-learn  \\nfor me anyway I was going to make a note about XGBoost but I but I won\\'t for large models you can use joblib and your  \\nfavorite circuit learn estimator for large data sets use our dark ML estimators so we\\'re gonna do a whirlwind tour of  \\nscikit-learn in in five minutes we\\'re going to load in some data so we\\'ll actually generate it we\\'ll import scikit-  \\nlearn for our ML algorithm create an estimator and then check the accuracy of the model OK so once again I\\'m actually  \\ngoing to clear all outputs after restarting the kernel OK so this is a utility function of scikit-learn to create some  \\ndata sets so I\\'m going to make a classification data set with four features and 10 000 samples and just have a quick  \\nview of some of it so just a reminder on ML x is the samples matrix the size of x is the number of samples in terms of  \\nrows number of features as columns and then a feature or an attribute is what we\\'re trying to predict essentially OK so  \\nwhy is the predictor variable which we\\'re where which we\\'re or the target variable which we\\'re trying to predict so  \\nlet\\'s have a quick view of why it\\'s zeros and ones in in this case OK so yep that\\'s what I\\'ve said here why are the  \\ntargets which are real numbers for regression tasks or integers for classification or any other discrete sets of values  \\nno words about unsupervised learning at the moment we\\'re just going to support we\\'re going to fit a support vector  \\nclassifier for this example so let\\'s just load the appropriate scikit-learn module we don\\'t really need to discuss what  \\nsupport vector classifiers are at the moment now this is one of the very beautiful things about the scikit-learn API in  \\nterms of fitting the the model we instantiate a classifier and we want to fit it to the features with respect to the  \\ntarget OK so the first argument is the features second argument is the target variable so we\\'ve done that now I\\'m not  \\ngoing to worry about inspecting the learn features I just want to see how accurate it was OK and once we see how  \\naccurate it was I\\'m not gonna do this but then we can make a prediction right using estimator dot predict on a new a new  \\ndata set so this estimator will tell us so this score will tell us the accuracy and essentially that\\'s the proportion or  \\npercentage a fraction of the results that were that the estimator got correct and we\\'re doing this on the training data  \\nset we\\'ve just trained the model on this so this is telling us the accuracy on the on the training data set OK so it\\'s  \\n90  \\n \\n\\n#### 00:52:01,760::\\t\\t4 minutes Mark -> new paragraph \\n \\naccurate on the training data set if you dive into this a bit more you\\'ll recognize that if we we really want to know  \\nthe accuracy on a holdout set or a test set and it should be probably a bit lower because this is what we use to fit it  \\nOK but all that having been said I expect you know if if this is all resonating with you it means we can really move on  \\nto the distributed stuff in in a second but the other thing that that\\'s important to note is that we\\'ve trained it but a  \\nlot of model a lot of estimators and models have hyper parameters that affect the fit but you that we need to specify up  \\nfront instead of being learned during training so you know there\\'s a c parameter here there\\'s a are we using shrinking  \\nor not so we specify those we didn\\'t need to specify them because there are default values but here we specify them OK  \\nand then we\\'re going to look at the score now OK this is amazing we\\'ve got 50 accuracy which is the worst score possible  \\njust think about this if if you\\'ve got binary classification task and you\\'ve got 40 accuracy then you just flip the  \\nlabels and that changes to 60 accuracy so it\\'s amazing that we\\'ve actually hit 50 accuracy we\\'re to be congratulated on  \\nthat and what I want to note here is that we have two sets of hyper parameters we\\'ve used one\\'s created 90 actual model  \\nwith 90 accuracy another one one with 50 accuracy so we want to find the best hyper parameters essentially and that\\'s  \\nwhy hyper parameter optimization is is so important there are several ways to do hyper parameter optimization one is  \\ncalled grid search cross validation I won\\'t talk about cross validation it\\'s essentially a more robust analogue of train  \\ntest split where you train on a subset of your data and compute the accuracy on a test on a holdout set or a test set  \\ncross validation is a as I said a slightly more robust analog of this it\\'s called grid search because we have a grid of  \\nhyper parameters so we have you know in this case we have a hyper parameter c we have a hyper parameter kernel and we  \\ncan imagine them in a in a grid and we\\'re performing we\\'re checking out the score over all this gr over this entire grid  \\nof hyper parameters OK so to do that I import grid search csv now I\\'m going to compute the estimator over over these  \\ntrain the estimator over over this grid and as you see this is taking Time now OK and what I wanted to make clear and I  \\nthink should be becoming clearer now is that if we have a large hyper parameter sweep we want to do on a small data set  \\ndas can be useful for that OK because we can send some of the parameters to one worker some to another and they can  \\nperform them in parallel so that\\'s embarrassingly parallel because you\\'re you\\'re doing the same work as you would  \\notherwise but sending it to a bunch of different workers we saw that took 30 seconds which is in my realm of comfort as  \\na data scientist I\\'m happy to wait 30 seconds if I had to wait much longer if this grid was bigger I\\'d start to get  \\nprobably a bit frustrated but we see that it computed it for c is equal to all combinations of these essentially OK so  \\nthat\\'s really all I wanted to say there and then we can see the best parameters and the best score so the best score was  \\n0.098 and it was c10 and the kernel rbf a radial basis function it doesn\\'t even Matter what that is though for the  \\npurposes of this so we\\'ve got 10 minutes left we\\'re going to we\\'re going to make it I can feel it I have a good I have a  \\ngood sense  \\n \\n\\n#### 00:56:00,400::\\t\\t4 minutes Mark -> new paragraph \\n \\n a good after the I mean this demo is actually going incredibly well given the initial technical hurdles so touchwood  \\nHugo OK so what we\\'ve done is we\\'ve really segmented ML scaling problems into two categories CPU bound and ram bound and  \\nI I really I can\\'t emphasize that enough because I see so many people like jumping in to use new cool technologies  \\nwithout perhaps taking it being a bit mindful and intentional about it and reasoning about when things are useful and  \\nand when not I suppose the one point there is that sure data science is a technical discipline but there are a lot of  \\nother aspects to it involving this type of reasoning as well so we then carried out a typical sklearn workflow for ML  \\nproblems with small models and small data and we reviewed hyper parameters and hyper parameter optimization so in this  \\nsection we\\'ll see how joblib which is a set of tools to provide lightweight pipelining in Python gives us parallelism  \\non our laptop and then we\\'ll see how dark ML can give us awesome parallelism on on clusters OK so essentially what I\\'m  \\ndoing here is I\\'m doing exactly the same as above with a grid search but I\\'m using the quark the keyword argument n jobs  \\nwhich tells you how many tasks to run in parallel using the cause available on your local workstation and specifying  \\nminus one jobs means the it just runs them the maximum possible OK so I\\'m going to execute that great so we should be  \\ndone in a second feel free to ask any questions in the chat oh Alex has a great question in the Q&A does das have see a  \\nsequel and query optimizer I\\'m actually so excited that [music] and James maybe you can provide a couple of links to  \\nthis we\\'re really excited to have seen dark dust SQL developments there recently so that\\'s dark hyphen hyphen SQL and  \\nwe\\'re actually we\\'re working on some some content and a blog post and maybe a live live coding session about that in in  \\nthe near future so if anyone if you want updates from from coil feel free to go to our website and sign up for our  \\nmailing list and we\\'ll let you know about all of this type of stuff but the short answer is yes Alex and it\\'s getting  \\nbetter and if James is able to post post a link there that would be that would be fantastic so we\\'ve done link in the  \\nchat fantastic [music] and so we\\'ve we\\'ve seen how we have [music] single machine parallelism here using the using the  \\nend jobs quark and in the final minutes let\\'s see multiple multi-machine parallelism with Dask OK so what I\\'m going to  \\ndo is I\\'m going to do my imports and create my client incentive my client and check it out OK so once again I\\'m working  \\nlocally I hit search and that\\'ll task is pretty smart in terms of like knowing which which client I want to check out do  \\nthe tasks stream because it\\'s my favorite I\\'ll do the cluster map otherwise known as the pew pew map and then I want  \\nsome progress we all we all crave progress don\\'t we and maybe my workers tab OK great so we\\'ve got that up and running  \\nnow I\\'m going to do a slightly larger hyper parameter search OK so remember we had just a couple for c a couple for  \\nkernel we\\'re going to do more we have some for shrinking now I\\'m actually going to comment that out because I don\\'t know  \\nhow long that\\'s going to take if you\\'re coding them on Binder now this May actually take far far too long for you but  \\nwe\\'ll we\\'ll see so I\\'ll execute this code and we should see just sick no we shouldn\\'t see any work happening yet but  \\nwhat I\\'m doing here is oh looks like OK my clusters back up great we\\'re doing our grid search but we\\'re going to use  \\nDask as as the back end right and this is a context manager where we\\'re asserting that and and we can just discuss the  \\nthe syntax there but it\\'s not particularly important currently I\\'m going to execute this now and let\\'s see fantastic  \\nwe\\'ll see all this data transfer happening here we\\'ll see our tasks happening here we can see these big batches of fit  \\nand score fit so fitting fitting the models then finding how well they perform via this k-fold cross validation which is  \\nreally cool and let\\'s just yep we can see what\\'s happening here we can see we currently have 12 processing we\\'ve got  \\nseven in memory and we have several more that we need to do our desk workers we can see us oh we can see our CPU usage  \\nwe can see how we can see CPU usage across all the workers which is which is pretty cool seeing that distribution is is  \\nreally nice whenever some form of b swarm plot if you have enough would would be useful there or even some form of  \\ncumulative distribution function or something like that not a histogram people OK you can go to my Bayesian tutorial  \\nthat I\\'ve taught here before to hear me rave about the the horrors of histograms so we saw that talk a minute which is  \\ngreat and we split it across you know eight cores or whatever it is and now we\\'ll have a look once again we get the same  \\nbest performer which is which is a sanity check and that\\'s pretty cool I think we have a we actually have a few minutes  \\nleft so I am gonna just see if I can oh let me think yeah I will see if I can burst burst to the cloud and and and do  \\nthis that will take a minute a minute or two to create the cluster again but while we\\'re while we\\'re doing that I\\'m  \\nwondering if we have any any questions or if anyone has any feedback on on this workshop I very much welcome welcome  \\nthat perhaps if there are any final messages you\\'d you\\'d like to say James while we\\'re spinning this up you can you can  \\nlet me know yeah sure I just also first off wanted to say thanks everyone for attending and like bearing  \\n \\n\\n#### 01:04:01,119::\\t\\t4 minutes Mark -> new paragraph \\n \\nbearing with us with the technical difficulties really appreciate that real quick I\\'m just yeah so if you have if you  \\nhave questions please post in the Q&A section while the cold cluster\\'s spinning up Theodore posted in the last largest  \\nexample of grid search how much performance gain did we get from using das and not just in jobs hmm that\\'s a great  \\nquestion and we actually didn\\'t see let\\'s see so it took 80 seconds ah let me get this they\\'re actually not comparable  \\nbecause I did the grid search over a different set of hyper parameters I did it over a larger set of hyper parameters  \\nright so when I did end jobs I did it there were only it was a two by two grid of hyper parameters whereas when I did it  \\nwith with Dask it was a one two three four five six six by three so let\\'s just reason about that this one was eighteen  \\nsix by three is eighteen which took eighty seconds and this one was two by two so it was four and it took 26 seconds so  \\na minor gain I think with this hyper parameter search if you multiply that by by four you\\'ll well 4.2 4.5 you\\'ll need  \\nthat would have taken maybe two minutes or something something like that so we saw some increase in efficiency not a  \\ngreat deal but James maybe you can say more to this part of the reason for that is that we\\'re doing it on kind of a very  \\nsmall example so we won\\'t necessarily see the gains in efficiency with a data set this size and with a small hyper  \\nparameter suite like this is that right yeah yeah and yeah exactly and I guess also this is more of an kind of an  \\nillustrative point here I guess so you\\'re just using directly using in jobs with something like joblib by default we\\'ll  \\nuse local threads and processes on like whatever machine you happen to be running on so like in this case on Hugo\\'s  \\nlaptop one of the real advantages of using joblib with the das back in will actually dispatch back to to run tasks on a  \\nDask cluster is that your cluster can expand beyond what local resources you have so you can run you know you can  \\nbasically scale out like for instance using the coil cluster to have many many CPUs and a large amount of ram that you  \\nwouldn\\'t have on your locally table to run and there you\\'ll see both large performance gains as well as you\\'ll be able  \\nto expand your the set of possible problems you can solve to larger than ram scenarios so you\\'re out of out of core  \\ntraining exactly and thank you Jack this was absolutely unplanned and we didn\\'t plan that question but that\\'s a  \\nwonderful segue into me now performing exactly the same compute with the same code using the Dask as the parallel back  \\nend on a on a coiled cluster which is an AWS cluster right so we can I\\'m more currently anyway so I will execute this  \\ncode and it\\'s exactly the same as we did whoa OK great so we see our tasks task stream here you see once again we see  \\nthe majority is being batch fit and and getting the scores out similarly we see the same result being the best I\\'ll just  \\nnotice that for this for this small task doing it on the cloud took 20 seconds doing it locally for me took 80 seconds  \\nso that\\'s a four-fold increase in performance on a very small task so imagine what that does if you can take the same  \\ncode as you\\'ve written  \\n \\n\\n#### 01:08:00,240::\\t\\t4 minutes Mark -> new paragraph \\n \\nhere and burst to the cloud with with one click or however however you do it I think that that\\'s incredibly powerful and  \\nthat the fact that your code and what\\'s happening in the back end with Dask generalizes immediately to the new setting  \\nof working on a cluster I personally find very exciting and if you work with larger data sets or building larger models  \\nor big hyper parameter sweeps I\\'m pretty sure it\\'s an exciting option for all of you also so on that note I\\'d like to  \\nreiterate James what James said and thanking you all so much for joining us for asking great questions and for bearing  \\nwith us through some some technical technical hurdles but it made it even even funnier when when we got up and running  \\nonce again I\\'d love to thank Mark Christina and and the rest of the organizers for doing such a wonderful job and doing  \\nsuch a great service to the data science and machine learning community and ecosystem worldwide so thank you once again  \\nfor having us thank you Hugo and James I have to say like with all the technical difficulties I was actually giggling  \\nbecause it was kind of funny yeah but we\\'re very sorry and we thank you for your patience and sticking through it and I  \\nwill be editing this video to you know make it as efficient as possible and have that available Tim supercard thank you  \\ngreat and I\\'ll just ask you if you are interested in checking out coiled go to our website if you want to check out our  \\nproduct go to cloud.coil.io we started building this company in February we\\'re really excited about building a new  \\nproduct so if you\\'re interested reach out we\\'d love to chat with you about what we\\'re doing and what we\\'re up to and  \\nit\\'s wonderful to be in the same community as you all, so thanks!  \\n   '\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'<!-- Editing Guide: The pipe (|) position in this comment is 120:                                                       | -->\n",
      "### Introduction\n",
      "\n",
      "Okay hello and welcome to Data Umbrella's webinar for October so I'm just going to go over the agenda I'm going to do a  \n",
      "brief introduction then there will be the workshop by Hugo and James and you can ask questions along the way in the chat  \n",
      "or actually the best place to ask questions is the Q&A and there's an option to upvote as well so yet asking the Q&A if  \n",
      "you happen to post it on the chat by mistake I can also transfer it over to Q&A so that would be fine too and this  \n",
      "webinar is being recorded. Briefly about me. I am a statistician and data scientist and I am the founder of Data Umbrella.  \n",
      "I am on a lot of platforms as Reshama so feel free to follow me on Twitter and LinkedIn. We have a code of conduct we're  \n",
      "dedicated to providing a harassment-free experience for everyone. Thank you for helping to make this a welcoming friendly  \n",
      "professional community for all and this code of conduct applies to the chat as well. So our mission is to provide an  \n",
      "inclusive community for underrepresented persons in data science and we are an all volunteer-run organization you can  \n",
      "support Data Umbrella by doing the following things: You can follow our code of conduct and keep our community a place  \n",
      "where everybody wants to keep coming to; You can donate to our open collective and that helps to pay meet-up dues and  \n",
      "other operational costs and you can check out this link here on GitHub we have this new initiative where all the videos  \n",
      "are being transcribed and so is to make them more accessible. So we take the YouTube videos and we put the raw there and  \n",
      "so we've had a number of volunteers help us transcribe it so feel free to check out this link and maybe if you do this  \n",
      "video maybe the two speakers will follow you on Twitter. I can't promise anything, but it's possible Data Umbrella has a  \n",
      "job board and it's at jobs.org and once this gets started I'll put some links in the chat. The job that we are  \n",
      "highlighting today is the machine learning engineer job by Development Seed. Development Seed is based in  \n",
      "Washington DC and Lisbon Portugal and they do I'm going to go to the next slide what they do is they're doing social  \n",
      "good work and so they're doing for instance mapping elections from Afghanistan to the US analyzing public health and  \n",
      "economic data from Palestine to Illinois and leading the strategy and development behind data world bank and some other  \n",
      "organizations. I will share a link to their job posting in the chat as well as soon as I finish this brief  \n",
      "introduction. Check out our website for resources there's a lot of resources on learning Python and R also for  \n",
      "contributing to open source also for guides on accessibility and responsibility and allyship. We have a monthly  \n",
      "newsletter that goes out towards the end of the month and it has information on our upcoming events. We have two great  \n",
      "events coming up in November and December on open source so subscribe to our newsletter to be in the know. We are on all  \n",
      "social media platforms as Data Umbrella Meetup is the best place to join to find out about upcoming events our website  \n",
      "has resources follow us on Twitter we also share a lot of information on LinkedIn and if you want to subscribe to our  \n",
      "YouTube channel we record all of our talks and post them there within about a week of the talk so it's a good way to get  \n",
      "information. OK and now we are ready to get started so I will put myself on mute and I will hand it over to Hugo and James  \n",
      "and let you take over but thank you all for joining!   \n",
      " \n",
      "\n",
      "#### 00:04:03,120::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "I just want to thank Reshama Christina and everyone else who tied all the tireless effort that  \n",
      "that goes into putting these meet-ups and these online sessions together I think one thing I want to say is actually  \n",
      "the last in-person workshop I gave either at the end of February or early March was Data Umbrella's inaugural  \n",
      "tutorial and Meetup if I recall correctly on Bayesian thinking and hacker statistics and simulation and  \n",
      "that type of stuff so it's just wonderful to be back particularly with my colleague and friend James we're  \n",
      "building really cool distributed data science products at coiled we'll say a bit about that but we'll do some  \n",
      "introductions in a bit I just wanted to get you all accustomed to it was February thank you Reshama we're working  \n",
      "with Jupyter notebooks in a GitHub repository the repository is pinned to the top of the chat this is what it looks like  \n",
      "these are all the files this is the file system now we use something called Binder which is a project out of and related  \n",
      "to project Jupyter which provides infrastructure to run notebooks without any local installs so there are two  \n",
      "ways you can code along on this tutorial the first is and I won't get you to do this yet is to launch Binder the  \n",
      "reason I won't get you to do that yet is because once you launch it we have 10 minutes to start coding or the Binder  \n",
      "session Times out I've been burnt by that before actually several Times I'm surprised I even remembered it this time the  \n",
      "other thing you can do is install everything locally by cloning the repository downloading anaconda creating a conda  \n",
      "environment if you haven't done that I suggest you do not do that now and you launch the Binder James is going to  \n",
      "start by telling us a few things about GAs and distributed computing in general my question for you James is  \n",
      "if we get people to launch this now will we get to execute a cell code cell in 10 minutes I would let's hold off for now  \n",
      "maybe yep maybe I'll indicate when we should launch Binder OK fantastic cool and just what I'm looking at right now is  \n",
      "the GitHub repository on your browser OK exactly so I will not launch Binder now I will not get you to now I've I'm  \n",
      "doing this locally and we see that I'm in notebook zero and if you want to actually have a look at this notebook before  \n",
      "launching Binder, it's in the notebooks Data Umbrella subdirectory and it's notebook zero and we're going to hopefully  \n",
      "make it through the overview then chatting about Dask, Dask delayed and data framing and machine learning great so we  \n",
      "have Hashim has said you could open in VSCode as well. You could. I mean that would require all your local installs and  \n",
      "that that type of stuff as well but we're to introduce me and James we work at coiled where we build products for  \n",
      "distributed computing in infrastructure as we'll see one of the big problems with like bursting to the cloud is all the  \n",
      "like Kubernetes AWS docker stuff so we build a one-click host of deployments for das but for data science and machine  \n",
      "learning in general James maintains task along with Matt Matt Rocklin who created Dask with a team of people and was  \n",
      "working with Continuum Anaconda at the Time and James is a software engineer at coiled and I run data science evangelism  \n",
      "Marketing work on a bunch of product stuff as well wear a bunch of different hats occasionally.  \n",
      " \n",
      "\n",
      "#### 00:08:01,680::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "There are many ways to think about distributed compute and how to do it in in Python we're going to present  \n",
      "hey James you're muted I'm taking it I went away based on what I see in the chat you did you did but now we're back I've  \n",
      "introduced you I've introduced me I've mentioned that there are many ways to do distributed compute in the Python  \n",
      "ecosystem and we'll be chatting about one called Dask and maybe I'll pass you in a second but I'll say one thing that I  \n",
      "really like about my background isn't in distributed compute my background's in Pythonic data science when thinking  \n",
      "about bursting to larger data sets and larger models there are a variety of options the thing that took me attracted me  \n",
      "to desk originally I saw Cameron's note the ghost in the machine aren't playing nice tonight I think that ain't that the  \n",
      "truth is that dark plays so nicely with the entire PyData ecosystem so as we'll see if you want to write dash code for  \n",
      "data frames dash data frames it really mimics your Pandas code same with numpy same with scikit-learn OK and the other  \n",
      "thing is dark essentially runs the Python code under the hood so your mental model of what's happening is actually  \n",
      "corresponds to the code being being executed OK now I'd like to pass over to James but it looks like he's disappeared  \n",
      "again I'm still here if you can hear me I've just turned my camera off oh yeah OK great I'm gonna turn my camera  \n",
      "hopefully that will help yeah and I might do do the same for bandwidth bandwidth issues so if if you want to jump in and  \n",
      "and talk about dark at a high level I'm sharing my screen and we can scroll through yeah that sounds great so that's  \n",
      "sort of a nutshell you can think of it as being composed of two main well components the first we call collections these  \n",
      "are the user interfaces that you use to actually construct a computation you would like to compute in parallel or on  \n",
      "distributed hardware there are a few different interfaces that Dask implements. For instance, there's Dask array for doing  \n",
      "nd array computations there's Dask dataframe for working with tabular data you can think of those as like Dask array as  \n",
      "a parallel version of numpy. Dask dataframe has a parallel version of Pandas and so on there are also a couple other  \n",
      "interfaces that we'll be talking about das delayed for instance we'll talk about that today we'll also talk about the  \n",
      "futures API those are sort of for lower level custom algorithms in sort of paralyzing existing existing code the main  \n",
      "takeaway is that there are several sort of familiar APIs that desk implements and that will use today to actually  \n",
      "construct your computation so that's the first part of desk it is these dash collections you then take these collections  \n",
      "set up your steps for your computation and then pass them off to the second component which are desk schedulers and  \n",
      "these will actually go through and execute your computation potentially in parallel there are two flavors of schedulers  \n",
      "that desk offers the first is a are called single machine schedulers and these just take advantage of your local  \n",
      "hardware they will spin up a a local thread or process pool and start submitting tasks in your computation to to be  \n",
      "executed in parallel either on multiple threads or multiple processes there's also a distributed scheduler or maybe a  \n",
      "better term for would actually be called the advanced scheduler because it works well on a single machine but it also  \n",
      "scales out to multiple machines so for instance as you'll see later we will actually spin up a distributed scheduler  \n",
      "that has workers on remote  \n",
      " \n",
      "\n",
      "#### 00:12:00,160::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "machines on AWS so you can actually scale out beyond your local resources like say what's on your laptop kind of  \n",
      "scrolling down then to the image of the cluster we can see the main components of the distributed scheduler and James I  \n",
      "might get people to spin up the Binder now because we're going to execute codes now is a good point yep so just here's a  \n",
      "quick break point before you know a teaser for schedulers and what's happening there I'll ask you to in the repository  \n",
      "there's also the link to the Binder click on launch Binder I'm going to open it in a new tab and what this will create  \n",
      "is an environment in which you can just execute the code in in the notebooks OK so hopefully by the Time we've gotten  \n",
      "gone through this section this will be ready to start executing code so if everyone wants to do that to code along  \n",
      "otherwise just watch or if you're running things locally also cool thanks James yeah yeah no problem thank you so so  \n",
      "yeah looking at the image for the distributed scheduler we're not gonna have Time to go into the a lot of detail about  \n",
      "the distributed scheduler in this workshop so but we do want to provide at least a high level overview of the the  \n",
      "different parts and components of the distributed scheduler so the first part I want to talk about is in the diagram  \n",
      "what's labeled as a client so this is the user facing entry point to a cluster so wherever you are running your Python  \n",
      "session that could be in a Jupyter lab session like we are here that could be in a Python script somewhere you will  \n",
      "create and instantiate a client object that connects to the second component which is the das scheduler so each desk  \n",
      "cluster has a single scheduler in it that sort of keeps track of all of the state for all of the the state of your  \n",
      "cluster and all the tasks you'd like to compute so from your client you might start submitting tasks to the cluster the  \n",
      "schedule will receive those tasks and compute things like all the dependencies needed for that task like say you're  \n",
      "implementing you say you want to compute task c but that actually requires first you have to compute task b and task a  \n",
      "like there are some dependency structures there it'll compute those dependencies as well as keep track of them it'll  \n",
      "also communicate with all the workers to understand what worker is working on which task and as space frees up on the  \n",
      "workers it will start farming out new tasks to compute to the workers so in this particular diagram there are three das  \n",
      "distributed workers here however you can have as you can have thousands of workers if you'd like so the workers are the  \n",
      "things that actually compute the tasks they also store the results of your tasks and then serve them back to you and the  \n",
      "client the scheduler basically manages all the state needed to perform the computations and you submit tasks from the  \n",
      "client so that's sort of a quick whirlwind tour of the different components for the distributed scheduler and at this  \n",
      "point I think it'd be great to actually see see some of this in action Hugo would like to take over absolutely thank you  \n",
      "for that wonderful introduction to Dask and and the schedulers in particular and we are going to see that with dark in  \n",
      "action I'll just note that this tab in which I launched the Binder is up and running if you're going to execute code  \n",
      "here click on notebooks click on Data Umbrella oop and then go to the overview notebook and you can drag around we'll  \n",
      "see the utility of these these dashboards in a second but you can you know drag your stuff around to to make you know  \n",
      "however you want to want to structure it and then you can execute code in here I'm not going to do that I'm going to do  \n",
      "this locally at the moment but just to see dust in action to begin with I'm going to I'm actually going to  \n",
      " \n",
      "\n",
      "#### 00:16:02,720::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "restart kernel and clear my outputs so I'm going to import from dash distributed the client the sorry the other thing I  \n",
      "wanted to mention is we made a decision around content for this we do have a notebook that we we love to teach on  \n",
      "schedulers but we decided to switch it out for machine learning for this workshop in particular we are teaching a  \n",
      "similar although distinct workshop at PyData global so we may see some of you there in which we'll be going more in  \n",
      "depth into schedulers as well so if you want to check that out definitely do so we instantiate the client which as James  \n",
      "mentioned is kind of what we work with as the user to submit our code so that will take take a few seconds OK it's got a  \n",
      "port in you so it's going going elsewhere what I'll just first get you to notice is that it tells us where our dashboard  \n",
      "is and we'll see those tools in a second tells us about our cluster that we have four workers eight cores between eight  \n",
      "and nine gigs of of ram OK now this is something I really love about Dask all the diagnostic tools if I click on the  \n",
      "little desk thing here and we've modified the Binder so that that exists there as well we can see I'll hit search and it  \n",
      "should that now corresponds to the the scheduler now I want to look at the task stream which will tell us in real Time  \n",
      "what's happening I also want to look at the cluster map so we see here this is already really cool we've got all of our  \n",
      "workers around here and our scheduler scheduler there and when we start doing some compute we'll actually see  \n",
      "information flowing between these and the other thing maybe I'll yeah I'll include a little progress and that can be an  \n",
      "alternate tab to ask I'm wondering perhaps I also want to include something about the workers yeah OK great so we've got  \n",
      "a bunch of stuff that's that's pretty interesting there and so the next thing I'm going to do we've got a little utility  \n",
      "file which downloads some of the data and this is what it does is if you're in Binder it downloads a subset of the data  \n",
      "if you're anywhere else it loads a larger set for this particular example we're dealing with a small data set you see  \n",
      "the utility of dark and distributed compute when it generalizes to larger data sets but for pedagogical purposes we're  \n",
      "going to sit with a smaller data set so that we can actually run run the code there's a trade-off there so actually that  \n",
      "was already downloaded it seems but you should all see it download I'm actually going to run that in the Binder just to  \n",
      "you should start seeing downloading NYC flights data set done extracting creating json data etc OK now what we're going  \n",
      "to do is we're going to read in this data as a Dask data frame and what I want you to notice is that it really the das  \n",
      "code mimics Pandas code so instead of pd read csv we've got dd read csv we've got you know this is the file path the  \n",
      "first argument we're doing some parse date setting some data types OK we've got a little wild card regular expression  \n",
      "there to to join to do a bunch of them and then we're performing a group by OK so we're grouping by the origin of these  \n",
      "flight flight data we're looking at the the mean departure delay group by origin the the one difference I want to make  \n",
      "clear is that in das we need a compute method that's because das performs lazy computation it won't actually do anything  \n",
      "because you don't want it to do anything on really large data sets until you explicitly tell it tell it to compute so  \n",
      "I'm going to execute this now and we should see some information  \n",
      " \n",
      "\n",
      "#### 00:20:01,520::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "transfer between the scheduler and the workers and we should see tasks starting starting to be done OK so moment of  \n",
      "truth fantastic so we call this a pew pew plot because we see pew pew pew we saw a bunch of data transfer happening  \n",
      "between them these are all our cause and we can see tasks happening it tells us what tasks there are we can see that  \n",
      "most of the Time was spent reading csvs then we have some group bias on chunks and and that type of stuff so  \n",
      "that's a really nice diagnostic tool to see what most of your work is is actually doing under dark work as you can see  \n",
      "memory used CPU use more fine-grained examples there so I I'd love to know if in the Q&A I'm going to ask were you able  \n",
      "to execute this code and if you were in Binder just a thumb up a vote would be no would be fantastic much appreciated so  \n",
      "as we've mentioned I just wanted to say a few things about tutorial goals the goal is to cover the basics of dark and  \n",
      "distributed compute we'd love for you to walk away with an understanding of when to use it when to not what it has to  \n",
      "offer we're going to be covering the basics of Dask delayed which although not immediately applicable to data science  \n",
      "provides a wonderful framework for thinking about Dask how dark works and understanding how it works under the hood then  \n",
      "we're going to go into dark data frames and then machine learning hopefully due to the technical considerations with  \n",
      "we've got less Time than than we thought we would but we'll definitely do the best we can we may have less Time to do  \n",
      "exercises so we've had two people who are able to execute this code if you if you tried to execute it in Binder and were  \n",
      "not able to perhaps post that in the Q&A but we also have several exercises and I'd like you to take a minute just to do  \n",
      "this exercise the I I'm not asking you to do this because I want to know if you're able to print hello world I'm  \n",
      "essentially asking you to do it so you get a sense of how these exercises work so if you can take 30 seconds to print  \n",
      "hello world then we'll we'll move on after that so just take 30 seconds now and it seems like we have a few more people  \n",
      "who are able to execute code which which was great OK fantastic so you will put your solution there for some reason I  \n",
      "have an extra cell here so I'm just going to clip that and to see a solution I'll just get you to execute this cell and  \n",
      "it provides the solution and then we can execute it and compare it to the the output of what you had OK hello world so  \n",
      "as as we saw I've done all this locally you may have done it on Binder there is an option to work directly from the  \n",
      "cloud and I'll I'll take you through this there are many ways to do this as I mentioned we're working on one way with  \n",
      "coil and I'll explain the rationale behind that in in a second but I'll show you how easy it is to get a cluster up and  \n",
      "running on on AWS without even interacting with AWS for free for example you can follow along by signing into coiled  \n",
      "cloud to be clear this is not a necessity and it does involve you signing up to our product so I just wanted to be  \n",
      "absolutely transparent about that it does not involve any credit card information or anything  \n",
      " \n",
      "\n",
      "#### 00:24:01,520::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "along those lines and in my opinion it does give a really nice example of how to run stuff on the cloud to do so you can  \n",
      "sign in at cloud.coiled.io you can also pip install coiled and then do authentication you can also spin up this  \n",
      "this hosted coiled notebook so I'm going to spin that up now and I'm going to post that here actually yep I'm gonna post  \n",
      "that in the ch chat if you let me get this right if you've if you've never logged in to code before it'll ask you to  \n",
      "sign up using gmail or GitHub so feel free to do that if you'd like if not that's also also cool but I just wanted to be  \n",
      "explicit about that the reason I want to do this is to show how Dask can be leveraged to do work on really large datasets  \n",
      "so you will recall that I had between eight and nine gigs of ram on my local system. Oh wow! Anthony says on iPad  \n",
      "\"unable to execute\" on Binder, incredible! I don't have a strong sense of how Binder works on iPad. I do know that I was able  \n",
      "to to check to use a Binder on my iPhone several years ago on my way to scipy doing code review for someone for Eric  \n",
      "Maher I think for what that that's worth but back to this we have this NYC taxi data set which is over 10 gigs it won't  \n",
      "even I can't even store that in local memory I don't have enough ram to store that so we do need either to do it locally  \n",
      "in an out of core mode of some sort or we can we can burst to the cloud and we're actually going to burst to the cloud  \n",
      "using using coiled so the notebook is running here for me and but I'm actually gonna do it from my local local notebook  \n",
      "but you'll see and once again feel free to code along here it's spinning up a notebook and James who is is my co-  \n",
      "instructor here is to be I'm I'm so grateful all the work is done on our notebooks in coiled you can launch the cluster  \n",
      "here and then analyze the entire over 10 gigs of data there I'm going to do it here so to do that I import coiled and  \n",
      "then I import the dash distributed stuff and then I can create my own software environment cluster configuration I'm not  \n",
      "going to do that because the standard coiled cluster configuration software environment works now I'm going to spin up a  \n",
      "cluster and instantiate a client now because we're spinning up a cluster in in the cloud it'll take it'll take a minute  \n",
      "a minute or two enough Time to make a cup of coffee but it's also enough Time for me to just talk a bit about why this  \n",
      "is important and there are a lot of a lot of good good people working on on similar things but part of the motivation  \n",
      "here is that if you want to you don't always want to do distributed data science OK first I'd ask you to look at instead  \n",
      "of using dark if you can optimize your Pandas code right second I'd ask if you've got big data sets it's a good question  \n",
      "do you actually need all the data so I would if you're doing machine learning plot your learning curve see how accurate  \n",
      "see how your accuracy or whatever your metric of interest is improves as you increase the amount of data right and if it  \n",
      "plateaus before you get to a large data size then you may as well most of the Time use your small data see if sub  \n",
      "sampling can actually give you the results you need so you can get a bigger bigger access to a bigger machine so you  \n",
      "don't have to burst to the cloud but after all these things if you do need to boast burst to the cloud until recently  \n",
      "you've had to get an AWS account you've had to you know set up containers with docker and or Kubernetes and do all of  \n",
      "these kind of  \n",
      " \n",
      "\n",
      "#### 00:28:00,640::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "I suppose devopsy software engineering foo stuff which which if you're into that I I absolutely encourage you encourage  \n",
      "you to do that but a lot of working data scientists aren't paid to do that and I don't necessarily want to so that's  \n",
      "something we're working on is thinking about these kind of one-click hosted deployments so you don't have to do all of  \n",
      "that having said that I very much encourage you to try doing that stuff if if you're interested we'll see that the the  \n",
      "cluster has just been created and what I'm going to do we see that oh I'm sorry I've done something funny here I'm I'm  \n",
      "referencing the previous client anna James yeah it looks like you should go ahead and connect a new client to the coil  \n",
      "cluster and making sure not to re-execute the cluster creation exactly so would that be how would I what's the call here  \n",
      "I would just open up a new cell and say client equals capital client and then pass in the cluster like open parentheses  \n",
      "cluster yeah great OK fantastic and what we're seeing is a slight version this we don't need to worry about this this is  \n",
      "essentially saying that the environment on the cloud mis is there's a slight mismatch with my with my local environment  \n",
      "we're fine with that I'm going to look here for a certain reason the the dashboard isn't quite working here at the  \n",
      "moment James would you suggest I just click on this and open a new yeah click on the ecs dashboard link oh yes fantastic  \n",
      "so yep there's some bug with the local dashboards that we're we're currently currently working on but what we'll see now  \n",
      "just a SEC I'm going to remove all of this we'll see now that I have access to 10 workers I have access to 40 cores and  \n",
      "I have access to over 170 gigs of memory OK so now I'm actually going to import this data set and it's the entire year  \n",
      "of data from 2019 and we'll start seeing on on the diagnostics all the all the processing happening OK so oh actually  \n",
      "not yet because we haven't called compute OK so it's done this lazily we've imported it it shows kind of like Pandas  \n",
      "when you show a data frame the column names and data types but it doesn't show the data because we haven't loaded it yet  \n",
      "it does tell you how many partitions it is so essentially and we'll see this soon das data frames correspond to  \n",
      "collections of Pandas data frames so they're really 127 Pandas data frames underlying this task data frame so now I'm  \n",
      "going to do the compute well I'm going to set myself up for the computation to do a group by passenger gown and look at  \n",
      "the main tip now that took a very small amount of Time we see the IPython magic Timing there because we haven't computed  \n",
      "it now we're actually going to compute and James if you'll see in the chat Eliana said her coil coiled authentication  \n",
      "failed I don't know if you're able to to help with that but if you are that would be great and it may be difficult to  \n",
      "debug in but look as we see we have the task stream now and we see how many you know we've got 40 cores working together  \n",
      "we saw the processing we saw the bytes stored it's over 10 gigs as I said and we see we were able  \n",
      " \n",
      "\n",
      "#### 00:32:01,519::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "to do our basic analytics we were able to do it on a 10 plus gig data set in in 21.3 seconds which is pretty pretty  \n",
      "exceptional if any any code based issues come up and they're correlated in particular so if you have questions about the  \n",
      "code execution please ask in the Q&A not in the chat because others cannot vote it and I will definitively prioritize  \n",
      "questions on technical stuff particularly ones that up that are upvoted but yeah I totally agree thanks thanks very much  \n",
      "so yeah let's jump into into data frames so of course we write here that in the last exercise we used ask delayed to  \n",
      "parallelize loading multiple csv files into a Pandas DataFrame we're not we we haven't done that but you can definitely  \n",
      "go through and have a look at that but I think perhaps even more immediately relevant for a data science crowd and an  \n",
      "analytics crowd is which is what I see here from the reasons people people have joined is jumping into Dask dataframes  \n",
      "and as I said before, a Dask dataframe really feels like a Pandas data frame but internally it's composed of many  \n",
      "different data frames this is one one way to think about it that we have all these Pandas data frames and the  \n",
      "collection of them is a dark data frame and as we saw before they're partitioned we saw when we loaded the taxi data set  \n",
      "in the dash data frame was 127 partitions right where each partition was a normal panda Pandas data frame and they can  \n",
      "live on disk as they did early in the first example dark in action or they can live on other machines as when I spun up  \n",
      "a coiled cluster and and did it on on AWS something I love about Dask data frames I mean I ran about this all the  \n",
      "time it's how it's the Pandas API and and Matt Matt Rocklin actually has a post on on the blog called a brief history of  \n",
      "Dask in which he talks about the technical goals of us but also talks about a social goal of task which in Matt's words  \n",
      "is to invent nothing he wanted and the team wanted the Dask API to be as comfortable and familiar for users as possible  \n",
      "and that's something I really appreciate about it so we see we have element element wires on operations we have the our  \n",
      "favorite row eyes selections we have loc we have the common aggregations we saw group buyers before we have is-ins we  \n",
      "have date Time string accessors oh James we forgot to I forgot to edit this and I it should be grouped by I don't know  \n",
      "what what a fruit buy is but that's something we'll make sure the next iteration to to get right at least we've got it  \n",
      "right there and in the code but have a look at the dash data frame API docs to check out what's happening and a lot of  \n",
      "the Time dash data frames can serve as drop in replacements for Pandas data frames the one thing that I just want to  \n",
      "make clear as I did before is that you need to call compute because of the lazy laser compute property of das so this is  \n",
      "wonderful to talk about when to use data frames so if your data fits in memory use Pandas if your data fits in memory  \n",
      "and your code doesn't run super quickly I wouldn't go to Dask I'd try to I'd do my best to optimize my Pandas code  \n",
      "before trying to get gains gains and efficiency but dark itself becomes useful when the data set you want to analyze is  \n",
      "larger than your machine's ram where you normally run into memory errors and that's what we saw  \n",
      " \n",
      "\n",
      "#### 00:36:01,520::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "with the taxicab example the other example that we'll see when we get to [music] machine learning is you can do machine  \n",
      "learning on a small data set that fits in memory but if you're building big models or training over like a lot of  \n",
      "different hyper parameters or different types of models you can you can parallelize that using using dark so there is  \n",
      "you know you want to use dash perhaps in the big data or medium to big data limit as we see here or in the medium to big  \n",
      "model limit where training for example takes and takes a lot of Time OK so without further ado let's get started with  \n",
      "das data frames you likely ran this preparation file to get the data in the previous notebook but if you didn't execute  \n",
      "that now we're going to get our file names by doing doing a few joins and we see our file is a string data NYC flights a  \n",
      "wildcard to access all of them dot dot csv and we're going to import our Dask.dataframe and read in our dataframe  \n",
      "parsing some dates setting some sending some data types OK I'll execute that we'll see we have 10 partitions as we noted  \n",
      "before if this was a Pandas data frame we'd see a bunch of entries here we don't we see only the column names and the  \n",
      "data types of the columns and the reason is as we've said it explicitly here is the representation of the data frame  \n",
      "object contains no data it's done Dask has done enough work to read the start of the file so that we know a bit about it  \n",
      "some of the important stuff and then further column types and column names and data types OK but we don't once again we  \n",
      "don't let's say we've got 100 gigs of data we don't want to like do this call and suddenly it's reading all that stuff  \n",
      "in and doing a whole bunch of compute until we explicitly tell it to OK now this is really cool if you know a bit of  \n",
      "Pandas you'll know that you can there's an attribute columns which prints out it's well it's actually the columns form  \n",
      "an index right the Pandas index object and we get the we get the column names there cool Pandas in dark form we can  \n",
      "check out the data types as well as we would in Pandas we see we've got some ins for the day of the week we've got some  \n",
      "floats for departure Time maybe we'd actually prefer that to be you know a date Time at some point we've got some  \n",
      "objects which generally are the most general on objects so generally strings so that's all Pandasey type stuff in  \n",
      "addition das data frames have an attribute n partitions which tells us the number of partitions and we saw before that  \n",
      "that's 10 so I'd expect to see 10 here hey look at that now this is something that we talk about a lot in the delayed  \n",
      "notebook is really the task graph and I don't want to say too much about that but really it's a visual schematic of of  \n",
      "the order in which different types of compute happen OK and so the task graph for read csv tells us what happens when we  \n",
      "call compute and essentially it reads csv 10 ten Times zero indexed of course because Python it reads csv ten different  \n",
      "Times into these ten different Pandas Pandas data frames and if there were group buys or stuff after that we'd see them  \n",
      "happen in in the in the graph there and we may see an example of this in a second so once again as with Pandas we're  \n",
      "going to view the the head of the data frame great and we see a bunch of stuff you know we we see the first first five  \n",
      "rows I'm actually also gonna gonna have a look at the  \n",
      " \n",
      "\n",
      "#### 00:40:02,240::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "the tail the final five rows that may take longer because it's accessing the the final I I there's a joke and it may not  \n",
      "even be a joke how much data analytics is actually biased by people looking at the first five rows before actually you  \n",
      "know interrogating the data more more seriously so how would all of our results look different if if our files were  \n",
      "ordered in in a different way that's another conversation for a more philosophical conversation for another Time so now  \n",
      "I want to show you some computations with dark data frames OK so since dash data frames implement a Pandas like API we  \n",
      "can just write our familiar Pandas codes so I want to look at the column departure delay and look at the maximum of that  \n",
      "column I'm going to call that max delay so you can see we're selecting the column and then applying the max method as we  \n",
      "would with Pandas. Oh what happened there gives us some Dask scalar series and what's happened is we haven't called compute  \n",
      "right so it hasn't actually done the compute yet we're going to do compute but first we're going to visualize the task  \n",
      "graph like we did here and let's try to reason what the task graph would look like right so the task graph first it's  \n",
      "going to read in all of these things and then it'll probably perform this selector on each of these different Pandas  \n",
      "data frames comprising the dash data frame and then it will compute the max of each of those and then do a max on all  \n",
      "those maxes I think that's what I would assume is happening here great so that's what we're what we're doing we're  \n",
      "reading this so we read the first perform the first read csv get this das data frame get item I think is that selection  \n",
      "then we're taking the max we're doing the same for all of them then we take all of these max's and aggregate them and  \n",
      "then take the max of that OK so that that's essentially what's happening when I call compute which I'm going to do now  \n",
      "moment of truth OK so that took around eight seconds and it tells us the max and I I'm sorry let's let's just get out  \n",
      "some of our dashboards up as well huh I think in this notebook we are using the single machine scheduler Hugo so I don't  \n",
      "think there is a dashboard to be seen exactly yeah thank you for that that that catch James great is even better James  \n",
      "we have a question around using dark for reinforcement learning can you can you speak to that yeah so it depends on this  \n",
      "I mean yeah short answer yes you can use GAs to train reinforcement learning models so there's a package that Hugo will  \n",
      "talk about called Dask ML that we'll see in the next notebook for distributing machine learning that paralyzes and and  \n",
      "distributes some existing models using desks so for instance things like random forces forest inside kit learn so so yes  \n",
      "you can use das to do distributed training for models I'm not actually sure if Dask ML implements any reinforcement  \n",
      "learning models in particular but that is certainly something that that can be done yeah and I'll I'll build on that by  \n",
      "saying we are about to jump into machine  \n",
      " \n",
      "\n",
      "#### 00:44:00,000::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "learning I don't think as James said I don't think there's reinforcement learning explicitly that that one can do but  \n",
      "you of course can use the das scheduler yourself to you know to distribute any reinforcement learning stuff you you have  \n",
      "as well and that's actually another another point to make that maybe James can speak to a bit more is that the dark team  \n",
      "of course built all of these high-level collections and task arrays and dust data frames and were pleasantly surprised  \n",
      "when you know maybe even up to half the people using dust came in all like we love all that but we're going to use the  \n",
      "scheduler for our own bespoke use cases right yeah exactly yeah the original intention was to like make basically a num  \n",
      "like a parallel numpy so that was like the desk array stuff like run run numpy and parallel on your laptop and and yeah  \n",
      "so in order to do that we ended up building a distributed scheduler which sort of does arbitrary task computations so  \n",
      "not just things like you know parallel numpy but really whatever you'd like to throw at it and it turns out that ended  \n",
      "up being really useful for people and so yeah now people use that sort of on their own just using the distributed  \n",
      "scheduler to do totally custom algorithms in parallel in addition to these like nice collections like you saw Hugo  \n",
      "presents the dash data frame API is you know the same as the panda's API so there is this like familiar space you can  \n",
      "use things like the high-level collections but you can also run whatever custom like Hugo said bespoke computations you  \n",
      "might have exactly and it's it's been wonderful to see so many people so many people do that and the first thing as  \n",
      "we'll see here the first thing to think about is if if you're doing lifestyle compute if there's anything you can you  \n",
      "know parallelize embarrassingly as they say right so just if you're doing a hyper parameter search you just run some on  \n",
      "one worker and some on the other and there there's no interaction effect so you don't need to worry about that as  \n",
      "opposed to if you're trying to do you know train on streaming data where you may require it all to happen on on on the  \n",
      "same worker OK yeah so even think about trying to compute the standard deviation of a of a a univariate data set right  \n",
      "in in that case you can't just send you can't just compute the standard deviation on two workers and then combine the  \n",
      "result in some some way you need to do something slightly slightly more nuanced and slightly slightly clever more clever  \n",
      "I mean you still can actually in in that case but you can't just do it as naively as that but so now we're talking about  \n",
      "parallel and distributed machine learning we have 20 minutes left so this is kind of going to be a whirlwind tour but  \n",
      "you know whirlwinds when safe exciting and informative I just want to make clear the material in this notebook is based  \n",
      "on the open source content from Dask's tutorial repository as there's a bunch of stuff we've shown you today the reason  \n",
      "we've done that is because they did it so well so I just want to give a shout out to all the das contributors OK so what  \n",
      "we're going to do now is just break down machine learning scaling problems into two categories just review a bit of  \n",
      "scikit-learn in passing solve a machine learning problem with single Michelle single Michelle I don't know who she is  \n",
      "but single Michelle wow single machine and parallelism with scikit-learning joblib then solve an l problem with an ML  \n",
      "problem with multiple machines and parallelism using dark as well and we won't have Time to burst for the cloud I don't  \n",
      "think but you can also play play around with that OK so as I mentioned before when thinking about distributed compute a  \n",
      "lot of people do it when they have large data they don't necessarily think about the large model limit and this  \n",
      "schematic kind of speaks to that if you've got a small model that fits in ram you don't need to think about  \n",
      " \n",
      "\n",
      "#### 00:48:00,480::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "distributed compute if your data size if your data is larger than your ram so your computer's ram bound then you want to  \n",
      "start going to a distributed setting or if your model is big and CPU bound such as like large-scale hyper-parameter  \n",
      "searches or like ensemble blended models of like machine learning algorithms whatever it is and then of course we have  \n",
      "the you know big data big model limit where distributed computer desk is incredibly handy as I'm sure you could imagine  \n",
      "OK and that's really what I've what I've gone through here a bird's-eye view of the strategies we think about if it's in  \n",
      "memory in the bottom left quadrant just use scikit-learn or your favorite ML library otherwise known as scikit-learn  \n",
      "for me anyway I was going to make a note about XGBoost but I but I won't for large models you can use joblib and your  \n",
      "favorite circuit learn estimator for large data sets use our dark ML estimators so we're gonna do a whirlwind tour of  \n",
      "scikit-learn in in five minutes we're going to load in some data so we'll actually generate it we'll import scikit-  \n",
      "learn for our ML algorithm create an estimator and then check the accuracy of the model OK so once again I'm actually  \n",
      "going to clear all outputs after restarting the kernel OK so this is a utility function of scikit-learn to create some  \n",
      "data sets so I'm going to make a classification data set with four features and 10 000 samples and just have a quick  \n",
      "view of some of it so just a reminder on ML x is the samples matrix the size of x is the number of samples in terms of  \n",
      "rows number of features as columns and then a feature or an attribute is what we're trying to predict essentially OK so  \n",
      "why is the predictor variable which we're where which we're or the target variable which we're trying to predict so  \n",
      "let's have a quick view of why it's zeros and ones in in this case OK so yep that's what I've said here why are the  \n",
      "targets which are real numbers for regression tasks or integers for classification or any other discrete sets of values  \n",
      "no words about unsupervised learning at the moment we're just going to support we're going to fit a support vector  \n",
      "classifier for this example so let's just load the appropriate scikit-learn module we don't really need to discuss what  \n",
      "support vector classifiers are at the moment now this is one of the very beautiful things about the scikit-learn API in  \n",
      "terms of fitting the the model we instantiate a classifier and we want to fit it to the features with respect to the  \n",
      "target OK so the first argument is the features second argument is the target variable so we've done that now I'm not  \n",
      "going to worry about inspecting the learn features I just want to see how accurate it was OK and once we see how  \n",
      "accurate it was I'm not gonna do this but then we can make a prediction right using estimator dot predict on a new a new  \n",
      "data set so this estimator will tell us so this score will tell us the accuracy and essentially that's the proportion or  \n",
      "percentage a fraction of the results that were that the estimator got correct and we're doing this on the training data  \n",
      "set we've just trained the model on this so this is telling us the accuracy on the on the training data set OK so it's  \n",
      "90  \n",
      " \n",
      "\n",
      "#### 00:52:01,760::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "accurate on the training data set if you dive into this a bit more you'll recognize that if we we really want to know  \n",
      "the accuracy on a holdout set or a test set and it should be probably a bit lower because this is what we use to fit it  \n",
      "OK but all that having been said I expect you know if if this is all resonating with you it means we can really move on  \n",
      "to the distributed stuff in in a second but the other thing that that's important to note is that we've trained it but a  \n",
      "lot of model a lot of estimators and models have hyper parameters that affect the fit but you that we need to specify up  \n",
      "front instead of being learned during training so you know there's a c parameter here there's a are we using shrinking  \n",
      "or not so we specify those we didn't need to specify them because there are default values but here we specify them OK  \n",
      "and then we're going to look at the score now OK this is amazing we've got 50 accuracy which is the worst score possible  \n",
      "just think about this if if you've got binary classification task and you've got 40 accuracy then you just flip the  \n",
      "labels and that changes to 60 accuracy so it's amazing that we've actually hit 50 accuracy we're to be congratulated on  \n",
      "that and what I want to note here is that we have two sets of hyper parameters we've used one's created 90 actual model  \n",
      "with 90 accuracy another one one with 50 accuracy so we want to find the best hyper parameters essentially and that's  \n",
      "why hyper parameter optimization is is so important there are several ways to do hyper parameter optimization one is  \n",
      "called grid search cross validation I won't talk about cross validation it's essentially a more robust analogue of train  \n",
      "test split where you train on a subset of your data and compute the accuracy on a test on a holdout set or a test set  \n",
      "cross validation is a as I said a slightly more robust analog of this it's called grid search because we have a grid of  \n",
      "hyper parameters so we have you know in this case we have a hyper parameter c we have a hyper parameter kernel and we  \n",
      "can imagine them in a in a grid and we're performing we're checking out the score over all this gr over this entire grid  \n",
      "of hyper parameters OK so to do that I import grid search csv now I'm going to compute the estimator over over these  \n",
      "train the estimator over over this grid and as you see this is taking Time now OK and what I wanted to make clear and I  \n",
      "think should be becoming clearer now is that if we have a large hyper parameter sweep we want to do on a small data set  \n",
      "das can be useful for that OK because we can send some of the parameters to one worker some to another and they can  \n",
      "perform them in parallel so that's embarrassingly parallel because you're you're doing the same work as you would  \n",
      "otherwise but sending it to a bunch of different workers we saw that took 30 seconds which is in my realm of comfort as  \n",
      "a data scientist I'm happy to wait 30 seconds if I had to wait much longer if this grid was bigger I'd start to get  \n",
      "probably a bit frustrated but we see that it computed it for c is equal to all combinations of these essentially OK so  \n",
      "that's really all I wanted to say there and then we can see the best parameters and the best score so the best score was  \n",
      "0.098 and it was c10 and the kernel rbf a radial basis function it doesn't even Matter what that is though for the  \n",
      "purposes of this so we've got 10 minutes left we're going to we're going to make it I can feel it I have a good I have a  \n",
      "good sense  \n",
      " \n",
      "\n",
      "#### 00:56:00,400::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      " a good after the I mean this demo is actually going incredibly well given the initial technical hurdles so touchwood  \n",
      "Hugo OK so what we've done is we've really segmented ML scaling problems into two categories CPU bound and ram bound and  \n",
      "I I really I can't emphasize that enough because I see so many people like jumping in to use new cool technologies  \n",
      "without perhaps taking it being a bit mindful and intentional about it and reasoning about when things are useful and  \n",
      "and when not I suppose the one point there is that sure data science is a technical discipline but there are a lot of  \n",
      "other aspects to it involving this type of reasoning as well so we then carried out a typical sklearn workflow for ML  \n",
      "problems with small models and small data and we reviewed hyper parameters and hyper parameter optimization so in this  \n",
      "section we'll see how joblib which is a set of tools to provide lightweight pipelining in Python gives us parallelism  \n",
      "on our laptop and then we'll see how dark ML can give us awesome parallelism on on clusters OK so essentially what I'm  \n",
      "doing here is I'm doing exactly the same as above with a grid search but I'm using the quark the keyword argument n jobs  \n",
      "which tells you how many tasks to run in parallel using the cause available on your local workstation and specifying  \n",
      "minus one jobs means the it just runs them the maximum possible OK so I'm going to execute that great so we should be  \n",
      "done in a second feel free to ask any questions in the chat oh Alex has a great question in the Q&A does das have see a  \n",
      "sequel and query optimizer I'm actually so excited that [music] and James maybe you can provide a couple of links to  \n",
      "this we're really excited to have seen dark dust SQL developments there recently so that's dark hyphen hyphen SQL and  \n",
      "we're actually we're working on some some content and a blog post and maybe a live live coding session about that in in  \n",
      "the near future so if anyone if you want updates from from coil feel free to go to our website and sign up for our  \n",
      "mailing list and we'll let you know about all of this type of stuff but the short answer is yes Alex and it's getting  \n",
      "better and if James is able to post post a link there that would be that would be fantastic so we've done link in the  \n",
      "chat fantastic [music] and so we've we've seen how we have [music] single machine parallelism here using the using the  \n",
      "end jobs quark and in the final minutes let's see multiple multi-machine parallelism with Dask OK so what I'm going to  \n",
      "do is I'm going to do my imports and create my client incentive my client and check it out OK so once again I'm working  \n",
      "locally I hit search and that'll task is pretty smart in terms of like knowing which which client I want to check out do  \n",
      "the tasks stream because it's my favorite I'll do the cluster map otherwise known as the pew pew map and then I want  \n",
      "some progress we all we all crave progress don't we and maybe my workers tab OK great so we've got that up and running  \n",
      "now I'm going to do a slightly larger hyper parameter search OK so remember we had just a couple for c a couple for  \n",
      "kernel we're going to do more we have some for shrinking now I'm actually going to comment that out because I don't know  \n",
      "how long that's going to take if you're coding them on Binder now this May actually take far far too long for you but  \n",
      "we'll we'll see so I'll execute this code and we should see just sick no we shouldn't see any work happening yet but  \n",
      "what I'm doing here is oh looks like OK my clusters back up great we're doing our grid search but we're going to use  \n",
      "Dask as as the back end right and this is a context manager where we're asserting that and and we can just discuss the  \n",
      "the syntax there but it's not particularly important currently I'm going to execute this now and let's see fantastic  \n",
      "we'll see all this data transfer happening here we'll see our tasks happening here we can see these big batches of fit  \n",
      "and score fit so fitting fitting the models then finding how well they perform via this k-fold cross validation which is  \n",
      "really cool and let's just yep we can see what's happening here we can see we currently have 12 processing we've got  \n",
      "seven in memory and we have several more that we need to do our desk workers we can see us oh we can see our CPU usage  \n",
      "we can see how we can see CPU usage across all the workers which is which is pretty cool seeing that distribution is is  \n",
      "really nice whenever some form of b swarm plot if you have enough would would be useful there or even some form of  \n",
      "cumulative distribution function or something like that not a histogram people OK you can go to my Bayesian tutorial  \n",
      "that I've taught here before to hear me rave about the the horrors of histograms so we saw that talk a minute which is  \n",
      "great and we split it across you know eight cores or whatever it is and now we'll have a look once again we get the same  \n",
      "best performer which is which is a sanity check and that's pretty cool I think we have a we actually have a few minutes  \n",
      "left so I am gonna just see if I can oh let me think yeah I will see if I can burst burst to the cloud and and and do  \n",
      "this that will take a minute a minute or two to create the cluster again but while we're while we're doing that I'm  \n",
      "wondering if we have any any questions or if anyone has any feedback on on this workshop I very much welcome welcome  \n",
      "that perhaps if there are any final messages you'd you'd like to say James while we're spinning this up you can you can  \n",
      "let me know yeah sure I just also first off wanted to say thanks everyone for attending and like bearing  \n",
      " \n",
      "\n",
      "#### 01:04:01,119::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "bearing with us with the technical difficulties really appreciate that real quick I'm just yeah so if you have if you  \n",
      "have questions please post in the Q&A section while the cold cluster's spinning up Theodore posted in the last largest  \n",
      "example of grid search how much performance gain did we get from using das and not just in jobs hmm that's a great  \n",
      "question and we actually didn't see let's see so it took 80 seconds ah let me get this they're actually not comparable  \n",
      "because I did the grid search over a different set of hyper parameters I did it over a larger set of hyper parameters  \n",
      "right so when I did end jobs I did it there were only it was a two by two grid of hyper parameters whereas when I did it  \n",
      "with with Dask it was a one two three four five six six by three so let's just reason about that this one was eighteen  \n",
      "six by three is eighteen which took eighty seconds and this one was two by two so it was four and it took 26 seconds so  \n",
      "a minor gain I think with this hyper parameter search if you multiply that by by four you'll well 4.2 4.5 you'll need  \n",
      "that would have taken maybe two minutes or something something like that so we saw some increase in efficiency not a  \n",
      "great deal but James maybe you can say more to this part of the reason for that is that we're doing it on kind of a very  \n",
      "small example so we won't necessarily see the gains in efficiency with a data set this size and with a small hyper  \n",
      "parameter suite like this is that right yeah yeah and yeah exactly and I guess also this is more of an kind of an  \n",
      "illustrative point here I guess so you're just using directly using in jobs with something like joblib by default we'll  \n",
      "use local threads and processes on like whatever machine you happen to be running on so like in this case on Hugo's  \n",
      "laptop one of the real advantages of using joblib with the das back in will actually dispatch back to to run tasks on a  \n",
      "Dask cluster is that your cluster can expand beyond what local resources you have so you can run you know you can  \n",
      "basically scale out like for instance using the coil cluster to have many many CPUs and a large amount of ram that you  \n",
      "wouldn't have on your locally table to run and there you'll see both large performance gains as well as you'll be able  \n",
      "to expand your the set of possible problems you can solve to larger than ram scenarios so you're out of out of core  \n",
      "training exactly and thank you Jack this was absolutely unplanned and we didn't plan that question but that's a  \n",
      "wonderful segue into me now performing exactly the same compute with the same code using the Dask as the parallel back  \n",
      "end on a on a coiled cluster which is an AWS cluster right so we can I'm more currently anyway so I will execute this  \n",
      "code and it's exactly the same as we did whoa OK great so we see our tasks task stream here you see once again we see  \n",
      "the majority is being batch fit and and getting the scores out similarly we see the same result being the best I'll just  \n",
      "notice that for this for this small task doing it on the cloud took 20 seconds doing it locally for me took 80 seconds  \n",
      "so that's a four-fold increase in performance on a very small task so imagine what that does if you can take the same  \n",
      "code as you've written  \n",
      " \n",
      "\n",
      "#### 01:08:00,240::\t\t4 minutes Mark -> new paragraph \n",
      " \n",
      "here and burst to the cloud with with one click or however however you do it I think that that's incredibly powerful and  \n",
      "that the fact that your code and what's happening in the back end with Dask generalizes immediately to the new setting  \n",
      "of working on a cluster I personally find very exciting and if you work with larger data sets or building larger models  \n",
      "or big hyper parameter sweeps I'm pretty sure it's an exciting option for all of you also so on that note I'd like to  \n",
      "reiterate James what James said and thanking you all so much for joining us for asking great questions and for bearing  \n",
      "with us through some some technical technical hurdles but it made it even even funnier when when we got up and running  \n",
      "once again I'd love to thank Mark Christina and and the rest of the organizers for doing such a wonderful job and doing  \n",
      "such a great service to the data science and machine learning community and ecosystem worldwide so thank you once again  \n",
      "for having us thank you Hugo and James I have to say like with all the technical difficulties I was actually giggling  \n",
      "because it was kind of funny yeah but we're very sorry and we thank you for your patience and sticking through it and I  \n",
      "will be editing this video to you know make it as efficient as possible and have that available Tim supercard thank you  \n",
      "great and I'll just ask you if you are interested in checking out coiled go to our website if you want to check out our  \n",
      "product go to cloud.coil.io we started building this company in February we're really excited about building a new  \n",
      "product so if you're interested reach out we'd love to chat with you about what we're doing and what we're up to and  \n",
      "it's wonderful to be in the same community as you all, so thanks!  \n",
      "   '\n"
     ]
    }
   ],
   "source": [
    "print(T)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def redo_transcript_cleanup(formatted_transcript):\n",
    "    \"\"\"\n",
    "    Return cleaned up event_dict['formatted_transcript'].\n",
    "    Wrapper to reprocess the current transcript.\n",
    "    Use case: At least one of the text-processing files\n",
    "    was amended => repeat text cleanup.\n",
    "    \"\"\"\n",
    "    if not len(formatted_transcript):\n",
    "        return ''\n",
    "\n",
    "    comment_line = ''\n",
    "    line1_idx = formatted_transcript.find('\\n')\n",
    "    if line1_idx > 0 :\n",
    "        line1 = formatted_transcript[:line1_idx]\n",
    "        comment_line = line1 if line1.startswith('<!--') else ''\n",
    "        formatted_transcript = formatted_transcript[line1_idx:]\n",
    "\n",
    "    txt_len = len(formatted_transcript)\n",
    "    chunksize = (1024 * 4)\n",
    "    n_chunks = int(np.ceil(txt_len/chunksize))\n",
    "\n",
    "    uppercase_list = TRX.readcsv(TRX.upper_file).upper.tolist()\n",
    "    titlecase_list = (TRX.readcsv(TRX.people_file).people.tolist()\n",
    "                    + TRX.readcsv(TRX.names_file).names.tolist()\n",
    "                    + TRX.readcsv(TRX.places_file).places.tolist())\n",
    "    corrections = TRX.get_corrections_dict()\n",
    "\n",
    "    md_parags = []\n",
    "    prev_e = 0\n",
    "    if n_chunks > 1:\n",
    "        for e in range(chunksize, txt_len, chunksize):\n",
    "            parag = formatted_transcript[prev_e:e]\n",
    "            new_parag = TRX.clean_text(parag,\n",
    "                                       uppercase_list,\n",
    "                                       titlecase_list,\n",
    "                                       corrections)\n",
    "            md_parags.append(new_parag)\n",
    "            prev_e = e\n",
    "    #last one (or only one):\n",
    "    parag = formatted_transcript[prev_e:]\n",
    "    new_parag = TRX.clean_text(parag,\n",
    "                               uppercase_list,\n",
    "                               titlecase_list,\n",
    "                               corrections)\n",
    "    md_parags.append(new_parag)\n",
    "    new_txt = comment_line + \"\".join(md_parags)\n",
    "    return new_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_transcript = AC.PC.page.children[1].children[1].value  # trx text\n",
    "txt_len = len(formatted_transcript)\n",
    "chunksize = (1024 * 4)\n",
    "n_chunks = TRX.math.ceil(txt_len/chunksize)\n",
    "n_chunks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "formatted_transcript = AC.PC.page.children[1].children[1].value  # trx text\n",
    "formatted_transcript[:126]\n",
    "#len('<!-- Editing Guide: The pipe (|) position in this comment is 120:                                                       | -->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "txa = AC.PC.page.children[1].children[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gui.left_sidebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names \n",
      " 'bayesian'\n"
     ]
    }
   ],
   "source": [
    "grid = AC.PC.page.children[0]\n",
    "footer_g = grid.children[3].children[0].children[0]\n",
    "v_file = footer_g.children[0].children[0].value\n",
    "v_entries = footer_g.children[0].children[1].children[1].value or None\n",
    "print(v_file, '\\n', v_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bayesian']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid, msg = CTR.validate_user_list(v_entries, v_file)\n",
    "print(msg)\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee206fb5f4a4849b277aff6053575da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black', height='160px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "class OutputWidgetHandler(logging.Handler):\n",
    "    \"\"\" Custom logging handler sending logs to an output widget \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(OutputWidgetHandler, self).__init__(*args, **kwargs)\n",
    "        layout = {\n",
    "            'width': '100%',\n",
    "            'height': '160px',\n",
    "            'border': '1px solid black'\n",
    "        }\n",
    "        self.out = ipw.Output(layout=layout)\n",
    "\n",
    "    def emit(self, record):\n",
    "        \"\"\" Overload of logging.Handler method \"\"\"\n",
    "        formatted_record = self.format(record)\n",
    "        new_output = {\n",
    "            'name': 'stdout',\n",
    "            'output_type': 'stream',\n",
    "            'text': formatted_record+'\\n'\n",
    "        }\n",
    "        self.out.outputs = (new_output, ) + self.out.outputs\n",
    "\n",
    "    def show_logs(self):\n",
    "        \"\"\" Show the logs \"\"\"\n",
    "        display(self.out)\n",
    "\n",
    "    def clear_logs(self):\n",
    "        \"\"\" Clear the current logs \"\"\"\n",
    "        self.out.clear_output()\n",
    "\n",
    "\n",
    "logger = logging.getLogger('Audit_Tests')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = OutputWidgetHandler()\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s  - [%(levelname)s] %(message)s'))\n",
    "logger.addHandler(handler)\n",
    "\n",
    "handler.show_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.clear_logs()\n",
    "logger.info('Starting program')\n",
    "\n",
    "try:\n",
    "    logger.info('About to try something dangerous...')\n",
    "    1.0/0.0\n",
    "except Exception as e:\n",
    "    logger.exception('Oops: ',exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_string.Textarea"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_box.GridBox"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_box.HBox"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_box.VBox"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_selectioncontainer.Accordion"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = AC.PC.page.children[0]  #grid header\n",
    "type(grid)\n",
    "main = grid.children[1]\n",
    "type(main)\n",
    "main_vbx1 = main.children[2]\n",
    "type(main_vbx1)\n",
    "main_out_idn = main_vbx1.children[0].outputs[0]['text'][5:-1]\n",
    "footer = grid.children[3]\n",
    "footer_acc = footer.children[0]\n",
    "type(footer_acc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "footer_acc.selected_index = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Current EDIT tasks:\n",
    "* Problem: cannot disable footer or its Accordion => footer loading done by ckick_btn_load()?\n",
    "* Problem: reprocessing the transcript after text-processing files update should not entails redoing the processing of the xml_captions.\n",
    "* FEAT: Only the MODIFY page will have the option to change the time-chunking parameter (which is in the xml file); right now: not exposed.\n",
    "\n",
    "## Solution?\n",
    "* Current: TR.redo_initial_transcript() is used\n",
    "* Needed: TR.YT.reprocess_text() => existing_transcript = initial_transcript + all changes saved by Editor => prior changes will be kept\n",
    "\n",
    "### => refactor self.xml_caption_to_text\n",
    "\n",
    "\n",
    "- Extract wrap function\n",
    "- Remove TW from clean_text\n",
    "- Add do_wrap param to clean_text\n",
    "\n",
    "### Limitation: only the remaining lowercase text will be affected since all replacements assume lowercase text.\n",
    "```\n",
    "self.YT.get_initial_transcript(replace=True)\n",
    "def reprocess_text(txt):\n",
    "    \"\"\"\n",
    "    txt could be from self.txa_editarea.value or get_transcript_text(self)\n",
    "    \"\"\"\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_box.GridBox"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_box.HBox"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ipywidgets.widgets.widget_output.Output"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'05-mini-max-demo-foo.md'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = AC.PC.page.children[0]  #grid header\n",
    "type(grid)\n",
    "main = grid.children[1]\n",
    "type(main)\n",
    "main_out_idn = main.children[2]\n",
    "type(main_out_idn)\n",
    "main_out_idn.outputs[0]['text'][5:-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_form = AC.PC.page.children[1].children[0]\n",
    "input_form "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Test: Horizontal RadioButtons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39b3d57ecf54e8e8704fa260c52e3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(flex_flow='row'), options=('Audio', 'Video'), value='Audio')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lo_radio = ipw.Layout(flex_flow='row')\n",
    "av_radio2 = ipw.RadioButtons(options=['Audio','Video'], value='Audio',\n",
    "                            layout=lo_radio)\n",
    "av_radio2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "ctr_kids = len(app.center.children) # n tabs\n",
    "for k in range(ctr_kids):\n",
    "    type(app.center.children[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Fixed: problem with event numbering from df :: new event dict in Meta\n",
    "```\n",
    "    def new_event_dict(self):\n",
    "        \"\"\"\n",
    "        Create a 'starter' event dict with event id generated\n",
    "        from the readme table df.\n",
    "        \"\"\"\n",
    "        new_dict = self.get_event_dict()\n",
    "\n",
    "        # Update dict with defaults:\n",
    "        new_dict['year'] = self.year\n",
    "        new = self.df.index.argmax() + self.row_offset\n",
    "        self.idn = idn_frmt(new)\n",
    "        new_dict['idn'] = self.idn\n",
    "        new_dict['transcriber'] = '?'\n",
    "        new_dict['extra_references'] = ''\n",
    "        new_dict['has_transcript'] = False\n",
    "        new_dict['status'] = TrStatus.TODO.value\n",
    "        new_dict['notes'] = ''\n",
    "        new_dict['video_href_w'] = DEF_IMG_W #thumbnail\n",
    "        \n",
    "        v1 = self.insertion_idx(HDR_TPL.format(**new_dict))\n",
    "        new_dict['trans_idx'] = v1\n",
    "        return new_dict\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "1. Produce the program flow chart depending on user status, e.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Utils for documenting the project - Networkx, Graphviz needed\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import pydot_layout\n",
    "filter_dir(nx.drawing.nx_pydot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: https://nbviewer.jupyter.org/github/xflr6/graphviz/blob/master/examples/notebook.ipynb\n",
    "\n",
    "import os\n",
    "from graphviz import Digraph, Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dir(Digraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Digraph?\n",
    "Init signature:\n",
    "Digraph(\n",
    "    name=None,\n",
    "    comment=None,\n",
    "    filename=None,\n",
    "    directory=None,\n",
    "    format=None,\n",
    "    engine=None,\n",
    "    encoding='utf-8',\n",
    "    graph_attr=None,\n",
    "    node_attr=None,\n",
    "    edge_attr=None,\n",
    "    body=None,\n",
    "    strict=False,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Digraph.render?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROGRAMFILES']\n",
    "os.environ['CONDA_PREFIX']\n",
    "#C:\\Program Files\\Graphviz 2.44.1\\bin"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "last = 'C:\\\\Users\\\\catch\\\\Anaconda3\\\\envs\\\\p37\\\\Library\\\\bin\\\\graphviz'\n",
    "len(last)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.environ[\"PATH\"] = os.environ[\"PATH\"][:-54]\n",
    "os.environ[\"PATH\"][-60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_gv_envir():\n",
    "    \"\"\" Ad-hoc fix to have Graphiz (v2.38) working on my system. \n",
    "    Note that in case the error ExecutableNotFound occurs, the path to \n",
    "    graphviz must be added to the PATH variable, e.g:\n",
    "    > \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" \n",
    "    > \"ExecutableNotFound: \n",
    "       failed to execute ['dot', '-Tsvg'], make sure the Graphviz executables are\n",
    "       on your systems' PATH\"\n",
    "    The above is not sufficient: the error occurred even though graphviz, dot and\n",
    "    neato are all on my system path.\n",
    "    Calling this function on failed `try` solved the problem. (?)\n",
    "\"\"\"\n",
    "    gviz = os.path.join(os.environ['PROGRAMFILES'], 'Graphviz 2.44.1', 'bin')\n",
    "    os.environ[\"PATH\"] += os.pathsep + gviz\n",
    "    cnd_gv = os.path.join(os.environ['CONDA_PREFIX'], 'Library', 'bin', 'python-graphviz') #'graphviz')\n",
    "    os.environ[\"PATH\"] += os.pathsep + cnd_gv\n",
    "    return gviz, cnd_gv\n",
    "\n",
    "set_gv_envir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test:\n",
    "gvfile = DIR_IMG.joinpath('tbl.gv')\n",
    "\n",
    "dot_dg = Digraph(comment='The Round Table', filename=gvfile, engine='dot')\n",
    "\n",
    "dot_dg.node('A', 'King Arthur')\n",
    "dot_dg.node('B', 'Sir Bedevere the Wise')\n",
    "dot_dg.node('L', 'Sir Lancelot the Brave')\n",
    "\n",
    "dot_dg.edges(['AB', 'AL'])\n",
    "dot_dg.edge('B', 'L', constraint='false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_dg.render(format='png', view=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".\n",
    "```\n",
    "# in utils_docs.py:\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_node(\"ROOT\")\n",
    "\n",
    "for i in range(5):\n",
    "    G.add_node(\"Child_%i\" % i)\n",
    "    G.add_node(\"Grandchild_%i\" % i)\n",
    "    G.add_node(\"Greatgrandchild_%i\" % i)\n",
    "\n",
    "    G.add_edge(\"ROOT\", \"Child_%i\" % i)\n",
    "    G.add_edge(\"Child_%i\" % i, \"Grandchild_%i\" % i)\n",
    "    G.add_edge(\"Grandchild_%i\" % i, \"Greatgrandchild_%i\" % i)\n",
    "\n",
    "# write dot file to use with graphviz\n",
    "# run \"dot -Tpng test.dot >test.png\"\n",
    "nx.nx_agraph.write_dot(G,'test.dot')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# same layout using matplotlib with no labels\n",
    "plt.title('draw_networkx')\n",
    "pos=graphviz_layout(G, prog='dot')\n",
    "nx.draw(G, pos, with_labels=False, arrows=False)\n",
    "plt.savefig('nx_test.png')\n",
    "#..............\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "g=nx.DiGraph()\n",
    "g.add_edges_from([(1,2), (1,3), (1,4), (2,5), (2,6), (2,7), (3,8), (3,9),\n",
    "                  (4,10), (5,11), (5,12), (6,13)])\n",
    "p=nx.drawing.nx_pydot.to_pydot(g)\n",
    "p.write_png('example.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = {'User Type:':['Admin', 'Tanscriber'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test:\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_node(\"ROOT\")\n",
    "\n",
    "for i in range(5):\n",
    "    G.add_node(\"Child_%i\" % i)\n",
    "    G.add_node(\"Grandchild_%i\" % i)\n",
    "    G.add_node(\"Greatgrandchild_%i\" % i)\n",
    "\n",
    "    G.add_edge(\"ROOT\", \"Child_%i\" % i)\n",
    "    G.add_edge(\"Child_%i\" % i, \"Grandchild_%i\" % i)\n",
    "    G.add_edge(\"Grandchild_%i\" % i, \"Greatgrandchild_%i\" % i)\n",
    "\n",
    "pos = nx.drawing.nx_pydot.graphviz_layout(G)\n",
    "nx.draw(G, pos=pos)\n",
    "\n",
    "# write dot file to use with graphviz\n",
    "# run \"dot -Tpng test.dot >test.png\"\n",
    "\n",
    "dotfile = DIR_DATA.joinpath('test.dot')\n",
    "nx.nx_pydot.write_dot(G, dotfile)\n",
    "\n",
    "p = nx.drawing.nx_pydot.to_pydot(G)\n",
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
