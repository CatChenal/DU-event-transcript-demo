<?xml version="1.0" encoding="utf-8" ?><transcript><text start="0" dur="2.03">Oh</text><text start="3.73" dur="2.06">you</text><text start="16.9" dur="2.06">you</text><text start="21.21" dur="2.059">you</text><text start="30.28" dur="2.06">you</text><text start="35.3" dur="6.31">if you have a</text><text start="37.05" dur="6.87">you can use the QA function so if you&amp;#39;re</text><text start="41.61" dur="4.35">on zoom and you hover over the video it</text><text start="43.92" dur="3.389">should be on the bottom below the video</text><text start="45.96" dur="4.529">it looks like QA and you can ask</text><text start="47.309" dur="5.82">questions there and then we&amp;#39;ll spend</text><text start="50.489" dur="4.051">some time to answer them we do a break</text><text start="53.129" dur="4.28">in the middle of the talk and then at</text><text start="54.54" dur="2.869">the end of the talk as well</text><text start="60.93" dur="2.06">you</text><text start="64.9" dur="5.7">hey Matt I saw you just logged in are</text><text start="68.99" dur="4.74">you good to go</text><text start="70.6" dur="5.379">yeah good to go whenever you are cool</text><text start="73.73" dur="3.66">yeah I just I&amp;#39;ll just do a quick intro</text><text start="75.979" dur="3.421">talk about the meetup group talk about</text><text start="77.39" dur="4.83">Gao take like two minutes I&amp;#39;ll start at</text><text start="79.4" dur="5.54">6:05 and then you can hop into your</text><text start="82.22" dur="8.34">presentation sounds good</text><text start="84.94" dur="9.72">how&amp;#39;s the new gig it&amp;#39;s going pretty well</text><text start="90.56" dur="6.449">week two yeah no no complaints yet it&amp;#39;s</text><text start="94.66" dur="4.959">been dropped a little bit into the deep</text><text start="97.009" dur="5.97">end but that&amp;#39;s okay what I was sort of</text><text start="99.619" dur="8.851">looking for yeah is it what&amp;#39;s it like on</text><text start="102.979" dur="7.801">boarding during a pandemic so in my</text><text start="108.47" dur="4.17">current role worst of it it&amp;#39;s</text><text start="110.78" dur="3.96">interesting secondly there wasn&amp;#39;t a ton</text><text start="112.64" dur="3.75">of onboarding there&amp;#39;s a there&amp;#39;s a bunch</text><text start="114.74" dur="4.17">of stuff that I have to do kind of all</text><text start="116.39" dur="4.89">on my own time you know compliance and</text><text start="118.91" dur="5.55">all of that but there hasn&amp;#39;t been a ton</text><text start="121.28" dur="5.67">of onboarding so I don&amp;#39;t know if that&amp;#39;s</text><text start="124.46" dur="4.71">just the nature of me joining in the</text><text start="126.95" dur="5.43">middle of this project or something else</text><text start="129.17" dur="5.099">but yeah I know I know about six</text><text start="132.38" dur="6.3">different people and that&amp;#39;s about it</text><text start="134.269" dur="7.411">okay yeah that&amp;#39;s interesting it&amp;#39;s it&amp;#39;s</text><text start="138.68" dur="6.33">weird because in a sense like remote</text><text start="141.68" dur="5.369">work or this environment we&amp;#39;re in you</text><text start="145.01" dur="3.75">cut a lot of the flow cut a lot of</text><text start="147.049" dur="3.69">things and I guess some people might</text><text start="148.76" dur="4.14">think they&amp;#39;re fluff or not but right</text><text start="150.739" dur="5.431">like a lot of the stuff you do the first</text><text start="152.9" dur="5.76">day at work or the first week it&amp;#39;s like</text><text start="156.17" dur="4.74">meeting people you know all sorts of</text><text start="158.66" dur="3.84">meetings and things like that that may</text><text start="160.91" dur="3.329">or may not have to do with actually the</text><text start="162.5" dur="6.66">lauric you have to do but they&amp;#39;re just</text><text start="164.239" dur="5.941">like work culture events right and so it</text><text start="169.16" dur="2.43">looks like some companies are just</text><text start="170.18" dur="2.3">cutting it out and you know it&amp;#39;s just</text><text start="171.59" dur="4.709">like whatever</text><text start="172.48" dur="5.17">yeah it&amp;#39;s it&amp;#39;ll be interesting when we</text><text start="176.299" dur="3.961">have to start going back into the</text><text start="177.65" dur="4.8">officer rather if we we have to do that</text><text start="180.26" dur="5.91">what things will look like then if there</text><text start="182.45" dur="6.93">will be a like a retro onboarding or</text><text start="186.17" dur="6.03">anything like that yeah well a lot of</text><text start="189.38" dur="4.32">uncertainty there is FINRA like I don&amp;#39;t</text><text start="192.2" dur="3.03">know like the culture at FINRA is it</text><text start="193.7" dur="2.25">like very much like when the government</text><text start="195.23" dur="2.759">opens they will</text><text start="195.95" dur="3.77">open up because I know in New York City</text><text start="197.989" dur="4.021">you know like officers opened up and</text><text start="199.72" dur="4.659">obviously companies like GA and other</text><text start="202.01" dur="4.08">tech companies are like let&amp;#39;s let&amp;#39;s hold</text><text start="204.379" dur="3.301">back but I have some friends that work</text><text start="206.09" dur="3.83">in investment banks they have to like</text><text start="207.68" dur="5.01">show up to work next week like full time</text><text start="209.92" dur="4.84">we we haven&amp;#39;t gotten anything about that</text><text start="212.69" dur="3.39">yet I think we&amp;#39;re following the to my</text><text start="214.76" dur="4.949">knowledge and what I&amp;#39;ve heard we&amp;#39;re</text><text start="216.08" dur="9.15">following the like the schools in DC so</text><text start="219.709" dur="6.961">I didn&amp;#39;t schedule we&amp;#39;ll see but haven&amp;#39;t</text><text start="225.23" dur="3.81">heard anything yet so I I would be</text><text start="226.67" dur="5.97">surprised if we go back before like</text><text start="229.04" dur="12.15">August 15 just based on where we are now</text><text start="232.64" dur="12">but who knows okay well we have like we</text><text start="241.19" dur="7.549">have yeah we still gonna come people</text><text start="244.64" dur="4.099">tricking in but I&amp;#39;ll just start my intro</text><text start="255.549" dur="6.19">this can everyone can you see my whole</text><text start="258.68" dur="4.739">screen now yep okay sometimes I forget</text><text start="261.739" dur="4.591">if you go fullscreen and Google Chrome</text><text start="263.419" dur="5.851">so thanks everyone for attending during</text><text start="266.33" dur="4.86">these uncertain times I hope you&amp;#39;ll get</text><text start="269.27" dur="4.769">as much out of this talk as I&amp;#39;m planning</text><text start="271.19" dur="5.07">to so the talk is called good fast cheap</text><text start="274.039" dur="6.091">how to do data science with missing data</text><text start="276.26" dur="5.85">and Matt brems will actually be leading</text><text start="280.13" dur="5.099">the talk I&amp;#39;m just here to do an intro</text><text start="282.11" dur="5.94">and moderate and login to my zoom</text><text start="285.229" dur="4.171">account so just a quick agenda we&amp;#39;re</text><text start="288.05" dur="3.57">gonna do an intro I&amp;#39;m just gonna</text><text start="289.4" dur="3.9">introduce Matt I&amp;#39;m gonna introduce kind</text><text start="291.62" dur="4.56">of our sponsors and the meetup group and</text><text start="293.3" dur="4.44">then we will open up for Q&amp;amp;A as I</text><text start="296.18" dur="3.87">mentioned before zoom has like a</text><text start="297.74" dur="4.83">built-in Q&amp;amp;A function so if you hover</text><text start="300.05" dur="4.08">over this video with your mouse on the</text><text start="302.57" dur="3.93">bottom below the video you&amp;#39;ll see a</text><text start="304.13" dur="6.21">things called Q&amp;amp;A so you can ask</text><text start="306.5" dur="5.76">questions directly there this will be</text><text start="310.34" dur="4.56">recorded it will send it&amp;#39;ll be sent out</text><text start="312.26" dur="4.62">and in a couple days so so call it two</text><text start="314.9" dur="5.519">or three days automatically by the GA</text><text start="316.88" dur="4.98">team if you do not get the recording for</text><text start="320.419" dur="5.301">some reason it could be in your spam box</text><text start="321.86" dur="5.88">but otherwise just email your local GA</text><text start="325.72" dur="3.729">kind of</text><text start="327.74" dur="4.59">campus and and they&amp;#39;ll be able to sort</text><text start="329.449" dur="5.911">it out or they&amp;#39;ll just email me and then</text><text start="332.33" dur="6.69">I&amp;#39;ll email them they&amp;#39;re recording so</text><text start="335.36" dur="6.989">Matt brems is the speaker he actually</text><text start="339.02" dur="6.209">just moved to FINRA maybe like two weeks</text><text start="342.349" dur="4.921">ago but before that he was at the global</text><text start="345.229" dur="3.991">instructor for GA Zeta Science immersive</text><text start="347.27" dur="4.709">program across the United States and</text><text start="349.22" dur="4.71">before that he was working in the</text><text start="351.979" dur="4.771">political consulting realm and if you</text><text start="353.93" dur="6.51">want to follow him on Twitter his symbol</text><text start="356.75" dur="6.389">is or his tag his handle is Matthew</text><text start="360.44" dur="4.229">brems I wasn&amp;#39;t able to figure all by oh</text><text start="363.139" dur="7.68">it was you know it&amp;#39;s too long for the</text><text start="364.669" dur="8.101">slide and then the event is run by data</text><text start="370.819" dur="3.541">umbrella so the mission of data</text><text start="372.77" dur="3.6">umbrellas provide a welcoming and</text><text start="374.36" dur="4.5">educational space for you our G&amp;#39;s or</text><text start="376.37" dur="4.829">underrepresented groups in the fields of</text><text start="378.86" dur="5.07">data science and machine learning you</text><text start="381.199" dur="5.4">can learn about upcoming events and the</text><text start="383.93" dur="7.01">mission at data umbrella org and you can</text><text start="386.599" dur="7.261">follow them at data umbrella on Twitter</text><text start="390.94" dur="4.81">data umbrella is running weekly events</text><text start="393.86" dur="5.609">during this time like weekly virtual</text><text start="395.75" dur="5.4">events so if you check the page there&amp;#39;ll</text><text start="399.469" dur="4.711">be another really interesting data</text><text start="401.15" dur="7.109">science event within the next seven to</text><text start="404.18" dur="6.299">ten days call it pi ladies is also a</text><text start="408.259" dur="4.291">co-sponsor it&amp;#39;s the New York City</text><text start="410.479" dur="4.521">chapter of a large national group so</text><text start="412.55" dur="5.13">it&amp;#39;s a group for python ladies and nine</text><text start="415" dur="5.74">non-binary people of all levels of</text><text start="417.68" dur="5.039">programming experience and you can check</text><text start="420.74" dur="4.079">the home page of the national</text><text start="422.719" dur="4.111">organization pi ladies calm and then</text><text start="424.819" dur="5.461">their Twitter handle is NYC PI ladies</text><text start="426.83" dur="5.519">and then last but not least the company</text><text start="430.28" dur="4.55">where I work at and where I met Matt</text><text start="432.349" dur="5.37">brims and is letting us borrow their</text><text start="434.83" dur="5.53">webinar zoom account is General Assembly</text><text start="437.719" dur="4.2">so General Assembly is a pioneer in the</text><text start="440.36" dur="3.63">education and career transformation</text><text start="441.919" dur="4.831">space specializing in today&amp;#39;s most</text><text start="443.99" dur="7.109">in-demand skills so the homepage is</text><text start="446.75" dur="7.68">General Assembly or General Assembly Y</text><text start="451.099" dur="6.481">and then the Twitter handle is at GA we</text><text start="454.43" dur="6.39">run a lot of cool classes and tech data</text><text start="457.58" dur="4.02">marketing product management if you&amp;#39;re</text><text start="460.82" dur="2.46">interested in learning</text><text start="461.6" dur="5.16">the classes you can also ask questions</text><text start="463.28" dur="6.06">to me I can I can help sort that out but</text><text start="466.76" dur="7.44">without further ado we will move on to</text><text start="469.34" dur="8.37">Matt&amp;#39;s talk awesome just stop staring my</text><text start="474.2" dur="7.29">screen and then you can yours yeah I&amp;#39;ll</text><text start="477.71" dur="5.7">go ahead and start sharing my screen as</text><text start="481.49" dur="4.17">well good evening everybody</text><text start="483.41" dur="4.98">like liked I shared my name is Matt</text><text start="485.66" dur="3.87">friends I use he/him pronouns I&amp;#39;ll go</text><text start="488.39" dur="2.91">ahead and share my screen here so</text><text start="489.53" dur="5.01">hopefully you&amp;#39;ll be able to see it I</text><text start="491.3" dur="6.87">dropped in the I dropped it in the chat</text><text start="494.54" dur="6.27">box a link to github where you can find</text><text start="498.17" dur="4.65">my content so if you would like to</text><text start="500.81" dur="5.01">follow along you&amp;#39;re certainly welcome to</text><text start="502.82" dur="4.5">I&amp;#39;ve got I&amp;#39;ll be focusing exclusively on</text><text start="505.82" dur="5.27">slides here</text><text start="507.32" dur="6.3">however in in the repository there are</text><text start="511.09" dur="5.949">in the repository there&amp;#39;s code and it</text><text start="513.62" dur="6.06">will clear breaks in the code show when</text><text start="517.039" dur="5.071">you can that show kind of when we&amp;#39;ll be</text><text start="519.68" dur="3.69">shifting from one slide to the next and</text><text start="522.11" dur="4.26">where the code sort of fills in the</text><text start="523.37" dur="6.45">blank I&amp;#39;m seeing that this is not</text><text start="526.37" dur="7.35">sharing so let me actually tie really</text><text start="529.82" dur="5.49">quickly are you able to see my slides so</text><text start="533.72" dur="4.44">no it just says Matt brands has started</text><text start="535.31" dur="6.21">screen sharing okay well let me go ahead</text><text start="538.16" dur="6.69">and restart sharing that I appreciate</text><text start="541.52" dur="6.63">folks as patience here with this this is</text><text start="544.85" dur="5.76">my first time setting up this my first</text><text start="548.15" dur="4.49">time using this iPad I switched iPad</text><text start="550.61" dur="6.98">since they no longer I&amp;#39;m using my GA one</text><text start="552.64" dur="4.95">so we will let&amp;#39;s see</text><text start="558.25" dur="4.66">yeah if everyone&amp;#39;s wondering the slides</text><text start="560.99" dur="3.93">from this presentation are in that link</text><text start="562.91" dur="4.919">that Matt shared in the chat and that</text><text start="564.92" dur="4.77">could help others PDF right so this is</text><text start="567.829" dur="3.601">not working so what I&amp;#39;m gonna do I&amp;#39;ve</text><text start="569.69" dur="3.329">got the slides on Google slides so I&amp;#39;ll</text><text start="571.43" dur="4.44">just go ahead and use the Google slides</text><text start="573.019" dur="5.55">version and edit through that instead of</text><text start="575.87" dur="7.139">instead of working on my iPad so I will</text><text start="578.569" dur="7.471">not get to use my my stylus that that is</text><text start="583.009" dur="5.611">that is okay so let me stop sharing that</text><text start="586.04" dur="4.77">and I will go ahead and reshare my</text><text start="588.62" dur="4.17">screen here again appreciate folks is</text><text start="590.81" dur="6.12">patience with this Oh were you trying to</text><text start="592.79" dur="6.359">cast your iPad into the zoom yeah I&amp;#39;ve</text><text start="596.93" dur="6.54">got a okay yeah I&amp;#39;ve got it</text><text start="599.149" dur="6.601">nice desktop thank you uh you would be</text><text start="603.47" dur="3.57">able to see my screen now my Google</text><text start="605.75" dur="4.8">slides have not pulled up can you see</text><text start="607.04" dur="7.26">him yep we can say it now okay I saw a</text><text start="610.55" dur="5.58">face light up so imagine that yeah that</text><text start="614.3" dur="3.599">that&amp;#39;s coming through now so I won&amp;#39;t be</text><text start="616.13" dur="4.769">able to annotate with my with my Apple</text><text start="617.899" dur="4.38">pencil but but that&amp;#39;s okay so thank you</text><text start="620.899" dur="3.87">very much time thank you very much</text><text start="622.279" dur="4.141">everybody for for having me join like I</text><text start="624.769" dur="4.05">mentioned my name is Matt friends I use</text><text start="626.42" dur="4.83">he in pronouns and what I&amp;#39;m here to talk</text><text start="628.819" dur="4.231">with you about is how to do data science</text><text start="631.25" dur="3.63">with missing data this is a really</text><text start="633.05" dur="5.43">challenging problem to work within to</text><text start="634.88" dur="5.73">try and grapple with but I think it&amp;#39;s an</text><text start="638.48" dur="4.709">important topic because if you do data</text><text start="640.61" dur="4.979">science you have inevitably run into</text><text start="643.189" dur="4.441">challenges with missing data or missing</text><text start="645.589" dur="4.17">information and the ways that we try and</text><text start="647.63" dur="5.189">handle that probably are not the ways</text><text start="649.759" dur="4.741">that we should be handling that so I&amp;#39;ve</text><text start="652.819" dur="3.481">got some stuff here you can see this in</text><text start="654.5" dur="4.05">the slides ty has already talked about</text><text start="656.3" dur="5.01">my background so I&amp;#39;ll go ahead and skip</text><text start="658.55" dur="4.289">beyond this you&amp;#39;re welcome to to hit me</text><text start="661.31" dur="5.85">up afterward if you&amp;#39;d like to talk about</text><text start="662.839" dur="5.761">any of these experiences but the lay of</text><text start="667.16" dur="2.97">the land for tonight so we&amp;#39;re gonna</text><text start="668.6" dur="4.02">start by talking about missing data</text><text start="670.13" dur="5.16">we&amp;#39;ll get into strategies for doing data</text><text start="672.62" dur="4.56">science with missing data and provided</text><text start="675.29" dur="3.84">that we&amp;#39;ve got time we&amp;#39;re gonna wrap up</text><text start="677.18" dur="4.709">with some practical considerations and</text><text start="679.13" dur="4.829">warnings for you to keep in mind as this</text><text start="681.889" dur="5.19">goes on please feel free to at any</text><text start="683.959" dur="5.401">moment drop notes in the slack or in the</text><text start="687.079" dur="4.261">zoom channel so I will keep an eye on</text><text start="689.36" dur="4.5">that as well as the Q&amp;amp;A so at</text><text start="691.34" dur="4.32">any point feel free to do that to drop</text><text start="693.86" dur="4.33">stuff in there and I&amp;#39;ll try and respond</text><text start="695.66" dur="5.929">kind of in real time</text><text start="698.19" dur="5.29">so how big of a problem is missing data</text><text start="701.589" dur="3.81">and this is just gonna be the the very</text><text start="703.48" dur="3.84">quick start of it it&amp;#39;s a really</text><text start="705.399" dur="4.021">challenging question for us to answer</text><text start="707.32" dur="4.11">because what&amp;#39;s going to happen when we</text><text start="709.42" dur="3.93">are trying to work with missing data we</text><text start="711.43" dur="3.99">try and quantify how big of a problem it</text><text start="713.35" dur="3.929">is is that from a practical point of</text><text start="715.42" dur="5.849">view and it sounds trivial to say this</text><text start="717.279" dur="5.851">we can only see what we observe so we&amp;#39;re</text><text start="721.269" dur="3.901">only going to be able to actually see</text><text start="723.13" dur="4.32">the data that we&amp;#39;ve gathered we don&amp;#39;t</text><text start="725.17" dur="4.89">know the value of that missing thing</text><text start="727.45" dur="5.61">itself so the only way for us to be able</text><text start="730.06" dur="5.19">to quantify or understand the magnitude</text><text start="733.06" dur="4.709">of how big of a problem missing data is</text><text start="735.25" dur="4.62">is there we can use simulated data to</text><text start="737.769" dur="4.831">try and help and answer that question</text><text start="739.87" dur="4.649">now in the interest of time we&amp;#39;re not</text><text start="742.6" dur="3.87">going to go through actually generating</text><text start="744.519" dur="4.771">the simulated data and looking at this</text><text start="746.47" dur="4.95">but if I move forward to this slide if</text><text start="749.29" dur="4.979">you would like to check this out all of</text><text start="751.42" dur="6.03">the code is pre-written for you in the</text><text start="754.269" dur="4.651">notebook and in the so in the repository</text><text start="757.45" dur="2.43">I&amp;#39;ve got three different sets of</text><text start="758.92" dur="5.13">notebooks</text><text start="759.88" dur="8.22">there&amp;#39;s one with a prefix 0 0 1 with a</text><text start="764.05" dur="6.06">prefix 0 1 and one with a prefix 0 2 so</text><text start="768.1" dur="4.049">you can run that on your own if you</text><text start="770.11" dur="4.65">would like but in short what you would</text><text start="772.149" dur="4.591">see at that notebook is we create some</text><text start="774.76" dur="4.62">data or we generate a complete data set</text><text start="776.74" dur="5.13">and then we take 20% of those</text><text start="779.38" dur="4.709">observations and turn them off or we set</text><text start="781.87" dur="3.69">those to be missing and we see how much</text><text start="784.089" dur="4.411">of an impact that would have on our</text><text start="785.56" dur="4.589">model if and what we notice is that if</text><text start="788.5" dur="3.839">we were to look at the slope and the</text><text start="790.149" dur="3.81">y-intercept of our simple linear</text><text start="792.339" dur="4.471">regression model there&amp;#39;s actually a</text><text start="793.959" dur="4.74">really really really large effect and so</text><text start="796.81" dur="4.589">that is a quick way for us to try and</text><text start="798.699" dur="5.07">quantify how bad or how big of a problem</text><text start="801.399" dur="4.741">missing data is now that depends on a</text><text start="803.769" dur="4.591">whole host of factors how much data do</text><text start="806.14" dur="5.069">you have what is the type of missing</text><text start="808.36" dur="5.25">data that you&amp;#39;re dealing with how what</text><text start="811.209" dur="4.801">type of model are you trying to fit how</text><text start="813.61" dur="4.71">many variables do you have all sorts of</text><text start="816.01" dur="5.43">things factored into that but in a very</text><text start="818.32" dur="5.37">very simple case we can see that missing</text><text start="821.44" dur="4.23">data is really going to undermine a lot</text><text start="823.69" dur="4.76">of our inferences and the conclusions</text><text start="825.67" dur="4.91">that we may try to make</text><text start="828.45" dur="3.87">so this brings me to what I want to get</text><text start="830.58" dur="4.38">into which is what is a realistic</text><text start="832.32" dur="5.85">approach for us so if you&amp;#39;re familiar</text><text start="834.96" dur="6.03">with the good fast cheap idea in project</text><text start="838.17" dur="5.25">management what that means is that you</text><text start="840.99" dur="5.07">can come up with a project that is good</text><text start="843.42" dur="4.8">and fast and if you come up with a</text><text start="846.06" dur="3.87">project that is good and fast what&amp;#39;s</text><text start="848.22" dur="4.2">going to end up happening is it&amp;#39;s not</text><text start="849.93" dur="4.68">gonna be cheap that is it will be more</text><text start="852.42" dur="3.84">expensive to be able to do that project</text><text start="854.61" dur="3.42">because if somebody wants something done</text><text start="856.26" dur="3.54">quickly and wants something done well</text><text start="858.03" dur="4.95">people will probably have to pay top</text><text start="859.8" dur="6.15">dollar for that on the other hand you</text><text start="862.98" dur="4.92">can think about a good and cheap project</text><text start="865.95" dur="3.93">so sometimes people will say hey I need</text><text start="867.9" dur="3.66">a project that is high quality and they</text><text start="869.88" dur="3.78">don&amp;#39;t want to pay a ton of money for it</text><text start="871.56" dur="3.75">and that&amp;#39;s a very realistic scenario to</text><text start="873.66" dur="4.11">come up but the challenge is if</text><text start="875.31" dur="4.41">somebody&amp;#39;s not willing to invest in it</text><text start="877.77" dur="4.11">and you want something that&amp;#39;s high</text><text start="879.72" dur="4.83">quality generally that will take a large</text><text start="881.88" dur="4.77">amount of time in order to deliver that</text><text start="884.55" dur="4.2">solution so here we see the overlap</text><text start="886.65" dur="4.83">between good and cheap is that it will</text><text start="888.75" dur="5.13">take time to deliver and then finally at</text><text start="891.48" dur="4.77">the bottom here we see fast and cheap</text><text start="893.88" dur="4.65">most frequently in my personal</text><text start="896.25" dur="4.2">experience people will say hey I want to</text><text start="898.53" dur="4.86">pay this low dollar amount for a project</text><text start="900.45" dur="4.83">and I need it done tomorrow well what&amp;#39;s</text><text start="903.39" dur="4.32">gonna happen is is that if something is</text><text start="905.28" dur="4.53">done fast and that something is done on</text><text start="907.71" dur="4.74">the cheap it&amp;#39;s not going to be the best</text><text start="909.81" dur="5.64">quality in most cases so because of that</text><text start="912.45" dur="6.09">we need to think about how can we take</text><text start="915.45" dur="5.25">this approach and apply it to - missing</text><text start="918.54" dur="5.1">data the reason that I bring this up is</text><text start="920.7" dur="5.61">that oftentimes what clients or managers</text><text start="923.64" dur="5.16">or anybody else once they will say I</text><text start="926.31" dur="4.47">want something that is good and fast and</text><text start="928.8" dur="4.5">cheap but that&amp;#39;s not going to be</text><text start="930.78" dur="5.07">feasible connecting this directly with</text><text start="933.3" dur="4.53">missing data we think about what missing</text><text start="935.85" dur="4.98">data means in terms of a fast and cheap</text><text start="937.83" dur="5.01">analysis that&amp;#39;s going to be you just</text><text start="940.83" dur="4.54">drop all of your missing values or you</text><text start="942.84" dur="4.66">do a single imputation</text><text start="945.37" dur="5.55">if you want an analysis that is done</text><text start="947.5" dur="5.61">well and is fairly inexpensive then you</text><text start="950.92" dur="3.9">have to fill in your missing values or</text><text start="953.11" dur="3.69">handle missing data in what I would call</text><text start="954.82" dur="5.01">the proper way so we can talk about</text><text start="956.8" dur="5.64">proper imputation or the pattern sub</text><text start="959.83" dur="4.05">model approach and then finally and</text><text start="962.44" dur="4.35">we&amp;#39;ll get into what those means shortly</text><text start="963.88" dur="5.25">and then finally if you want an analysis</text><text start="966.79" dur="4.739">that&amp;#39;s good and your analysis to be very</text><text start="969.13" dur="4.41">quick then you should gather your data</text><text start="971.529" dur="3.991">in a complete manner the downside of</text><text start="973.54" dur="4.049">that of course is it&amp;#39;s incredibly</text><text start="975.52" dur="4.259">expensive to do that you have to figure</text><text start="977.589" dur="4.921">out how can you collect complete data</text><text start="979.779" dur="4.381">without missing any data and oftentimes</text><text start="982.51" dur="3.24">you have to pay top dollar for that</text><text start="984.16" dur="3.51">something that might be a hard sell for</text><text start="985.75" dur="5.07">when you&amp;#39;re talking with your boss or</text><text start="987.67" dur="5.91">your client or somebody else so I&amp;#39;m not</text><text start="990.82" dur="5.43">I what you should come away from this</text><text start="993.58" dur="4.53">evening is not this is the specific way</text><text start="996.25" dur="4.47">I need to always handle my missing data</text><text start="998.11" dur="4.56">but get a better understanding of what</text><text start="1000.72" dur="4.5">are the trade-offs and missing data what</text><text start="1002.67" dur="4.169">are the different challenges in are the</text><text start="1005.22" dur="6.059">trade-offs if I go with one approach</text><text start="1006.839" dur="6.331">versus another approach so given that</text><text start="1011.279" dur="3.991">introduction what I&amp;#39;d like to do is talk</text><text start="1013.17" dur="6.12">about what are strategies for doing data</text><text start="1015.27" dur="5.91">science with missing data so the first</text><text start="1019.29" dur="5.669">thing to do is let&amp;#39;s talk about how to</text><text start="1021.18" dur="5.399">avoid missing data so something that I</text><text start="1024.959" dur="3.24">think is really important to note is</text><text start="1026.579" dur="4.23">that it&amp;#39;s usually going to be more</text><text start="1028.199" dur="5.971">expensive up front but cheaper in the</text><text start="1030.809" dur="5.821">long run to avoid missing data so as you</text><text start="1034.17" dur="7.039">think and this is probably the this is</text><text start="1036.63" dur="6.569">not the best way of going about best way</text><text start="1041.209" dur="7.6">trying to come up with the right phrase</text><text start="1043.199" dur="8.101">for it this is not the this is not the</text><text start="1048.809" dur="4.351">fanciest or coolest approach to missing</text><text start="1051.3" dur="3.9">data for me to spend time to talk about</text><text start="1053.16" dur="3.84">avoiding missing data you&amp;#39;re like yeah</text><text start="1055.2" dur="4.53">like okay I know that it&amp;#39;s better for us</text><text start="1057" dur="5.429">to collect data as opposed to collect</text><text start="1059.73" dur="3.78">data that is missing and I get that but</text><text start="1062.429" dur="3.151">it&amp;#39;s important to talk about this</text><text start="1063.51" dur="3.63">because oftentimes in the long run it&amp;#39;s</text><text start="1065.58" dur="3.14">going to be a better thing for you to</text><text start="1067.14" dur="3.99">avoid that missing data upfront</text><text start="1068.72" dur="4.42">depending on the jobs you have were the</text><text start="1071.13" dur="3.45">jobs you want to have if you&amp;#39;re working</text><text start="1073.14" dur="3.63">in an organization where you&amp;#39;re</text><text start="1074.58" dur="4.26">gathering survey data or you&amp;#39;re working</text><text start="1076.77" dur="4.56">to collect data in some capacity</text><text start="1078.84" dur="5.46">if you have any control over that there</text><text start="1081.33" dur="4.83">may be small to moderate design changes</text><text start="1084.3" dur="3.78">that can be implemented there that</text><text start="1086.16" dur="4.35">allowed to gather significantly more</text><text start="1088.08" dur="4.71">data if you gather more data you don&amp;#39;t</text><text start="1090.51" dur="4.32">have to invest time in how to handle</text><text start="1092.79" dur="4.14">that missing data later you can just use</text><text start="1094.83" dur="4.29">the entirety of your data if you&amp;#39;ve got</text><text start="1096.93" dur="3.84">more data your inferences and your</text><text start="1099.12" dur="4.62">predictions and everything tend to be</text><text start="1100.77" dur="5.19">more precise your variance is lower and</text><text start="1103.74" dur="4.08">so because of all of this it&amp;#39;s often</text><text start="1105.96" dur="5.04">better for us to try and avoid missing</text><text start="1107.82" dur="5.16">data upfront if we can so I want to take</text><text start="1111" dur="3.54">a moment to talk very briefly about some</text><text start="1112.98" dur="4.02">of these again you can read all of these</text><text start="1114.54" dur="4.02">on your screen but some of the things</text><text start="1117" dur="3.87">that I think about our for example</text><text start="1118.56" dur="4.41">decreasing the burden on your respondent</text><text start="1120.87" dur="4.14">or minimizing the number of questions</text><text start="1122.97" dur="3.9">somebody has to respond I responded to</text><text start="1125.01" dur="3.78">two surveys earlier today I&amp;#39;m Survey</text><text start="1126.87" dur="4.32">Monkey a former colleague had posted</text><text start="1128.79" dur="4.29">some stuff on LinkedIn and I filled</text><text start="1131.19" dur="3.54">those out and they were I was willing to</text><text start="1133.08" dur="4.62">do it even on my phone because they were</text><text start="1134.73" dur="4.86">relatively short surveys I like the</text><text start="1137.7" dur="5.52">beach Chipotle app I eat from Chipotle</text><text start="1139.59" dur="5.7">quite frequently and up until actually</text><text start="1143.22" dur="3.87">like last week what they would do is if</text><text start="1145.29" dur="4.11">you ordered online through the app they</text><text start="1147.09" dur="5.1">would follow up about an hour later with</text><text start="1149.4" dur="4.65">a green smiley face or a red frowny face</text><text start="1152.19" dur="2.31">and say hey how was your dinner this</text><text start="1154.05" dur="2.25">evening</text><text start="1154.5" dur="3.6">and you could click that smiley face or</text><text start="1156.3" dur="3.72">the frowning face and if you click the</text><text start="1158.1" dur="3.93">smiley face they said hey thank you so</text><text start="1160.02" dur="4.41">much if you click the frowny face you</text><text start="1162.03" dur="5.04">got to put a couple of checkboxes and</text><text start="1164.43" dur="4.35">say this is what I this is what I didn&amp;#39;t</text><text start="1167.07" dur="4.44">like or this was this was not</text><text start="1168.78" dur="4.47">satisfactory and that was it so many</text><text start="1171.51" dur="5.01">other organizations there were so many</text><text start="1173.25" dur="6.18">other data collection mechanisms end up</text><text start="1176.52" dur="5.46">requiring you to fill out 20 30 40 50</text><text start="1179.43" dur="5.16">questions and so what ends up happening</text><text start="1181.98" dur="4.95">is that you often will create missing</text><text start="1184.59" dur="5.25">data by the design of what you&amp;#39;re</text><text start="1186.93" dur="3.45">looking at is opposed to any anything</text><text start="1189.84" dur="2.61">else</text><text start="1190.38" dur="4.08">so making some changes on how you can</text><text start="1192.45" dur="5.07">decrease the burden on your respondent</text><text start="1194.46" dur="5.31">maybe making questions closed-ended</text><text start="1197.52" dur="3.96">instead of open-ended like like a fill</text><text start="1199.77" dur="3.75">in the blank question</text><text start="1201.48" dur="4.83">one other note that I want to make here</text><text start="1203.52" dur="5.37">is thinking about in groups thinking</text><text start="1206.31" dur="6.24">about improving accessibility that&amp;#39;s a</text><text start="1208.89" dur="5.07">very very important point so there are</text><text start="1212.55" dur="2.96">lots of different ways you can get into</text><text start="1213.96" dur="3.09">this we can think about language</text><text start="1215.51" dur="3.07">accessibility we can think about</text><text start="1217.05" dur="3.21">readability we can think about</text><text start="1218.58" dur="5.16">individuals who may be hard of hearing</text><text start="1220.26" dur="5.01">and ways to gather data from individuals</text><text start="1223.74" dur="3.78">but it&amp;#39;s important to think about</text><text start="1225.27" dur="4.23">accessibility and inclusivity when we</text><text start="1227.52" dur="3.87">design that so if you are part of an</text><text start="1229.5" dur="4.38">organization where you are gathering</text><text start="1231.39" dur="6.09">data in some capacity is there a way to</text><text start="1233.88" dur="5.79">improve that accessibility to others for</text><text start="1237.48" dur="4.8">example when I was doing polling and</text><text start="1239.67" dur="4.5">surveys in the context of politics we</text><text start="1242.28" dur="3.78">would administer surveys in multiple</text><text start="1244.17" dur="4.23">languages specifically English and</text><text start="1246.06" dur="4.35">Spanish when we were calling different</text><text start="1248.4" dur="6.36">populations or specifically different</text><text start="1250.41" dur="6.42">states that had a a particularly large</text><text start="1254.76" dur="3.93">Hispanic population or spanish-speaking</text><text start="1256.83" dur="3.48">population that was something that we</text><text start="1258.69" dur="3.81">wanted to do because otherwise we were</text><text start="1260.31" dur="4.53">just leaving out broad swaths of the</text><text start="1262.5" dur="4.74">population which would of course down</text><text start="1264.84" dur="3.99">the road compromise our inferences so</text><text start="1267.24" dur="4.41">you can make a compelling business case</text><text start="1268.83" dur="5.55">for doing something like this it&amp;#39;s not I</text><text start="1271.65" dur="6">mean accessibility in my opinion is in</text><text start="1274.38" dur="5.22">and of itself a valuable goal in</text><text start="1277.65" dur="3.81">addition to that I think that it&amp;#39;s</text><text start="1279.6" dur="4.02">important to recognize that when</text><text start="1281.46" dur="3.99">attempting to encourage other people or</text><text start="1283.62" dur="3.9">share with other people that they should</text><text start="1285.45" dur="4.22">take some action or invest some funds or</text><text start="1287.52" dur="5.69">some energy in that that there are some</text><text start="1289.67" dur="5.92">positive business effects to it as well</text><text start="1293.21" dur="4.42">moving onto the next slide so we talked</text><text start="1295.59" dur="4.95">about avoiding missing data how do we</text><text start="1297.63" dur="4.95">ignore missing data well the very very</text><text start="1300.54" dur="5.04">short summary is that we&amp;#39;re going to</text><text start="1302.58" dur="5.49">assume that any observation that we&amp;#39;ve</text><text start="1305.58" dur="4.29">observed is similar to those</text><text start="1308.07" dur="4.14">observations for which we are missing</text><text start="1309.87" dur="5.43">data when we ignore we&amp;#39;re making an</text><text start="1312.21" dur="6.33">implicit assumption which may or may not</text><text start="1315.3" dur="5.52">be a valid thing to do when I was in</text><text start="1318.54" dur="6.09">grad school a professor shared that a</text><text start="1320.82" dur="5.61">very general rough guideline is that if</text><text start="1324.63" dur="5.61">you are missing less than five percent</text><text start="1326.43" dur="7.5">of all of your data you may be okay</text><text start="1330.24" dur="5.58">ignoring that data that&amp;#39;s missing now if</text><text start="1333.93" dur="4.17">you are trying to do something like</text><text start="1335.82" dur="3.81">supervised learning you fit a model</text><text start="1338.1" dur="4.41">where you&amp;#39;ve got a bunch of inputs and</text><text start="1339.63" dur="4.86">an output your Y variable if you&amp;#39;re</text><text start="1342.51" dur="2.87">missing a ton of data from your Y</text><text start="1344.49" dur="3.41">variable</text><text start="1345.38" dur="5.19">then even if you&amp;#39;re missing less than 5%</text><text start="1347.9" dur="5.07">of your data overall that may compromise</text><text start="1350.57" dur="4.62">your inferences too much you may not be</text><text start="1352.97" dur="3.87">willing to do that or if there are</text><text start="1355.19" dur="3.48">certain variables that you know were</text><text start="1356.84" dur="3.6">believed to be really meaningful and</text><text start="1358.67" dur="4.5">you&amp;#39;re missing a lot of data from those</text><text start="1360.44" dur="4.92">maybe ignoring missing data isn&amp;#39;t the</text><text start="1363.17" dur="4.83">right way to go now when we ignore</text><text start="1365.36" dur="4.77">missing data effectively it&amp;#39;s what most</text><text start="1368" dur="4.65">software&amp;#39;s are going to do by default in</text><text start="1370.13" dur="5.31">our in Python in something else if you</text><text start="1372.65" dur="4.17">just put your data into your model and</text><text start="1375.44" dur="3.21">press GO</text><text start="1376.82" dur="4.95">or dot fit or whatever it is you choose</text><text start="1378.65" dur="4.77">to do if you were to do that and didn&amp;#39;t</text><text start="1381.77" dur="4.02">handle your missing data in some</text><text start="1383.42" dur="4.35">capacity or in some way then you&amp;#39;re</text><text start="1385.79" dur="4.26">probably going to that&amp;#39;s effectively</text><text start="1387.77" dur="4.77">ignoring it your model or your software</text><text start="1390.05" dur="4.74">is almost certainly going to drop all of</text><text start="1392.54" dur="4.77">your observations that contain one or</text><text start="1394.79" dur="3.84">more missing values in it and that may</text><text start="1397.31" dur="3.21">be okay to do if that number is</text><text start="1398.63" dur="4.74">relatively small but I want to emphasize</text><text start="1400.52" dur="5.22">up here there is an assumption that you</text><text start="1403.37" dur="6.5">are making with that and that may or may</text><text start="1405.74" dur="4.13">not be a valid assumption to make</text><text start="1410.499" dur="4.78">so the last thing that I want to talk</text><text start="1412.549" dur="5.01">about is how to account for missing data</text><text start="1415.279" dur="4.74">I mentioned how to avoid missing data</text><text start="1417.559" dur="5.791">upfront if you can&amp;#39;t avoid it you might</text><text start="1420.019" dur="5.13">say can I ignore it well here before we</text><text start="1423.35" dur="3.51">account for MIT and if we can&amp;#39;t ignore</text><text start="1425.149" dur="3.421">it then we have to account for it but</text><text start="1426.86" dur="4.409">before getting into that I want to shift</text><text start="1428.57" dur="6.209">our mindset a little bit because there</text><text start="1431.269" dur="5.851">is a a belief that we can just plug in</text><text start="1434.779" dur="4.53">those gaps in our data that you know and</text><text start="1437.12" dur="3.96">perhaps you were perhaps someone</text><text start="1439.309" dur="3.48">expected that you would be able to come</text><text start="1441.08" dur="3.479">here tonight and I would give you a new</text><text start="1442.789" dur="4.321">Python package that allows you to fill</text><text start="1444.559" dur="4.291">in missing data and and you&amp;#39;ve got that</text><text start="1447.11" dur="3.029">technique you can put in your work flow</text><text start="1448.85" dur="3.63">and in your wallet and kind of move on</text><text start="1450.139" dur="3.691">your way but the problem with that is</text><text start="1452.48" dur="4.529">that you have to do this in a specific</text><text start="1453.83" dur="6.99">way or we&amp;#39;re really just making up data</text><text start="1457.009" dur="6.15">and making up data has all sorts of</text><text start="1460.82" dur="4.92">issues a we might be wrong be it&amp;#39;s not a</text><text start="1463.159" dur="4.89">an ethical thing to do in my opinion and</text><text start="1465.74" dur="4.35">so because of this we need to be very</text><text start="1468.049" dur="3.96">careful about how we would fill in some</text><text start="1470.09" dur="4.829">of those gaps or how we how we tackle</text><text start="1472.009" dur="4.711">missing data but I like to shift our</text><text start="1474.919" dur="4.23">mindset a little bit and say in most</text><text start="1476.72" dur="4.589">cases we&amp;#39;re not really fixing missing</text><text start="1479.149" dur="4.77">data it&amp;#39;s not like we just have this as</text><text start="1481.309" dur="4.71">a new step in our workflow where I fit</text><text start="1483.919" dur="4.411">some some method in pandas or in</text><text start="1486.019" dur="3.9">scikit-learn and then move on with the</text><text start="1488.33" dur="4.96">rest of my day we&amp;#39;re really just</text><text start="1489.919" dur="6.201">learning how to cope with missing data</text><text start="1493.29" dur="3.88">so given that shift in our mindset that</text><text start="1496.12" dur="3.36">we&amp;#39;re really just learning how to</text><text start="1497.17" dur="4.44">effectively and in a principled way cope</text><text start="1499.48" dur="5.43">with our missing data let&amp;#39;s move beyond</text><text start="1501.61" dur="6.27">this so we want to talk about how to</text><text start="1504.91" dur="5.31">account for missing data and there is</text><text start="1507.88" dur="4.23">code in the repository to go through</text><text start="1510.22" dur="5.43">both unit missingness and item</text><text start="1512.11" dur="5.55">missingness so I want to I want to share</text><text start="1515.65" dur="4.25">that with you and that&amp;#39;s again in the</text><text start="1517.66" dur="4.77">repository if you&amp;#39;d like to take a look</text><text start="1519.9" dur="4.87">one note that I want to make is please</text><text start="1522.43" dur="4.23">again feel free to drop questions in the</text><text start="1524.77" dur="3.36">chat if there are questions that you</text><text start="1526.66" dur="3.39">have because I want to make sure that I</text><text start="1528.13" dur="3.69">can answer them as we go I really want</text><text start="1530.05" dur="7.14">this to be as helpful as possible for</text><text start="1531.82" dur="6.78">for each so let&amp;#39;s talk about unit and</text><text start="1537.19" dur="3.2">item missingness there are a couple of</text><text start="1538.6" dur="4.68">different ways that data can be missing</text><text start="1540.39" dur="5.65">unit missingness is where we&amp;#39;re missing</text><text start="1543.28" dur="6.72">all of our values from one observation</text><text start="1546.04" dur="7.2">so for example here index 3 if I&amp;#39;m</text><text start="1550" dur="5.76">gathering data on individuals and let&amp;#39;s</text><text start="1553.24" dur="4.38">say that person 3 just did not respond</text><text start="1555.76" dur="4.11">to my survey or for whatever reason I</text><text start="1557.62" dur="5.28">have no information from this person if</text><text start="1559.87" dur="5.97">I have n A&amp;#39;s for all of those that would</text><text start="1562.9" dur="4.35">be an example of unit missingness this</text><text start="1565.84" dur="4.1">person did not share their information</text><text start="1567.25" dur="5.67">with me and so I have no information</text><text start="1569.94" dur="5.02">item missingness or I like to refer to</text><text start="1572.92" dur="4.68">it as Swiss cheese missing this is where</text><text start="1574.96" dur="6.42">there are holes in your data so indices</text><text start="1577.6" dur="6.96">1 2 in 10,000 have this for example for</text><text start="1581.38" dur="6.63">index 1 we do not have information on</text><text start="1584.56" dur="7.17">age or income but we do have information</text><text start="1588.01" dur="6.24">on sex here for individual 2 or index 2</text><text start="1591.73" dur="5.22">we do not have sex but we do have access</text><text start="1594.25" dur="5.1">to age and income data and then all the</text><text start="1596.95" dur="6.53">way down to row 10,000 we&amp;#39;re missing one</text><text start="1599.35" dur="4.13">value here in the income</text><text start="1603.52" dur="6.06">so the way that we handle unit and item</text><text start="1606.58" dur="6.96">missingness is a little bit different so</text><text start="1609.58" dur="6.599">in terms of unit missingness the very</text><text start="1613.54" dur="5.91">very quick summary of how to handle unit</text><text start="1616.179" dur="5.461">missingness and i am let me actually go</text><text start="1619.45" dur="6.14">ahead and pull up this notebook just to</text><text start="1621.64" dur="3.95">very quickly show what this looks like</text><text start="1628.41" dur="4.06">I&amp;#39;m gonna go ahead and pull open this</text><text start="1630.61" dur="3.93">jupiter notebook if you do not have</text><text start="1632.47" dur="3.45">jupiter notebook on your computer or if</text><text start="1634.54" dur="2.97">you&amp;#39;re not familiar with python or</text><text start="1635.92" dur="3.63">anything like that that&amp;#39;s okay</text><text start="1637.51" dur="4.65">i&amp;#39;m probably only gonna spend about two</text><text start="1639.55" dur="6.95">to three minutes talking about this but</text><text start="1642.16" dur="4.34">do want to pull this up as an example</text><text start="1647.27" dur="6.67">so when we are that is item missing this</text><text start="1652.23" dur="3.42">what I meant to do is pull up the unit</text><text start="1653.94" dur="3.6">missing this one I grabbed the wrong one</text><text start="1655.65" dur="5.04">so I apologize I&amp;#39;m gonna move back over</text><text start="1657.54" dur="7.56">here I&amp;#39;m going to go ahead and shut that</text><text start="1660.69" dur="8.3">down and open up the jupiter notebook 0</text><text start="1665.1" dur="3.89">1 unit missing this</text><text start="1670.86" dur="2.06">you</text><text start="1673.85" dur="6.48">and I&amp;#39;ll drop that in the chat here zero</text><text start="1676.52" dur="7.83">one unit missing this IP Y and B which</text><text start="1680.33" dur="6.8">can be found in that repository in my</text><text start="1684.35" dur="5.43">experience the most common method of</text><text start="1687.13" dur="5.71">handling unit missingness where we&amp;#39;re</text><text start="1689.78" dur="4.98">missing an entire row of data is if we</text><text start="1692.84" dur="4.17">have supplemental data on that</text><text start="1694.76" dur="5.07">individual to do something called weight</text><text start="1697.01" dur="5.04">class adjustments where we take our</text><text start="1699.83" dur="4.14">observations and we break them into</text><text start="1702.05" dur="6.42">classes and then we will weight them</text><text start="1703.97" dur="7.71">before doing our analysis so for example</text><text start="1708.47" dur="4.95">let&amp;#39;s say that I&amp;#39;m working in HR</text><text start="1711.68" dur="4.2">analytics so I&amp;#39;m working in human</text><text start="1713.42" dur="5.76">resources and I want to understand how</text><text start="1715.88" dur="6.48">satisfied are individuals within our</text><text start="1719.18" dur="5.52">organization let&amp;#39;s say that to make this</text><text start="1722.36" dur="4.56">simple we have two different departments</text><text start="1724.7" dur="4.08">we have a finance department and an</text><text start="1726.92" dur="4.29">accounting department for which I want</text><text start="1728.78" dur="5.75">to study individuals and let&amp;#39;s say that</text><text start="1731.21" dur="6.15">maybe when I administer these surveys</text><text start="1734.53" dur="5.86">that in the finance and accounting team</text><text start="1737.36" dur="5.28">they&amp;#39;re split perfectly evenly got 50%</text><text start="1740.39" dur="4.89">of people in finance 50% of people in</text><text start="1742.64" dur="4.8">accounting but let&amp;#39;s say that maybe</text><text start="1745.28" dur="4.71">people in finance had too much other</text><text start="1747.44" dur="4.71">stuff to do or were less responsible or</text><text start="1749.99" dur="3.75">whatever kind of motivation you want to</text><text start="1752.15" dur="4.47">ascribe to that and let&amp;#39;s say that</text><text start="1753.74" dur="5.31">people in finance were less likely to</text><text start="1756.62" dur="4.02">respond to my survey and let&amp;#39;s say that</text><text start="1759.05" dur="3.06">people in accounting whether it&amp;#39;s</text><text start="1760.64" dur="3.15">because they had less on their plate</text><text start="1762.11" dur="3.33">they&amp;#39;re more organized they&amp;#39;re more</text><text start="1763.79" dur="4.38">conscientious of this they just wanted</text><text start="1765.44" dur="5.73">to reply whatever else let&amp;#39;s say that</text><text start="1768.17" dur="7.08">accounting people responded more to my</text><text start="1771.17" dur="8.16">server and so because of this if I</text><text start="1775.25" dur="7.23">scroll down here what we&amp;#39;re going to do</text><text start="1779.33" dur="6.24">is we&amp;#39;re going to see that if I look at</text><text start="1782.48" dur="5.82">all of my survey responses so let&amp;#39;s say</text><text start="1785.57" dur="4.92">I administer this survey 50% of people</text><text start="1788.3" dur="4.2">are in accounting 50% of people are in</text><text start="1790.49" dur="3.96">finance but when I get my surveys back I</text><text start="1792.5" dur="3.5">get a disproportionate number of</text><text start="1794.45" dur="6.39">responses in the accounting department</text><text start="1796" dur="7.3">here about 77% of my responses are in</text><text start="1800.84" dur="4.47">accounting meaning that only about 22 or</text><text start="1803.3" dur="4.5">23 percent of my responses are in</text><text start="1805.31" dur="4.14">finance well if</text><text start="1807.8" dur="4.77">I was going to just take these values</text><text start="1809.45" dur="5.61">and I was just gonna do a simple average</text><text start="1812.57" dur="5.22">to understand on average how happy are</text><text start="1815.06" dur="5.13">my employees I might be putting some</text><text start="1817.79" dur="5.28">additional bias in my model here and</text><text start="1820.19" dur="5.04">that bias may come in because I received</text><text start="1823.07" dur="4.83">way more responses from accounting than</text><text start="1825.23" dur="4.98">from Finance so what I would like to do</text><text start="1827.9" dur="3.93">the strategy that we can employ is</text><text start="1830.21" dur="3.78">something called a weight class</text><text start="1831.83" dur="4.38">adjustment where I&amp;#39;m going to basically</text><text start="1833.99" dur="4.68">down weight all of my people from</text><text start="1836.21" dur="5.22">accounting I&amp;#39;m going to up weight all of</text><text start="1838.67" dur="4.44">my respondents from finance and what</text><text start="1841.43" dur="3.48">that&amp;#39;s going to do is going to put them</text><text start="1843.11" dur="4.53">back on an equal playing field because</text><text start="1844.91" dur="6.21">again 50% of people were in finance and</text><text start="1847.64" dur="6.63">50% of people were in accounting so the</text><text start="1851.12" dur="6.18">way that we do that is we take our full</text><text start="1854.27" dur="4.56">sample of people do all of the 100</text><text start="1857.3" dur="4.14">percent of people who we administered</text><text start="1858.83" dur="4.11">those surveys to both the observed and</text><text start="1861.44" dur="3.24">the missing we&amp;#39;re going to lump them all</text><text start="1862.94" dur="4.35">together and we break them into</text><text start="1864.68" dur="5.55">subgroups based on characteristics that</text><text start="1867.29" dur="5.61">we know in this case I know accounting</text><text start="1870.23" dur="6.09">and finance I&amp;#39;m going to give every</text><text start="1872.9" dur="6.96">individual a weight as well so the</text><text start="1876.32" dur="5.61">weight for people in group I is going to</text><text start="1879.86" dur="6.21">be what&amp;#39;s the true percentage of people</text><text start="1881.93" dur="7.08">in that group divided by what&amp;#39;s the</text><text start="1886.07" dur="6.24">percentage of observed responses in that</text><text start="1889.01" dur="5.58">group so for example in the accounting</text><text start="1892.31" dur="8.45">group the true percentage of people who</text><text start="1894.59" dur="9.36">were in accounting is 1/2 divided by the</text><text start="1900.76" dur="7.44">percentage of responses from accounting</text><text start="1903.95" dur="7.8">which was as we saw appear about 77% I</text><text start="1908.2" dur="6.79">let Python do the math and so the weight</text><text start="1911.75" dur="5.85">for each accounting vote is about 64</text><text start="1914.99" dur="5.34">point six percent</text><text start="1917.6" dur="5.939">I do the exact same thing for finance</text><text start="1920.33" dur="6.929">and what that means is that each finance</text><text start="1923.539" dur="6.27">though gets a weight of 2.2 so for every</text><text start="1927.259" dur="4.47">finance person this is finance and this</text><text start="1929.809" dur="4.68">is accounting every single finance</text><text start="1931.729" dur="7.02">person who replied they get a vote</text><text start="1934.489" dur="8">that&amp;#39;s 2.2 times so one person submits a</text><text start="1938.749" dur="6.39">survey I&amp;#39;m gonna up with them 2.2 times</text><text start="1942.489" dur="4.361">for every accounting person who</text><text start="1945.139" dur="4.321">responded instead of each person getting</text><text start="1946.85" dur="6.509">one vote they effectively get point six</text><text start="1949.46" dur="6.079">four five votes if we want to take the</text><text start="1953.359" dur="5.31">weights in each of those groups and</text><text start="1955.539" dur="6.22">multiply that by the number of responses</text><text start="1958.669" dur="5.85">that we get that ends up equalizing</text><text start="1961.759" dur="5.25">things so that the total weight from all</text><text start="1964.519" dur="4.41">of my accounting responses and the total</text><text start="1967.009" dur="4.41">weight from all of my finance responses</text><text start="1968.929" dur="7.47">end up being equal or in this case</text><text start="1971.419" dur="7.47">almost exactly equal once you have</text><text start="1976.399" dur="6.72">created those weights what you can do is</text><text start="1978.889" dur="6.181">just pass them in to SK learn so you&amp;#39;ll</text><text start="1983.119" dur="4.8">create a column of weights I&amp;#39;ve done</text><text start="1985.07" dur="6.55">that here just added a column in pandas</text><text start="1987.919" dur="6.461">DF bracket weights</text><text start="1991.62" dur="6.45">and then what we can do is use that in</text><text start="1994.38" dur="6.03">order to do more complex analyses so if</text><text start="1998.07" dur="6">I were to just calculate for example the</text><text start="2000.41" dur="6.03">raw average of employee satisfaction I</text><text start="2004.07" dur="5.7">did an employee satisfaction score of</text><text start="2006.44" dur="5.85">about five point seven but if I</text><text start="2009.77" dur="5.25">calculate the weighted average based on</text><text start="2012.29" dur="5.97">my employee satisfaction score it&amp;#39;s</text><text start="2015.02" dur="6.57">significantly lower I get here five</text><text start="2018.26" dur="5.37">point four five so that that average</text><text start="2021.59" dur="4.44">score went from five point seven down to</text><text start="2023.63" dur="4.86">five point four five above a decent drop</text><text start="2026.03" dur="4.44">and that&amp;#39;s because people in accounting</text><text start="2028.49" dur="3.9">this is based on the data I generated up</text><text start="2030.47" dur="4.5">top but people in accounting were on</text><text start="2032.39" dur="4.41">average happier with their jobs people</text><text start="2034.97" dur="4.8">in finance were on average less</text><text start="2036.8" dur="5.04">satisfied with their jobs what ends up</text><text start="2039.77" dur="4.47">happening though is when accounting over</text><text start="2041.84" dur="6.75">responds and Finance under responds is</text><text start="2044.24" dur="6.51">that that&amp;#39;s gonna skew our results now</text><text start="2048.59" dur="4.47">if you want to take this information and</text><text start="2050.75" dur="5.61">you want to build a more sophisticated</text><text start="2053.06" dur="6.63">model with this you can do so by passing</text><text start="2056.36" dur="5.67">DF weight if you&amp;#39;re if you&amp;#39;re a user of</text><text start="2059.69" dur="6.27">Python and specifically scikit-learn</text><text start="2062.03" dur="5.849">if you want to you can pass DF bracket</text><text start="2065.96" dur="4.23">weight that column or that vector of</text><text start="2067.879" dur="3.03">weights when you fit your model and it</text><text start="2070.19" dur="3.18">will weight</text><text start="2070.909" dur="4.081">your models results based on those those</text><text start="2073.37" dur="3.09">weights that you&amp;#39;ve given it so you</text><text start="2074.99" dur="3.87">could do this for a linear regression</text><text start="2076.46" dur="4.53">model or you could do this for a random</text><text start="2078.86" dur="5.34">forest or something else if you would</text><text start="2080.99" dur="6.45">like to two quick things that I want to</text><text start="2084.2" dur="4.92">call out one is that our goal with post</text><text start="2087.44" dur="5.34">weighting when we do this weight class</text><text start="2089.12" dur="6.42">adjustment is to decrease our bias but</text><text start="2092.78" dur="5.59">what should we be concerned about well</text><text start="2095.54" dur="5.2">when we decrease bias</text><text start="2098.37" dur="3.54">we tend to increase variance and so</text><text start="2100.74" dur="3.42">there&amp;#39;s an article from the New York</text><text start="2101.91" dur="4.26">Times back in 2016 some of you may be</text><text start="2104.16" dur="5.94">familiar with this there was an</text><text start="2106.17" dur="8.01">individual I believe it was the the</text><text start="2110.1" dur="6.3">title of this article is how one</text><text start="2114.18" dur="5.24">nineteen year old Illinois man is</text><text start="2116.4" dur="5.7">distorting national polling averages and</text><text start="2119.42" dur="4.3">what this ended up and so I encourage</text><text start="2122.1" dur="3.63">you to take a look at this if you would</text><text start="2123.72" dur="4.38">like to take a look at and read this but</text><text start="2125.73" dur="3.48">effectively they created these buckets</text><text start="2128.1" dur="3.21">they weren&amp;#39;t just looking at the</text><text start="2129.21" dur="5.03">accounting and Finance Department but</text><text start="2131.31" dur="6.18">instead they looked at for example age</text><text start="2134.24" dur="5.23">geography sex other information maybe</text><text start="2137.49" dur="3.93">political party all of these different</text><text start="2139.47" dur="3.75">buckets and by creating so many</text><text start="2141.42" dur="4.26">different buckets and attaching a weight</text><text start="2143.22" dur="4.89">to those buckets based on response rates</text><text start="2145.68" dur="5.22">what ends up happening is they decrease</text><text start="2148.11" dur="5.13">bias but there tends to be an increase</text><text start="2150.9" dur="4.74">in variance and so what ended up</text><text start="2153.24" dur="4.26">happening here I would encourage you to</text><text start="2155.64" dur="4.08">perhaps take a look at that later if you</text><text start="2157.5" dur="5.22">would like but what ends up happening is</text><text start="2159.72" dur="5.4">that the person who had that who is</text><text start="2162.72" dur="5.19">distorting national polling averages his</text><text start="2165.12" dur="6.21">weight as assigned by this approach was</text><text start="2167.91" dur="5.52">a was thirty times higher than the</text><text start="2171.33" dur="4.89">average individuals weight and was</text><text start="2173.43" dur="5.01">actually 300 times more than the person</text><text start="2176.22" dur="4.62">with the smallest weight in this poll so</text><text start="2178.44" dur="4.65">this one individual had an enormous li</text><text start="2180.84" dur="6">outsized influence on these polling</text><text start="2183.09" dur="7.32">averages so it&amp;#39;s something to to keep in</text><text start="2186.84" dur="5.88">mind related to this I&amp;#39;m making an</text><text start="2190.41" dur="4.8">assumption here I&amp;#39;m making an assumption</text><text start="2192.72" dur="6.33">and thank you Sam for for sharing that</text><text start="2195.21" dur="5.55">link I&amp;#39;m making an assumption here that</text><text start="2199.05" dur="3.12">I know that fifty percent of my people</text><text start="2200.76" dur="3.33">are in accounting and fifty percent of</text><text start="2202.17" dur="5.37">my people are in finance but that&amp;#39;s not</text><text start="2204.09" dur="6.42">always a realistic assumption so if I</text><text start="2207.54" dur="4.56">want to understand what percentage of</text><text start="2210.51" dur="3.75">people will support the Democratic</text><text start="2212.1" dur="3.96">candidate in the upcoming election and I</text><text start="2214.26" dur="3.99">want to look at things across age groups</text><text start="2216.06" dur="5.01">eighteen to thirty four thirty five to</text><text start="2218.25" dur="4.89">fifty four or fifty five and up I have</text><text start="2221.07" dur="3.78">to make a guess about that as I write</text><text start="2223.14" dur="4.53">here hopefully it&amp;#39;s an educated guess</text><text start="2224.85" dur="4.26">but what we see in past elections may</text><text start="2227.67" dur="3.39">not be indicative of what we&amp;#39;re going to</text><text start="2229.11" dur="2.969">see in this election thinking about the</text><text start="2231.06" dur="3.36">2016</text><text start="2232.079" dur="4.051">kradic presidential primary my</text><text start="2234.42" dur="4.26">understanding is far more young people</text><text start="2236.13" dur="5.07">came out and voted in that Democratic</text><text start="2238.68" dur="4.95">presidential primary largely in support</text><text start="2241.2" dur="4.919">of Bernie Sanders thinking about that</text><text start="2243.63" dur="6.869">that&amp;#39;s something that we may not have</text><text start="2246.119" dur="6.24">noticed in 2010 2008 2004</text><text start="2250.499" dur="3.87">so if we use past data to predict the</text><text start="2252.359" dur="4.26">future it can be really challenging to</text><text start="2254.369" dur="4.531">do that in a way that&amp;#39;s principled we</text><text start="2256.619" dur="4.68">think about when then-senator Obama was</text><text start="2258.9" dur="5.909">running in 2008 what ends up happening</text><text start="2261.299" dur="5.22">was when when he was running against</text><text start="2264.809" dur="4.381">secretary or then I should say</text><text start="2266.519" dur="4.47">then-senator Clinton in oh wait what</text><text start="2269.19" dur="4.409">ends up happening is that if you just</text><text start="2270.989" dur="4.88">looked at information in 2000 and 2004</text><text start="2273.599" dur="4.561">you&amp;#39;re likely going to dramatically</text><text start="2275.869" dur="4.271">underestimate the proportion of people</text><text start="2278.16" dur="4.26">of color specifically black voters who</text><text start="2280.14" dur="5.729">came out in support of Obama during the</text><text start="2282.42" dur="5.669">2008 election so this is this weight</text><text start="2285.869" dur="4.68">class adjustment method is something</text><text start="2288.089" dur="4.68">that you can sometimes do but you can&amp;#39;t</text><text start="2290.549" dur="4.29">always do that and so it&amp;#39;s important to</text><text start="2292.769" dur="4.35">keep in mind some of the limitations of</text><text start="2294.839" dur="4.62">this we would be assuming that we know</text><text start="2297.119" dur="4.261">what the distribution of in this case</text><text start="2299.459" dur="5.49">what I&amp;#39;ve highlighted the age groups are</text><text start="2301.38" dur="5.939">but that&amp;#39;s certainly not a guarantee I&amp;#39;m</text><text start="2304.949" dur="6.231">gonna go ahead and shut this and I&amp;#39;m</text><text start="2307.319" dur="3.861">gonna move back over to the slides</text><text start="2311.45" dur="2.06">you</text><text start="2314.91" dur="4.57">so to try and talk about how to pull</text><text start="2317.74" dur="3.78">some of these pieces together in a work</text><text start="2319.48" dur="4.379">flow we have not talked about we have</text><text start="2321.52" dur="4.5">not talked about imputation 4 for unit</text><text start="2323.859" dur="6.121">non-response yet but we&amp;#39;ll get into that</text><text start="2326.02" dur="5.97">here in terms of my work flow I start by</text><text start="2329.98" dur="4.589">saying how much missing data do I have</text><text start="2331.99" dur="4.17">and is it worth my time to try and</text><text start="2334.569" dur="3.391">address it anytime I&amp;#39;m doing a data</text><text start="2336.16" dur="4.8">science problem that&amp;#39;s one of the first</text><text start="2337.96" dur="4.98">things I look at then I say is it</text><text start="2340.96" dur="3.99">reasonable to attempt deductive</text><text start="2342.94" dur="5.34">imputation which we&amp;#39;re going to talk</text><text start="2344.95" dur="5.159">about momentarily then if my goal is to</text><text start="2348.28" dur="4.47">generate predictions then I&amp;#39;m going to</text><text start="2350.109" dur="4.26">use the pattern sub model approach if I</text><text start="2352.75" dur="3.78">want to conduct inference that I will</text><text start="2354.369" dur="5.581">use the best imputation method available</text><text start="2356.53" dur="4.769">ideally proper imputation so we&amp;#39;ll talk</text><text start="2359.95" dur="2.7">about what each of these are because a</text><text start="2361.299" dur="4.351">lot of those bold in terms are things we</text><text start="2362.65" dur="4.919">haven&amp;#39;t seen yet in order to get into</text><text start="2365.65" dur="4.23">that though one last thing that I need</text><text start="2367.569" dur="3.331">to talk about are three different types</text><text start="2369.88" dur="3.57">of missing data</text><text start="2370.9" dur="4.53">now you when you look at your data and</text><text start="2373.45" dur="4.29">you see a bunch of na s in your data or</text><text start="2375.43" dur="4.2">a handful of na s in your data they all</text><text start="2377.74" dur="3.24">look the same to us but there are</text><text start="2379.63" dur="3.35">actually three different types of</text><text start="2380.98" dur="4.53">missing data that are important to know</text><text start="2382.98" dur="4.9">so I&amp;#39;m taking inspiration from my friend</text><text start="2385.51" dur="4.5">Allison here my friend Allison she is</text><text start="2387.88" dur="4.35">getting her PhD in biology at Notre Dame</text><text start="2390.01" dur="4.14">very proud of her so let&amp;#39;s say that</text><text start="2392.23" dur="4.53">she&amp;#39;s a grad student in a lab working</text><text start="2394.15" dur="4.62">late and while she&amp;#39;s pipetting in the</text><text start="2396.76" dur="4.079">lab she reaches for her pen and</text><text start="2398.77" dur="4.62">accidentally knocks one petri dish off</text><text start="2400.839" dur="4.5">the desk so from that petri dish my</text><text start="2403.39" dur="4.28">friend Allison loses all of the data</text><text start="2405.339" dur="5.671">that she otherwise would have collected</text><text start="2407.67" dur="5.169">so over here I&amp;#39;m looking at what maybe</text><text start="2411.01" dur="4.02">that might look like in terms of data</text><text start="2412.839" dur="4.351">gathering so Allison was able to measure</text><text start="2415.03" dur="5.339">how much bacteria or what was the width</text><text start="2417.19" dur="5.34">of the bacteria in her petri dish on day</text><text start="2420.369" dur="4.861">one for all of these different petri</text><text start="2422.53" dur="5.789">dishes did the same thing on day two</text><text start="2425.23" dur="5.16">but this here you can see 16 millimeters</text><text start="2428.319" dur="3.931">but really that would be an n/a in our</text><text start="2430.39" dur="4.38">data that&amp;#39;s something that we do not</text><text start="2432.25" dur="6.059">have access to we would call that</text><text start="2434.77" dur="5.069">missing completely at random this data</text><text start="2438.309" dur="3.78">is missing completely at random because</text><text start="2439.839" dur="4.26">there&amp;#39;s no systematic differences</text><text start="2442.089" dur="4.551">between that data that&amp;#39;s missing and the</text><text start="2444.099" dur="5.071">data that we&amp;#39;ve observed</text><text start="2446.64" dur="5.35">moving on to the next example there&amp;#39;s</text><text start="2449.17" dur="4.8">something called missing at random and I</text><text start="2451.99" dur="4.41">apologize please do not shoot the</text><text start="2453.97" dur="3.93">messenger I was not in the room when</text><text start="2456.4" dur="3.09">people decided on these terms</text><text start="2457.9" dur="3.03">I think these terms are silly and I wish</text><text start="2459.49" dur="5.55">there was a better way to describe them</text><text start="2460.93" dur="6.87">but I apologize on behalf of</text><text start="2465.04" dur="5.19">statisticians here so we talked about</text><text start="2467.8" dur="5.13">missing completely at random here we&amp;#39;ve</text><text start="2470.23" dur="3.81">got missing at random so let&amp;#39;s say that</text><text start="2472.93" dur="3.12">we work for the Department of</text><text start="2474.04" dur="4.44">Transportation and we&amp;#39;re looking at the</text><text start="2476.05" dur="5.1">Pennsylvania Turnpike a toll road that a</text><text start="2478.48" dur="4.86">highway that hasn&amp;#39;t told me with on it</text><text start="2481.15" dur="4.23">so that you can understand people have</text><text start="2483.34" dur="3.33">to pay whenever they go through this</text><text start="2485.38" dur="3.63">toll booth in order to use the</text><text start="2486.67" dur="5.49">Pennsylvania Turnpike and let&amp;#39;s say that</text><text start="2489.01" dur="6.63">there&amp;#39;s a a sensor set up to track how</text><text start="2492.16" dur="4.56">many cars go through a given gate in a</text><text start="2495.64" dur="3.6">given time window</text><text start="2496.72" dur="4.5">well that sensor breaks and doesn&amp;#39;t</text><text start="2499.24" dur="4.95">gather any information between 7 and 10</text><text start="2501.22" dur="6.63">a.m. what we would describe that as is</text><text start="2504.19" dur="5.58">data that&amp;#39;s missing at random and the</text><text start="2507.85" dur="4.95">reason that we call it missing at random</text><text start="2509.77" dur="6.3">is that conditional on some data that we</text><text start="2512.8" dur="6.21">do have in hand the data of interest is</text><text start="2516.07" dur="4.86">not systematically different so whether</text><text start="2519.01" dur="4.44">or not that data point is missing</text><text start="2520.93" dur="6.18">depends on data that we have observed in</text><text start="2523.45" dur="6.98">this case we have observed time that is</text><text start="2527.11" dur="6.66">information that we do have and time</text><text start="2530.43" dur="5.32">contributes to that missingness that</text><text start="2533.77" dur="4.29">missingness is based on are contingent</text><text start="2535.75" dur="5.04">upon those specific hours that data is</text><text start="2538.06" dur="4.32">missing you might imagine this solution</text><text start="2540.79" dur="4.32">for trying to tackle this type of</text><text start="2542.38" dur="5.34">missing data is maybe we want to use the</text><text start="2545.11" dur="5.13">time in order to help us fill in or</text><text start="2547.72" dur="6.649">generate some value for those number of</text><text start="2550.24" dur="7.309">vehicles that are that we are missing</text><text start="2554.369" dur="5.19">the last type of data that&amp;#39;s missing the</text><text start="2557.549" dur="5.49">last type of missingness I should say is</text><text start="2559.559" dur="5.13">data that&amp;#39;s not missing at random let&amp;#39;s</text><text start="2563.039" dur="4.111">say that I administer a survey and that</text><text start="2564.689" dur="5.13">survey includes a question about income</text><text start="2567.15" dur="4.859">people who have lower incomes are less</text><text start="2569.819" dur="4.47">likely to respond to that question about</text><text start="2572.009" dur="4.11">income what we call that data that&amp;#39;s not</text><text start="2574.289" dur="4.5">missing at random because whether or not</text><text start="2576.119" dur="6.811">something is missing depends on the</text><text start="2578.789" dur="6.72">value of that missing thing itself here</text><text start="2582.93" dur="5.909">for example we see that people who have</text><text start="2585.509" dur="5.55">lower incomes are on average less likely</text><text start="2588.839" dur="3.96">to share their incomes with me so if we</text><text start="2591.059" dur="5.341">wanted to do something simple like</text><text start="2592.799" dur="6.06">calculate the average of this data I can</text><text start="2596.4" dur="4.649">calculate the average of income but it&amp;#39;s</text><text start="2598.859" dur="4.14">going to be skewed pretty significantly</text><text start="2601.049" dur="5.01">upward we&amp;#39;re gonna see a value that&amp;#39;s</text><text start="2602.999" dur="4.681">much higher than it should be now this</text><text start="2606.059" dur="3.42">is the most complicated type of</text><text start="2607.68" dur="5.629">missingness to work with because we</text><text start="2609.479" dur="6.151">don&amp;#39;t have access to these incomes here</text><text start="2613.309" dur="4.18">so those are the three different types</text><text start="2615.63" dur="3.929">of missingness and i want to talk about</text><text start="2617.489" dur="6.63">a couple of ways that in the 15 minutes</text><text start="2619.559" dur="6.66">we&amp;#39;ve got left that we can handle so</text><text start="2624.119" dur="4.59">there are five different methods here</text><text start="2626.219" dur="4.47">that I outline I&amp;#39;m going to move quickly</text><text start="2628.709" dur="3.57">through them because there&amp;#39;s a reporting</text><text start="2630.689" dur="3.451">here and I want to be respectful of</text><text start="2632.279" dur="4.861">folks this time and not hold folks over</text><text start="2634.14" dur="5.33">but again please ask any questions that</text><text start="2637.14" dur="4.62">you have in the chat</text><text start="2639.47" dur="6.88">so let&amp;#39;s start by talking about</text><text start="2641.76" dur="6.39">deductive imputation deductive is it has</text><text start="2646.35" dur="4.26">to do with logic we&amp;#39;re going to deduce</text><text start="2648.15" dur="6.03">values we&amp;#39;re going to use logical rules</text><text start="2650.61" dur="4.8">to understand how we can fill data in so</text><text start="2654.18" dur="3.15">let&amp;#39;s say that there was a survey that</text><text start="2655.41" dur="3.51">asks if somebody was the victim of a</text><text start="2657.33" dur="4.74">crime in the last 12 months and that</text><text start="2658.92" dur="4.98">person says no and then of the same</text><text start="2662.07" dur="3.87">survey has a later question that says</text><text start="2663.9" dur="4.08">really the victim of a violent crime in</text><text start="2665.94" dur="5.7">the last 12 months and that respondent</text><text start="2667.98" dur="5.61">leaves the answer blank we can use logic</text><text start="2671.64" dur="4.08">we don&amp;#39;t have to make any guesses or we</text><text start="2673.59" dur="5.46">don&amp;#39;t have to do any inference we can</text><text start="2675.72" dur="6.15">use logic to say given the answer to my</text><text start="2679.05" dur="4.71">first question I know the answer to that</text><text start="2681.87" dur="5.18">and I can fill in that missing value</text><text start="2683.76" dur="6.48">through logic this requires specific</text><text start="2687.05" dur="5.29">coding so you would have to as we get</text><text start="2690.24" dur="3.9">new data we have to recognize how do my</text><text start="2692.34" dur="4.02">variables or my columns relate to one</text><text start="2694.14" dur="4.35">another you would have to code that up</text><text start="2696.36" dur="4.5">that&amp;#39;s not something that is going to be</text><text start="2698.49" dur="4.2">consistent across all data sets so you</text><text start="2700.86" dur="5.19">can&amp;#39;t just download a library to do that</text><text start="2702.69" dur="5.55">it can be time-consuming but it&amp;#39;s good</text><text start="2706.05" dur="4.41">because it doesn&amp;#39;t require any inference</text><text start="2708.24" dur="4.41">and it does not matter what type of</text><text start="2710.46" dur="4.53">missing data you&amp;#39;re working with whether</text><text start="2712.65" dur="4.62">it&amp;#39;s missing at random not at random</text><text start="2714.99" dur="7.59">completely at random you can do this in</text><text start="2717.27" dur="7.34">any of those cases the next thing that I</text><text start="2722.58" dur="5.16">want to bring up is mean median and mode</text><text start="2724.61" dur="4.74">imputation so I imagine that many of you</text><text start="2727.74" dur="5.52">have done this at some point or another</text><text start="2729.35" dur="5.8">for any n a value or any missing value</text><text start="2733.26" dur="3.9">and your data you just replace your</text><text start="2735.15" dur="5.61">missing value with the mean or the</text><text start="2737.16" dur="6.6">median or the mode of that column it&amp;#39;s a</text><text start="2740.76" dur="5.09">quick fix it&amp;#39;s easy to implement and it</text><text start="2743.76" dur="4.8">seems reasonable but it can really</text><text start="2745.85" dur="5.02">significantly distort your histogram and</text><text start="2748.56" dur="4.2">it underestimates your variance and</text><text start="2750.87" dur="3.66">we&amp;#39;ll talk in a minute about why that</text><text start="2752.76" dur="4.89">variance or that variability is so</text><text start="2754.53" dur="5.88">important it should only be considered</text><text start="2757.65" dur="5.97">if your data is missing completely at</text><text start="2760.41" dur="5.85">random so if you can say based on my</text><text start="2763.62" dur="5.58">understanding of my data I can truly</text><text start="2766.26" dur="5.13">believe and maybe some quick analyses</text><text start="2769.2" dur="3.93">that I do in my data you can say look I</text><text start="2771.39" dur="4.229">believe that my data are missing</text><text start="2773.13" dur="5.25">completely at random this would only be</text><text start="2775.619" dur="4.591">appropriate in that case but even then</text><text start="2778.38" dur="5.969">you probably shouldn&amp;#39;t do this in their</text><text start="2780.21" dur="6.659">better ways of handling missing data so</text><text start="2784.349" dur="4.881">this is an example so what I have up top</text><text start="2786.869" dur="4.351">and this is all in that zero two</text><text start="2789.23" dur="4.69">notebook if you want to take a look at</text><text start="2791.22" dur="5.03">that zero two item missingness these</text><text start="2793.92" dur="4.71">visuals come directly from that movement</text><text start="2796.25" dur="5.68">so here up top</text><text start="2798.63" dur="6.84">this is the real histogram of data with</text><text start="2801.93" dur="7.649">in blue this blue vertical bar shows us</text><text start="2805.47" dur="6.96">the true average of that data down below</text><text start="2809.579" dur="6.361">we&amp;#39;re looking at the same data but in</text><text start="2812.43" dur="6.75">blue I have which data I&amp;#39;ve observed and</text><text start="2815.94" dur="6.359">then any value that was missing I filled</text><text start="2819.18" dur="5.7">the mean in so here this orange bar</text><text start="2822.299" dur="4.171">gives me the mean you&amp;#39;ll notice that</text><text start="2824.88" dur="4.05">that&amp;#39;s a pretty dramatic difference</text><text start="2826.47" dur="5.25">between the top and the bottom if you&amp;#39;re</text><text start="2828.93" dur="4.649">missing ten twenty thirty percent of</text><text start="2831.72" dur="3.119">values in a column which is not the</text><text start="2833.579" dur="4.02">craziest thing in the world to think</text><text start="2834.839" dur="4.95">through you might get something like</text><text start="2837.599" dur="4.71">this so first off you&amp;#39;re going to</text><text start="2839.789" dur="5.78">distort that histogram so that&amp;#39;s one</text><text start="2842.309" dur="3.26">challenge of working with this</text><text start="2845.92" dur="4.23">the other thing that I want to run</text><text start="2847.96" dur="5.28">through is why is under estimating</text><text start="2850.15" dur="5.78">variance a bad thing so here I&amp;#39;ve got</text><text start="2853.24" dur="5.82">the formula for your sample standard</text><text start="2855.93" dur="6.19">deviation and you can go through this if</text><text start="2859.06" dur="4.95">you would like but in short if you are</text><text start="2862.12" dur="4.86">working with the sample standard</text><text start="2864.01" dur="5.6">deviation what you&amp;#39;ll notice is that if</text><text start="2866.98" dur="5.85">you&amp;#39;ve observed your first K</text><text start="2869.61" dur="6.04">observations and then observation k plus</text><text start="2872.83" dur="5.31">1 all the way through n those are</text><text start="2875.65" dur="6.36">missing and you try and use mean</text><text start="2878.14" dur="6.45">imputation what you&amp;#39;ll do is for K plus</text><text start="2882.01" dur="5.22">1 through n you fill the meaning for</text><text start="2884.59" dur="5.16">those values that formula ships instead</text><text start="2887.23" dur="6.12">of dividing by K you&amp;#39;re gonna divide by</text><text start="2889.75" dur="6.42">n that denominator gets bigger but you</text><text start="2893.35" dur="5.67">can rewrite this formula by breaking it</text><text start="2896.17" dur="5.93">out for the first K values those values</text><text start="2899.02" dur="7.08">you&amp;#39;ve observed the actual real data and</text><text start="2902.1" dur="5.77">here k plus 1 through n all of those</text><text start="2906.1" dur="5.4">values that were missing and you filled</text><text start="2907.87" dur="6.66">in the mean for we&amp;#39;ll notice here we&amp;#39;ve</text><text start="2911.5" dur="5.19">got x-bar which is our sample mean minus</text><text start="2914.53" dur="5.73">x-bar which is our sample mean that&amp;#39;s</text><text start="2916.69" dur="5.79">zero zero squared is zero and if you add</text><text start="2920.26" dur="5.4">a bunch of those up that still gives you</text><text start="2922.48" dur="6.21">zero so what ends up happening is that</text><text start="2925.66" dur="4.74">this part of your this part of your</text><text start="2928.69" dur="5.88">variance or this part of your standard</text><text start="2930.4" dur="6.42">deviation remains exactly the same you</text><text start="2934.57" dur="5.61">don&amp;#39;t add anything there but your</text><text start="2936.82" dur="5.43">denominator gets bigger the reason I</text><text start="2940.18" dur="5.19">walk through that is by doing this your</text><text start="2942.25" dur="4.59">standard deviation gets smaller your</text><text start="2945.37" dur="4.23">standard deviation is used in a number</text><text start="2946.84" dur="5.55">of ways one if you try and generate a</text><text start="2949.6" dur="5.04">confidence interval then your standard</text><text start="2952.39" dur="5.85">you&amp;#39;re gonna get maybe a 95% confidence</text><text start="2954.64" dur="6.09">interval but that&amp;#39;s gonna get smaller I</text><text start="2958.24" dur="4.65">apologize if you can you can either see</text><text start="2960.73" dur="6.03">the Lightning or hear the thunder on</text><text start="2962.89" dur="6.57">sign so you may see your your confidence</text><text start="2966.76" dur="4.35">interval get much smaller depending on</text><text start="2969.46" dur="3.51">how many of those values you move it</text><text start="2971.11" dur="4.17">that&amp;#39;s not because you&amp;#39;re getting more</text><text start="2972.97" dur="4.05">confident or that&amp;#39;s not because you</text><text start="2975.28" dur="3.08">decrease your level of confidence and</text><text start="2977.02" dur="4.55">say 95</text><text start="2978.36" dur="5.24">to 90% confidence that&amp;#39;s just because we</text><text start="2981.57" dur="4.43">imputed our meat so we might become</text><text start="2983.6" dur="4.93">falsely confident in our results</text><text start="2986" dur="5.38">something similar happens to p-value</text><text start="2988.53" dur="5.55">where your p-value may shrink so your</text><text start="2991.38" dur="5.97">p-value of point O 7 now becomes a</text><text start="2994.08" dur="5.64">p-value of say point O 3 all of the</text><text start="2997.35" dur="5.55">sudden we get a significant result but</text><text start="2999.72" dur="5.43">even though we but that significant</text><text start="3002.9" dur="6.54">result is only because we filled in this</text><text start="3005.15" dur="5.94">missing data so Ramesh asks and I</text><text start="3009.44" dur="3.33">apologize if I mispronounce anybody&amp;#39;s</text><text start="3011.09" dur="4.14">name thank you for your question you</text><text start="3012.77" dur="4.44">mesh do you see significant gains in</text><text start="3015.23" dur="4.56">model performance from imputing using</text><text start="3017.21" dur="4.62">methods like Miss forests or other model</text><text start="3019.79" dur="4.83">based in QT how do they compare it in</text><text start="3021.83" dur="5.43">simpler imputing methods like mean</text><text start="3024.62" dur="4.5">imputation so mean median and mode</text><text start="3027.26" dur="5.07">imputation and if I move to the next</text><text start="3029.12" dur="4.95">slide this is a write-up of that</text><text start="3032.33" dur="4.04">confidence interval gets smaller the</text><text start="3034.07" dur="4.32">p-value gets smaller but that&amp;#39;s not</text><text start="3036.37" dur="5.35">something that&amp;#39;s that&amp;#39;s not real that&amp;#39;s</text><text start="3038.39" dur="6.39">not valid if we look at mode imputation</text><text start="3041.72" dur="5.55">we see a similar challenge where one</text><text start="3044.78" dur="5.04">value is artificially inflated at ton</text><text start="3047.27" dur="4.14">but we&amp;#39;re still going to see in effect</text><text start="3049.82" dur="3.29">where that standard deviation is almost</text><text start="3051.41" dur="4.8">certainly going to get smaller and</text><text start="3053.11" dur="6.16">artificially so so when it comes to</text><text start="3056.21" dur="5.25">trying to impute properly what you</text><text start="3059.27" dur="5.82">should do and in the interest of time</text><text start="3061.46" dur="5.76">I&amp;#39;m gonna skip ahead if you were to try</text><text start="3065.09" dur="4.5">and fit a single regression imputation</text><text start="3067.22" dur="4.65">where you fit a model to your data like</text><text start="3069.59" dur="3.93">Miss forests or something else you&amp;#39;re</text><text start="3071.87" dur="3.81">gonna get values that look like this</text><text start="3073.52" dur="4.35">where those imputed values are all</text><text start="3075.68" dur="4.2">lumped in the middle it&amp;#39;s still not</text><text start="3077.87" dur="4.26">going to work the way that you expect it</text><text start="3079.88" dur="4.83">to and that&amp;#39;s because when you generate</text><text start="3082.13" dur="5.52">those predictions that deterministic</text><text start="3084.71" dur="7.71">computation that you do is usually going</text><text start="3087.65" dur="6.3">to fall on one line instead you imagine</text><text start="3092.42" dur="3.24">that you probably want to generate</text><text start="3093.95" dur="4.44">values that look more like this that</text><text start="3095.66" dur="4.34">resembled a true variability that you</text><text start="3098.39" dur="5.75">see in your rule</text><text start="3100" dur="8.43">so because of that in order to properly</text><text start="3104.14" dur="7.17">impute we need to impose because anytime</text><text start="3108.43" dur="5.189">you fill in a value with one number</text><text start="3111.31" dur="4.68">you&amp;#39;re treating it like you know that</text><text start="3113.619" dur="5.791">true number and that&amp;#39;s not the case if</text><text start="3115.99" dur="6.39">you fill in any missing value na with a</text><text start="3119.41" dur="5.64">zero or a ten or a thousand you&amp;#39;re</text><text start="3122.38" dur="4.89">acting like you know that value don&amp;#39;t so</text><text start="3125.05" dur="5.7">the way to properly impute missing data</text><text start="3127.27" dur="7.2">is to make like ten copies of your data</text><text start="3130.75" dur="6.48">set to do imputation with some</text><text start="3134.47" dur="4.71">stochastic behavior so you can add in</text><text start="3137.23" dur="4.11">like a random error if you were to do a</text><text start="3139.18" dur="4.59">regression model or if you were to do</text><text start="3141.34" dur="5.13">miss forest you could you would be able</text><text start="3143.77" dur="6.18">to maybe adding some randomness into</text><text start="3146.47" dur="6.33">that you would do that on all ten of</text><text start="3149.95" dur="4.89">your copies of your data sets once</text><text start="3152.8" dur="3.72">you&amp;#39;ve got ten copies of your data set</text><text start="3154.84" dur="4.29">that are full they&amp;#39;ve got some different</text><text start="3156.52" dur="4.47">values in them then you would build like</text><text start="3159.13" dur="5.16">your final model or do your final</text><text start="3160.99" dur="5.73">analysis on each of those data sets then</text><text start="3164.29" dur="4.98">combine your results together just like</text><text start="3166.72" dur="4.71">you would aggregate results in a random</text><text start="3169.27" dur="4.92">forest so if you are doing a</text><text start="3171.43" dur="4.46">classification or a regression model you</text><text start="3174.19" dur="3.659">can average your predictions for</text><text start="3175.89" dur="4.75">classification you would do like a vote</text><text start="3177.849" dur="5.671">based prediction across all ten of those</text><text start="3180.64" dur="5.1">models that you constructed there&amp;#39;s a</text><text start="3183.52" dur="4.86">visual here that I think helps to drive</text><text start="3185.74" dur="6.119">that point home again you start with</text><text start="3188.38" dur="6.03">your data there&amp;#39;s a bunch of pound signs</text><text start="3191.859" dur="6.061">or hash tag symbols in here that</text><text start="3194.41" dur="6.51">represent missing data you make a bunch</text><text start="3197.92" dur="6.449">of copies of that this image has three</text><text start="3200.92" dur="5.64">copies of your data and fills those in</text><text start="3204.369" dur="4.621">using some sort of random imputation</text><text start="3206.56" dur="6.48">method like a regression model with some</text><text start="3208.99" dur="6.75">random error then you build your final</text><text start="3213.04" dur="5.94">model or analysis on those three data</text><text start="3215.74" dur="6.599">sets you get your results and then you</text><text start="3218.98" dur="5.28">combine them together if your goal is to</text><text start="3222.339" dur="3.391">do prediction you can do that if your</text><text start="3224.26" dur="4.14">goal is to do inference there&amp;#39;s</text><text start="3225.73" dur="4.44">something called rubens rules we&amp;#39;re</text><text start="3228.4" dur="4.87">gonna drop that name in the channel here</text><text start="3230.17" dur="5.65">Ruben&amp;#39;s rules</text><text start="3233.27" dur="7.02">in order to combine those estimates</text><text start="3235.82" dur="7.11">together so I&amp;#39;ve got a little bit of</text><text start="3240.29" dur="4.74">context on that on Rubens rules so</text><text start="3242.93" dur="3.57">there&amp;#39;s documentation in the repository</text><text start="3245.03" dur="4.82">if that&amp;#39;s something that you want to</text><text start="3246.5" dur="3.35">explore on your</text><text start="3250.54" dur="4.06">so there&amp;#39;s content in the notebook on</text><text start="3253.1" dur="4.47">that but the last thing that I want to</text><text start="3254.6" dur="5.55">go through is I&amp;#39;ve talked about those</text><text start="3257.57" dur="5.52">first four methods of imputation the</text><text start="3260.15" dur="5.94">pattern sub-model approach is where I</text><text start="3263.09" dur="4.77">want to end up for today and that</text><text start="3266.09" dur="4.08">pattern sub-model approach for handling</text><text start="3267.86" dur="5.88">missing data is you&amp;#39;re gonna take your</text><text start="3270.17" dur="5.73">data set and you break it into subsets</text><text start="3273.74" dur="5.34">of data based on how your data is</text><text start="3275.9" dur="6">missing then what you&amp;#39;re going to do is</text><text start="3279.08" dur="5.13">build one model on each of those subsets</text><text start="3281.9" dur="4.02">creating many different models you won&amp;#39;t</text><text start="3284.21" dur="4.53">combine those models together you will</text><text start="3285.92" dur="5.94">end up with many different models so a</text><text start="3288.74" dur="5.64">visual example look at the data set on</text><text start="3291.86" dur="8">the left hand side I have a Y I have an</text><text start="3294.38" dur="10.29">x1 and an x2 what I can do is I can take</text><text start="3299.86" dur="7.09">my first two rows what here because I&amp;#39;ve</text><text start="3304.67" dur="6.21">observed the same data I&amp;#39;m gonna call</text><text start="3306.95" dur="7.8">that pattern one my next two rows I&amp;#39;m</text><text start="3310.88" dur="7.74">gonna call that pattern to the next two</text><text start="3314.75" dur="7.02">rows our pattern 3 and then my final row</text><text start="3318.62" dur="5.43">is pattern 4 I&amp;#39;m gonna group my data</text><text start="3321.77" dur="4.71">into four chunks based on how my data</text><text start="3324.05" dur="4.32">are observed in missing together and I&amp;#39;m</text><text start="3326.48" dur="3.42">gonna fit a different model on each of</text><text start="3328.37" dur="4.05">those so you would end up with four</text><text start="3329.9" dur="4.68">different models here one model would be</text><text start="3332.42" dur="5.76">if you wanted to do a linear regression</text><text start="3334.58" dur="7.23">model y equals beta naught plus beta 1</text><text start="3338.18" dur="5.04">times X 1 plus beta 2 times X 2 and that</text><text start="3341.81" dur="3.84">would be fit on those first few</text><text start="3343.22" dur="4.91">observations then you would fit a</text><text start="3345.65" dur="6.03">separate linear regression model on</text><text start="3348.13" dur="6.25">these next observations where you just</text><text start="3351.68" dur="4.8">exclude any value for X 2 because you</text><text start="3354.38" dur="4.35">don&amp;#39;t have any value for x 2 so that</text><text start="3356.48" dur="5.43">would be a model like y equals beta</text><text start="3358.73" dur="4.92">naught plus beta 1 times X 1 so you</text><text start="3361.91" dur="6.27">would based on this come up with four</text><text start="3363.65" dur="6.63">different models there are a lot of</text><text start="3368.18" dur="5.22">advantages to this and if your goal is</text><text start="3370.28" dur="5.07">just to make predictions then you should</text><text start="3373.4" dur="4.56">use the pattern sub model approach that</text><text start="3375.35" dur="5.07">I described here that this will</text><text start="3377.96" dur="4.15">outperform imputation methods if your</text><text start="3380.42" dur="3.67">data are not missing at random</text><text start="3382.11" dur="4.08">and it&amp;#39;s gonna perform about on par with</text><text start="3384.09" dur="4.11">imputation methods are filling in</text><text start="3386.19" dur="3.63">methods if your data are missing at</text><text start="3388.2" dur="4.41">random or missing completely at random</text><text start="3389.82" dur="5.01">it does not require missingness</text><text start="3392.61" dur="3.48">assumptions so that&amp;#39;s one of in my</text><text start="3394.83" dur="4.68">opinion one of the cool things about</text><text start="3396.09" dur="6.21">that it is a based on my understanding a</text><text start="3399.51" dur="5.37">relatively new method there was a paper</text><text start="3402.3" dur="5.28">released in I think September of 2018 on</text><text start="3404.88" dur="7.14">that I think it was mentioned a a long</text><text start="3407.58" dur="5.7">time ago in a more esoteric paper but to</text><text start="3412.02" dur="3.72">my knowledge there&amp;#39;s not a ton of</text><text start="3413.28" dur="3.81">machinery like in Python to implement</text><text start="3415.74" dur="5.88">this so it has to be a little bit more</text><text start="3417.09" dur="6.39">of a manual process so I know that we&amp;#39;re</text><text start="3421.62" dur="4.2">right at the end of time here and I do</text><text start="3423.48" dur="4.59">want to be respectful of focus time so</text><text start="3425.82" dur="5.4">first off thank you so much for for</text><text start="3428.07" dur="6.12">showing up I totally understand if you</text><text start="3431.22" dur="5.37">need to hop off so please feel free to</text><text start="3434.19" dur="4.08">do that I&amp;#39;m happy to stick around and</text><text start="3436.59" dur="6.05">tie you feel free to come on if you mean</text><text start="3438.27" dur="4.37">to give me the hook and pull me off</text></transcript>