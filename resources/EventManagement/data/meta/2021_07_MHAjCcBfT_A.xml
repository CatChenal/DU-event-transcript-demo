<?xml version="1.0" encoding="utf-8" ?><transcript><text start="3.439" dur="4.241">okay</text><text start="5.04" dur="3.599">hello and welcome to data umbrella&amp;#39;s</text><text start="7.68" dur="4.24">webinar for</text><text start="8.639" dur="3.281">um october</text><text start="12.799" dur="3.681">so i&amp;#39;m just going to go over the agenda</text><text start="14.559" dur="4.56">i&amp;#39;m going to do a brief introduction</text><text start="16.48" dur="5.12">then there will be the workshop by hugo</text><text start="19.119" dur="3.441">and james and you can ask questions</text><text start="21.6" dur="3.519">along the way</text><text start="22.56" dur="4.08">in the chat or actually the best place</text><text start="25.119" dur="4.32">to ask questions is the q</text><text start="26.64" dur="3.68">a and there&amp;#39;s an option to upvote as</text><text start="29.439" dur="3.441">well</text><text start="30.32" dur="4.64">um so yet um asking the q a if you</text><text start="32.88" dur="2.64">happen to post it on the chat by mistake</text><text start="34.96" dur="2.64">i can</text><text start="35.52" dur="3.52">also transfer it over to q a so that</text><text start="37.6" dur="5.44">would be fine too</text><text start="39.04" dur="4">and this webinar is being recorded</text><text start="43.36" dur="4.08">uh briefly about me i am a statistician</text><text start="45.76" dur="3.2">and data scientist and i am the founder</text><text start="47.44" dur="4.799">of data umbrella um</text><text start="48.96" dur="7.119">i am on a lot of platforms as rachmas so</text><text start="52.239" dur="3.84">feel free to follow me on twitter and</text><text start="56.84" dur="4.199">linkedin we have a code of conduct</text><text start="59.359" dur="3.36">we&amp;#39;re dedicated to providing harassment</text><text start="61.039" dur="3.52">free experience for everyone</text><text start="62.719" dur="3.521">um thank you for helping to make this a</text><text start="64.559" dur="2.88">welcoming friendly professional</text><text start="66.24" dur="3.68">community for all</text><text start="67.439" dur="3.04">and this code of conduct applies to the</text><text start="69.92" dur="3.92">chat</text><text start="70.479" dur="3.361">as well</text><text start="74.479" dur="3.201">so our mission is to provide an</text><text start="76" dur="3.759">inclusive community for underrepresented</text><text start="77.68" dur="5.759">persons in data science and we are an</text><text start="79.759" dur="5.841">all volunteer run organization</text><text start="83.439" dur="3.921">you can support data umbrella by doing</text><text start="85.6" dur="3.04">the following things you can follow our</text><text start="87.36" dur="4.24">code of conduct and</text><text start="88.64" dur="5.439">keep our community a place where</text><text start="91.6" dur="4.879">everybody wants to keep coming to</text><text start="94.079" dur="4.561">you can donate to our open collective</text><text start="96.479" dur="4.481">and that helps to pay meet-up dues and</text><text start="98.64" dur="4.96">other operational costs</text><text start="100.96" dur="3.519">and you can check out this link here on</text><text start="103.6" dur="2.879">github</text><text start="104.479" dur="5.92">we have this new initiative where all</text><text start="106.479" dur="5.68">the videos are being transcribed and so</text><text start="110.399" dur="4.08">is to make them more accessible so we</text><text start="112.159" dur="4.161">take the youtube videos and we put the</text><text start="114.479" dur="4.24">raw there and so we&amp;#39;ve had a number of</text><text start="116.32" dur="5.759">volunteers help us transcribe it</text><text start="118.719" dur="6.4">so feel free to check out this link</text><text start="122.079" dur="5.121">and maybe if you do this video</text><text start="125.119" dur="4.081">maybe the two speakers will follow you</text><text start="127.2" dur="4.72">on twitter i can&amp;#39;t promise anything but</text><text start="129.2" dur="2.72">it&amp;#39;s possible</text><text start="132.959" dur="5.841">um dave umbrella has a job board and</text><text start="135.599" dur="7.121">it&amp;#39;s at jobs.org and once this gets</text><text start="138.8" dur="6">started i&amp;#39;ll put some links in the chat</text><text start="142.72" dur="4.32">the job that we are highlighting today</text><text start="144.8" dur="2.56">is is the machine learning engineer job</text><text start="147.04" dur="3.44">by</text><text start="147.36" dur="5.28">development seed and development seat is</text><text start="150.48" dur="3.039">based in washington dc and lisbon</text><text start="152.64" dur="4.319">portugal</text><text start="153.519" dur="4.321">and they do i&amp;#39;m going to go to the next</text><text start="156.959" dur="3.121">slide</text><text start="157.84" dur="4.24">what they do is they&amp;#39;re doing social</text><text start="160.08" dur="3.6">good work and so they&amp;#39;re doing</text><text start="162.08" dur="4.08">for instance mapping elections from</text><text start="163.68" dur="4.639">afghanistan to the u.s analyzing public</text><text start="166.16" dur="3.28">health and economic data from palestine</text><text start="168.319" dur="3.041">to illinois</text><text start="169.44" dur="3.68">and leading the strategy and development</text><text start="171.36" dur="3.92">behind data world bank</text><text start="173.12" dur="4.96">and some other organizations and i will</text><text start="175.28" dur="4.48">share a link to their job posting in the</text><text start="178.08" dur="4.76">chat as well</text><text start="179.76" dur="4.72">as soon as i finish this brief</text><text start="182.84" dur="3.64">introduction</text><text start="184.48" dur="3.759">check out our website for resources</text><text start="186.48" dur="5.119">there&amp;#39;s a lot of resources on learning</text><text start="188.239" dur="4.881">python um and r also for contributing to</text><text start="191.599" dur="3.601">open source</text><text start="193.12" dur="3.199">also for guides on accessibility and</text><text start="195.2" dur="4.56">responsibility</text><text start="196.319" dur="5.361">and allyship</text><text start="199.76" dur="3.68">we have a monthly newsletter that goes</text><text start="201.68" dur="2.72">out towards the end of the month and it</text><text start="203.44" dur="2.879">has information</text><text start="204.4" dur="3.36">on our upcoming events we have two great</text><text start="206.319" dur="5.441">events coming up in</text><text start="207.76" dur="7.52">november and december on open source so</text><text start="211.76" dur="5.44">subscribe to our newsletter um to be in</text><text start="215.28" dur="4.16">the know</text><text start="217.2" dur="3.36">we are on all social media platforms as</text><text start="219.44" dur="3.519">data umbrella</text><text start="220.56" dur="4.319">um meetup is the best place to join to</text><text start="222.959" dur="4.321">find out about upcoming events</text><text start="224.879" dur="3.041">our website has resources follow us on</text><text start="227.28" dur="3.2">twitter</text><text start="227.92" dur="3.599">we also share a lot of information on</text><text start="230.48" dur="2.399">linkedin</text><text start="231.519" dur="3.36">and if you want to subscribe to our</text><text start="232.879" dur="3.521">youtube channel we record all of our</text><text start="234.879" dur="4.961">talks and post them there</text><text start="236.4" dur="6.72">um within about a week of the talk so</text><text start="239.84" dur="6.319">it&amp;#39;s a good way to get information</text><text start="243.12" dur="3.36">okay and now we are ready to get started</text><text start="246.159" dur="3.28">so</text><text start="246.48" dur="5.6">i will hand it over to um put myself on</text><text start="249.439" dur="4.641">mute and i will hand it over to</text><text start="252.08" dur="3.2">hugo and james and let you take over but</text><text start="254.08" dur="2.32">thank you all for joining i just want to</text><text start="255.28" dur="4.72">thank reishima um</text><text start="256.4" dur="5.359">christina and and everyone else who tied</text><text start="260" dur="2.56">all the tireless effort that that goes</text><text start="261.759" dur="3.041">into putting</text><text start="262.56" dur="3.84">um these meet-ups and these online</text><text start="264.8" dur="4.24">sessions together</text><text start="266.4" dur="3.44">i i think um one thing i want to say is</text><text start="269.04" dur="3.52">actually the</text><text start="269.84" dur="4.32">the last in-person workshop i gave</text><text start="272.56" dur="2.4">either at the end of february or early</text><text start="274.16" dur="4.24">march</text><text start="274.96" dur="5.44">um was data umbrellas in inaugural</text><text start="278.4" dur="3.76">tutorial and meetup if i if i recall</text><text start="280.4" dur="4.16">correctly on on bayesian bayesian</text><text start="282.16" dur="4">thinking and um hacker statistics and</text><text start="284.56" dur="3.919">simulation and that type of stuff</text><text start="286.16" dur="3.599">so it&amp;#39;s it&amp;#39;s just wonderful to be back</text><text start="288.479" dur="2.72">particularly with um</text><text start="289.759" dur="2.961">my colleague and friend friend james</text><text start="291.199" dur="2.401">we&amp;#39;re building really cool um</text><text start="292.72" dur="3.919">distributed</text><text start="293.6" dur="4.319">uh data science products um at coiled</text><text start="296.639" dur="3.601">we&amp;#39;ll say a bit about that</text><text start="297.919" dur="4">um but we&amp;#39;ll do some introductions in in</text><text start="300.24" dur="4.64">a bit i just wanted to</text><text start="301.919" dur="4.321">um get you all accustomed to it was</text><text start="304.88" dur="4.879">february thank you reishma</text><text start="306.24" dur="4.16">um uh we&amp;#39;re working um with uh jupiter</text><text start="309.759" dur="3.681">notebooks</text><text start="310.4" dur="5.68">in a github repository the repository is</text><text start="313.44" dur="3.039">pinned uh to the top of the chat this is</text><text start="316.08" dur="1.6">um</text><text start="316.479" dur="3.361">what it looks like these are all the</text><text start="317.68" dur="5.28">files this is the file system</text><text start="319.84" dur="6.56">now we use something called binder</text><text start="322.96" dur="5.76">which is a project um</text><text start="326.4" dur="4.72">out of and related to project project</text><text start="328.72" dur="5.759">jupiter which provides infrastructure</text><text start="331.12" dur="5.04">to run um notebooks without any local</text><text start="334.479" dur="3.121">installs so there are two ways</text><text start="336.16" dur="3.28">you can you can code along on this</text><text start="337.6" dur="2.56">tutorial the first is and i won&amp;#39;t get</text><text start="339.44" dur="3.52">you to do this</text><text start="340.16" dur="4.08">yet um is to launch binder the reason i</text><text start="342.96" dur="2.88">won&amp;#39;t get you to do that yet is because</text><text start="344.24" dur="3.6">once you launch it we have 10 minutes to</text><text start="345.84" dur="3.04">start coding or the binder session times</text><text start="347.84" dur="4.16">out i&amp;#39;ve been burnt</text><text start="348.88" dur="5.12">by that before um actually several times</text><text start="352" dur="3.759">i&amp;#39;m surprised i even remembered it this</text><text start="354" dur="3.68">time the other thing you can do</text><text start="355.759" dur="3.841">is install everything locally by cloning</text><text start="357.68" dur="2.56">the repository downloading anaconda</text><text start="359.6" dur="2.719">creating</text><text start="360.24" dur="3.28">a condor environment um if you haven&amp;#39;t</text><text start="362.319" dur="4.241">done that um</text><text start="363.52" dur="6.399">i suggest you do not do that now and you</text><text start="366.56" dur="6.8">launch launch the binder um</text><text start="369.919" dur="4.161">james is going to start by telling us a</text><text start="373.36" dur="2.48">few</text><text start="374.08" dur="3.6">a few things about about gas and</text><text start="375.84" dur="3.919">distributed compute in general</text><text start="377.68" dur="4">my question for you james is if we get</text><text start="379.759" dur="5.44">people to launch this now</text><text start="381.68" dur="7.04">will we get to execute a cell</text><text start="385.199" dur="5.361">code cell in 10 minutes um i would let&amp;#39;s</text><text start="388.72" dur="3.759">hold off for now maybe</text><text start="390.56" dur="3.68">yep maybe i&amp;#39;ll indicate when we should</text><text start="392.479" dur="4.401">uh launch binder okay</text><text start="394.24" dur="3.519">fantastic cool um and just what i&amp;#39;m</text><text start="396.88" dur="3.28">looking at right now</text><text start="397.759" dur="3.041">is the github repository on your browser</text><text start="400.16" dur="3.2">okay</text><text start="400.8" dur="4.16">exactly so i will not launch binder now</text><text start="403.36" dur="4.64">i will not get you to now</text><text start="404.96" dur="5.92">i&amp;#39;ve i&amp;#39;m doing this locally um and we</text><text start="408" dur="4.16">see that i&amp;#39;m in uh notebook zero and if</text><text start="410.88" dur="2.879">you want to actually have a look at this</text><text start="412.16" dur="4.72">notebook before launching binder</text><text start="413.759" dur="5.361">it&amp;#39;s in the notebooks data umbrella uh</text><text start="416.88" dur="4.08">subdirectory and its notebook zero and</text><text start="419.12" dur="4.079">we&amp;#39;re going to hopefully make it through</text><text start="420.96" dur="3.44">the overview then chatting about dusk</text><text start="423.199" dur="4">dusk delayed</text><text start="424.4" dur="3.76">um and and data framing and machine</text><text start="427.199" dur="4.4">learning</text><text start="428.16" dur="5.759">um great so we have uh hashim has said</text><text start="431.599" dur="3.681">you could open in vs code as well</text><text start="433.919" dur="3.521">you could i mean that would require all</text><text start="435.28" dur="3.039">your local installs and that that type</text><text start="437.44" dur="4.08">of stuff</text><text start="438.319" dur="6.401">as well um but uh we&amp;#39;re to introduce</text><text start="441.52" dur="3.679">uh me and james um we we work at coiled</text><text start="444.72" dur="3.12">um</text><text start="445.199" dur="4.641">where we uh build products for</text><text start="447.84" dur="3.359">distributed compute in infrastructure as</text><text start="449.84" dur="2.96">we&amp;#39;ll see one of the big problems with</text><text start="451.199" dur="2.081">like bursting to the cloud is all the</text><text start="452.8" dur="3.28">like</text><text start="453.28" dur="4.8">kubernetes aws docker stuff so we build</text><text start="456.08" dur="3.119">a one-click host of deployments for das</text><text start="458.08" dur="2.16">but for data science and machine</text><text start="459.199" dur="4.241">learning</text><text start="460.24" dur="5.28">in general um james maintains task um</text><text start="463.44" dur="3.52">along with matt matt rocklin um who</text><text start="465.52" dur="3.04">created dusk uh</text><text start="466.96" dur="3.84">with a team people who was working with</text><text start="468.56" dur="5.28">continuum anaconda at the time</text><text start="470.8" dur="3.36">um and uh james is a software engineer</text><text start="473.84" dur="4.479">at</text><text start="474.16" dur="5.599">called and i run data science evangelism</text><text start="478.319" dur="3.361">marketing work on a bunch of product</text><text start="479.759" dur="4.481">product stuff as well um</text><text start="481.68" dur="3.519">wear a bunch of different different hats</text><text start="484.24" dur="3.6">occasionally</text><text start="485.199" dur="4.56">um there are many ways to think about uh</text><text start="487.84" dur="2.799">distributed compute and how to do it in</text><text start="489.759" dur="4.481">in python</text><text start="490.639" dur="6.321">we&amp;#39;re going to present um hey james</text><text start="494.24" dur="2.72">you&amp;#39;re muted</text><text start="500.24" dur="3.92">i&amp;#39;m taking it i went away based on what</text><text start="502" dur="3.759">i see in the chat you did you did but</text><text start="504.16" dur="3.439">now we&amp;#39;re back i&amp;#39;ve introduced you</text><text start="505.759" dur="3.361">i&amp;#39;ve introduced me i&amp;#39;ve mentioned that</text><text start="507.599" dur="2.241">there are many ways to do distributed</text><text start="509.12" dur="3.039">compute</text><text start="509.84" dur="3.28">um in the python ecosystem and we&amp;#39;ll be</text><text start="512.159" dur="3.841">chatting about one</text><text start="513.12" dur="4.399">called dask um and maybe i&amp;#39;ll pass you</text><text start="516" dur="2.719">in a second but i&amp;#39;ll say one thing that</text><text start="517.519" dur="2.561">i really like about</text><text start="518.719" dur="3.12">my background isn&amp;#39;t in distributed</text><text start="520.08" dur="4.4">compute my background&amp;#39;s in</text><text start="521.839" dur="4.321">pythonic data science um when thinking</text><text start="524.48" dur="2.56">about bursting to larger data sets and</text><text start="526.16" dur="3.04">larger models</text><text start="527.04" dur="3.32">there are a variety of options the thing</text><text start="529.2" dur="4.639">that took me</text><text start="530.36" dur="5.08">attracted me to desk uh originally</text><text start="533.839" dur="2.721">i saw cameron&amp;#39;s note the ghost in the</text><text start="535.44" dur="2.48">machine aren&amp;#39;t playing nice tonight i</text><text start="536.56" dur="5.76">think that ain&amp;#39;t that the truth</text><text start="537.92" dur="6.96">um is that dark plays so nicely</text><text start="542.32" dur="3.28">with the entire pi data ecosystem so as</text><text start="544.88" dur="2.88">we&amp;#39;ll see</text><text start="545.6" dur="4.4">if you want to write dash code for data</text><text start="547.76" dur="4.72">frames dash data frames it really mimics</text><text start="550" dur="3.44">your pandas code um same with numpy same</text><text start="552.48" dur="3.68">with scikit-learn</text><text start="553.44" dur="3.519">okay and the other thing is dark</text><text start="556.16" dur="2.48">essentially</text><text start="556.959" dur="3.281">runs the python code under the hood so</text><text start="558.64" dur="4">your mental model of what&amp;#39;s happening</text><text start="560.24" dur="3.2">is actually corresponds to the code um</text><text start="562.64" dur="4.639">being</text><text start="563.44" dur="5.92">um being executed okay um</text><text start="567.279" dur="4.081">now i&amp;#39;d like to pass over to james but</text><text start="569.36" dur="5.039">it looks like he&amp;#39;s disappeared</text><text start="571.36" dur="4.64">again i&amp;#39;m still here if you can hear me</text><text start="574.399" dur="3.521">i&amp;#39;ve just turned my camera off</text><text start="576" dur="3.839">oh yeah okay great i&amp;#39;m gonna turn my</text><text start="577.92" dur="3.52">camera hopefully that will help</text><text start="579.839" dur="3.44">yeah and i might do do the same for</text><text start="581.44" dur="4.88">bandwidth bandwidth issues so</text><text start="583.279" dur="5.281">if if you want to jump in and and</text><text start="586.32" dur="3.44">talk about dark at a high level um i&amp;#39;m</text><text start="588.56" dur="2.88">sharing my screen and we can scroll</text><text start="589.76" dur="5.12">through</text><text start="591.44" dur="3.68">yeah that sounds great so um that&amp;#39;s sort</text><text start="594.88" dur="2.56">of</text><text start="595.12" dur="4.08">uh a nutshell you can think of it as</text><text start="597.44" dur="4.959">being composed of</text><text start="599.2" dur="6.16">two main uh uh well components</text><text start="602.399" dur="3.761">the first we call collections these are</text><text start="605.36" dur="4.96">the</text><text start="606.16" dur="6">user interfaces that you use to actually</text><text start="610.32" dur="4.4">construct a computation you would like</text><text start="612.16" dur="3.76">to compute in parallel or on distributed</text><text start="614.72" dur="2.64">hardware</text><text start="615.92" dur="3.52">there are a few different interfaces</text><text start="617.36" dur="3.28">that desk implements uh for instance</text><text start="619.44" dur="4.32">there&amp;#39;s dask array</text><text start="620.64" dur="5.199">for doing nd array computations there&amp;#39;s</text><text start="623.76" dur="2.639">das data frame for working with tabular</text><text start="625.839" dur="2.56">data</text><text start="626.399" dur="4.56">you can think of those as like gask</text><text start="628.399" dur="4.801">array as a parallel version of numpy</text><text start="630.959" dur="3.841">das data frame has a parallel version of</text><text start="633.2" dur="3.92">pandas and so on</text><text start="634.8" dur="4">there are also a couple other interfaces</text><text start="637.12" dur="3.279">that uh we&amp;#39;ll be talking about das</text><text start="638.8" dur="2.719">delayed for instance we&amp;#39;ll talk about</text><text start="640.399" dur="2.321">that today we&amp;#39;ll also talk about the</text><text start="641.519" dur="3.121">futures api</text><text start="642.72" dur="4.48">those are sort of for lower level uh</text><text start="644.64" dur="5.12">custom algorithms</text><text start="647.2" dur="3.92">in sort of paralyzing existing uh</text><text start="649.76" dur="2.72">existing code</text><text start="651.12" dur="3.839">the main takeaway is that there are</text><text start="652.48" dur="4.4">several sort of familiar apis that desk</text><text start="654.959" dur="4.88">implements and that will use today</text><text start="656.88" dur="3.84">to actually construct your computation</text><text start="659.839" dur="2.481">so that&amp;#39;s the first</text><text start="660.72" dur="3.52">part of desk it is these dash</text><text start="662.32" dur="3.12">collections you then take these</text><text start="664.24" dur="3.44">collections</text><text start="665.44" dur="3.28">uh uh set up your steps for your</text><text start="667.68" dur="3.44">computation</text><text start="668.72" dur="3.52">and then pass them off to uh the second</text><text start="671.12" dur="4.24">component which are</text><text start="672.24" dur="4.08">desk schedulers and these will actually</text><text start="675.36" dur="3.36">go through and</text><text start="676.32" dur="3.92">execute your computation potentially in</text><text start="678.72" dur="3.679">parallel</text><text start="680.24" dur="3.44">there are two flavors of schedulers that</text><text start="682.399" dur="3.601">desk offers the first</text><text start="683.68" dur="3.279">is a are called single machine</text><text start="686" dur="2.48">schedulers</text><text start="686.959" dur="3.281">and these just take advantage of your</text><text start="688.48" dur="5.919">local hardware they will</text><text start="690.24" dur="5.76">spin up a a local thread or process pool</text><text start="694.399" dur="3.44">and start submitting tasks in your</text><text start="696" dur="3.12">computation to to be executed in</text><text start="697.839" dur="2.721">parallel</text><text start="699.12" dur="3.839">either on multiple threads or multiple</text><text start="700.56" dur="3.12">processes there&amp;#39;s also a distributed</text><text start="702.959" dur="3.601">scheduler</text><text start="703.68" dur="4.08">or maybe a better term for would</text><text start="706.56" dur="2.88">actually be called the advanced</text><text start="707.76" dur="2.56">scheduler because it works well on a</text><text start="709.44" dur="4.079">single machine</text><text start="710.32" dur="4.8">but it also scales out to uh multiple</text><text start="713.519" dur="2.721">machines so for instance as you&amp;#39;ll see</text><text start="715.12" dur="5.04">later we will actually</text><text start="716.24" dur="6.96">spin up a uh distributed scheduler that</text><text start="720.16" dur="4.88">has workers on uh remote</text><text start="723.2" dur="3.36">machines on aws so you can actually</text><text start="725.04" dur="3.799">scale out beyond your</text><text start="726.56" dur="3.76">local resources like say what&amp;#39;s on your</text><text start="728.839" dur="4.12">laptop</text><text start="730.32" dur="3.68">um kind of scrolling down then to the</text><text start="732.959" dur="3.601">image of the</text><text start="734" dur="3.6">uh cluster we can see the main</text><text start="736.56" dur="3.36">components of</text><text start="737.6" dur="3.919">the distributed scheduler and james i</text><text start="739.92" dur="2">might get people to spin up the binder</text><text start="741.519" dur="2.401">now</text><text start="741.92" dur="3.2">because we&amp;#39;re going to execute codes now</text><text start="743.92" dur="3.12">is a good point yep</text><text start="745.12" dur="4.159">so just here&amp;#39;s a quick break point</text><text start="747.04" dur="4.56">before you know a teaser for</text><text start="749.279" dur="3.601">um schedulers and what&amp;#39;s happening there</text><text start="751.6" dur="5.44">i&amp;#39;ll ask you</text><text start="752.88" dur="5.519">to um in the repository there&amp;#39;s also the</text><text start="757.04" dur="3.12">link to the binder</text><text start="758.399" dur="3.201">click on launch binder i&amp;#39;m going to open</text><text start="760.16" dur="4.16">it in a new tab and</text><text start="761.6" dur="4.479">what this will create is an environment</text><text start="764.32" dur="3.68">in which you can just execute the code</text><text start="766.079" dur="3.76">in in the notebooks okay</text><text start="768" dur="3.2">so hopefully by the time we&amp;#39;ve gotten</text><text start="769.839" dur="2.721">gone through this section</text><text start="771.2" dur="3.28">this will be ready to start executing</text><text start="772.56" dur="3.279">code so if everyone wants to do that to</text><text start="774.48" dur="2.4">code along otherwise just</text><text start="775.839" dur="4.081">watch or if you&amp;#39;re running things</text><text start="776.88" dur="5.759">locally also cool thanks james</text><text start="779.92" dur="4.719">yeah yeah no problem thank you so so</text><text start="782.639" dur="4.481">yeah looking at the image</text><text start="784.639" dur="3.681">for the distributed scheduler we&amp;#39;re not</text><text start="787.12" dur="3.36">gonna have time to go into</text><text start="788.32" dur="3.519">the um a lot of detail about the</text><text start="790.48" dur="2.72">distributed scheduler in this workshop</text><text start="791.839" dur="3.281">so but we do want to provide at least a</text><text start="793.2" dur="3.52">high level overview of the</text><text start="795.12" dur="3.12">the different parts and components of</text><text start="796.72" dur="3.52">the distributed scheduler</text><text start="798.24" dur="3.599">um so the first part i want to talk</text><text start="800.24" dur="3.279">about is in the diagram what&amp;#39;s labeled</text><text start="801.839" dur="4.081">as a client</text><text start="803.519" dur="3.44">so this is the user facing entry point</text><text start="805.92" dur="3.599">to a cluster</text><text start="806.959" dur="3.521">so um wherever you are running your</text><text start="809.519" dur="3.601">python session</text><text start="810.48" dur="3.599">that could be in a jupiter lab session</text><text start="813.12" dur="2.88">like we are here</text><text start="814.079" dur="3.76">that could be in a python script</text><text start="816" dur="4.16">somewhere you will create</text><text start="817.839" dur="3.281">and instantiate a client object that</text><text start="820.16" dur="3.6">connects</text><text start="821.12" dur="3.6">to the second component which is the das</text><text start="823.76" dur="4.24">scheduler</text><text start="824.72" dur="6.32">so each desk cluster has</text><text start="828" dur="5.44">a single scheduler in it that sort of uh</text><text start="831.04" dur="3.52">keeps track of all of the state for all</text><text start="833.44" dur="2.24">of the</text><text start="834.56" dur="3.36">the state of your cluster and all the</text><text start="835.68" dur="4.24">tasks you&amp;#39;d like to compute so from your</text><text start="837.92" dur="3.599">client you might start submitting tasks</text><text start="839.92" dur="2.8">to the cluster the schedule will receive</text><text start="841.519" dur="2.961">those tasks</text><text start="842.72" dur="4.239">and compute things like all the</text><text start="844.48" dur="5.039">dependencies needed for that task like</text><text start="846.959" dur="3.44">say you&amp;#39;re uh implementing you say you</text><text start="849.519" dur="3.841">want to compute</text><text start="850.399" dur="4.24">task c but that actually requires first</text><text start="853.36" dur="2.719">you have to compute task b</text><text start="854.639" dur="3.041">and task a like there are some</text><text start="856.079" dur="3.44">dependency structures there</text><text start="857.68" dur="3.839">it&amp;#39;ll compute those dependencies as well</text><text start="859.519" dur="4.721">as keep track of them</text><text start="861.519" dur="3.841">it&amp;#39;ll also uh communicate with all the</text><text start="864.24" dur="4.08">workers to understand</text><text start="865.36" dur="3.44">what worker is working on which task and</text><text start="868.32" dur="2.079">as</text><text start="868.8" dur="3.279">space frees up on the workers it will</text><text start="870.399" dur="4.641">start farming out uh</text><text start="872.079" dur="3.601">new tasks to compute to the workers um</text><text start="875.04" dur="2.32">so</text><text start="875.68" dur="4.399">in this particular diagram there are</text><text start="877.36" dur="4.56">three das distributed workers here</text><text start="880.079" dur="3.841">um however you can have as you can have</text><text start="881.92" dur="3.44">thousands of workers if you&amp;#39;d like</text><text start="883.92" dur="3.279">so the workers are the things that</text><text start="885.36" dur="3.76">actually compute the tasks</text><text start="887.199" dur="3.521">they also store the results of your</text><text start="889.12" dur="3.12">tasks and then serve them back to you</text><text start="890.72" dur="3.52">and the client</text><text start="892.24" dur="4.88">the scheduler basically manages all the</text><text start="894.24" dur="2.88">state needed to</text><text start="897.6" dur="3.28">perform the computations um and you</text><text start="899.839" dur="3.841">submit tasks</text><text start="900.88" dur="3.84">from the client so that&amp;#39;s sort of a</text><text start="903.68" dur="2.32">quick</text><text start="904.72" dur="3.28">whirlwind tour of the different</text><text start="906" dur="4">components for the distributed scheduler</text><text start="908" dur="3.279">um and at this point i think it&amp;#39;d be</text><text start="910" dur="3.6">great to actually see</text><text start="911.279" dur="3.601">see some of this in action um hugo would</text><text start="913.6" dur="2.88">like to take over</text><text start="914.88" dur="3.519">absolutely thank you for that wonderful</text><text start="916.48" dur="4.08">introduction to darsk and</text><text start="918.399" dur="4">and the schedulers in particular and we</text><text start="920.56" dur="2.56">are going to see that um with dark in</text><text start="922.399" dur="4.24">action</text><text start="923.12" dur="7.44">uh i&amp;#39;ll just note that this tab in which</text><text start="926.639" dur="5.76">i launched the binder is up and running</text><text start="930.56" dur="3.12">if you&amp;#39;re going to execute code here</text><text start="932.399" dur="4.56">click on notebooks</text><text start="933.68" dur="6.399">click on data umbrella oop</text><text start="936.959" dur="5.041">and then go to the overview notebook</text><text start="940.079" dur="4.401">and you can drag around we&amp;#39;ll see the</text><text start="942" dur="3.92">utility of these these dashboards</text><text start="944.48" dur="3.68">in a second but you can you know drag</text><text start="945.92" dur="4.719">your stuff around to</text><text start="948.16" dur="3.599">to make you know however you want to</text><text start="950.639" dur="2.161">want to structure it and then you can</text><text start="951.759" dur="2.961">execute code</text><text start="952.8" dur="3.279">in here i&amp;#39;m not going to do that i&amp;#39;m</text><text start="954.72" dur="3.84">going to do this</text><text start="956.079" dur="3.76">locally at the moment but just to see</text><text start="958.56" dur="4.16">dust in action</text><text start="959.839" dur="4.161">to begin with i&amp;#39;m going to i&amp;#39;m actually</text><text start="962.72" dur="4.4">going to</text><text start="964" dur="5.04">restart kernel and clear my outputs</text><text start="967.12" dur="3.04">um so i&amp;#39;m going to import uh from dash</text><text start="969.04" dur="2.719">distributed the client</text><text start="970.16" dur="4.08">the sorry the other thing i wanted to</text><text start="971.759" dur="3.361">mention is um we made a decision around</text><text start="974.24" dur="3.12">content for this</text><text start="975.12" dur="4.079">we do have a notebook that we we love to</text><text start="977.36" dur="3.279">teach on schedulers but we decided to</text><text start="979.199" dur="3.361">switch it out for machine learning for</text><text start="980.639" dur="3.921">this workshop in particular we are</text><text start="982.56" dur="4.16">teaching a similar although distinct</text><text start="984.56" dur="3.519">workshop um at pi data global</text><text start="986.72" dur="3.52">so we may see some of you there in which</text><text start="988.079" dur="4.801">we&amp;#39;ll be going um more in depth</text><text start="990.24" dur="3.44">into schedulers as well um so if you</text><text start="992.88" dur="4">want to check that out</text><text start="993.68" dur="3.76">definitely do so we instantiate the</text><text start="996.88" dur="2.639">client</text><text start="997.44" dur="4.56">which as james mentioned is kind of what</text><text start="999.519" dur="3.281">we work with as the user um to submit</text><text start="1002" dur="4.8">our code</text><text start="1002.8" dur="6.24">um so that will take take a few seconds</text><text start="1006.8" dur="3.279">um okay it&amp;#39;s got a port in you so it&amp;#39;s</text><text start="1009.04" dur="4">going going elsewhere</text><text start="1010.079" dur="4">what i&amp;#39;ll just um first get you to</text><text start="1013.04" dur="3.2">notice is that it</text><text start="1014.079" dur="3.76">tells us where our dashboard is um and</text><text start="1016.24" dur="2.959">we&amp;#39;ll see those tools in a second</text><text start="1017.839" dur="3.12">tells us about our cluster that we have</text><text start="1019.199" dur="5.041">four workers eight cores</text><text start="1020.959" dur="5.681">um between eight and nine gigs of of ram</text><text start="1024.24" dur="4.719">okay um now this is something i really</text><text start="1026.64" dur="4.64">love about dusk all the diagnostic um</text><text start="1028.959" dur="3.041">tools if i click on the little desk</text><text start="1031.28" dur="3.12">thing here</text><text start="1032" dur="4">and we&amp;#39;ve um modified the binder so that</text><text start="1034.4" dur="5.12">that exists there as well</text><text start="1036" dur="5.6">um we can see i&amp;#39;ll hit search and it</text><text start="1039.52" dur="4.319">should that now corresponds to</text><text start="1041.6" dur="3.04">the the scheduler now i want to look at</text><text start="1043.839" dur="3.84">the task</text><text start="1044.64" dur="5.36">stream which will tell us in real time</text><text start="1047.679" dur="3.12">what&amp;#39;s happening i also want to look at</text><text start="1050" dur="4.16">the</text><text start="1050.799" dur="4.401">cluster map so we see um here this is</text><text start="1054.16" dur="3.92">already really cool</text><text start="1055.2" dur="4.56">um we&amp;#39;ve got uh all of our workers</text><text start="1058.08" dur="4.64">around here and our scheduler</text><text start="1059.76" dur="4.96">scheduler there and when we start um</text><text start="1062.72" dur="4.72">doing some compute we&amp;#39;ll actually see</text><text start="1064.72" dur="4.56">information flowing between these um and</text><text start="1067.44" dur="5.359">the other thing</text><text start="1069.28" dur="6.8">maybe i&amp;#39;ll yeah i&amp;#39;ll</text><text start="1072.799" dur="7.76">include a little progress um</text><text start="1076.08" dur="7.68">and that can be an alternate tab to um</text><text start="1080.559" dur="6.161">ask um i&amp;#39;m wondering</text><text start="1083.76" dur="4">perhaps i also want to include something</text><text start="1086.72" dur="3.76">about the workers</text><text start="1087.76" dur="3.279">yeah okay great so we&amp;#39;ve got a bunch of</text><text start="1090.48" dur="3.199">stuff</text><text start="1091.039" dur="3.601">that&amp;#39;s that&amp;#39;s pretty interesting there</text><text start="1093.679" dur="2.481">and so</text><text start="1094.64" dur="3.76">the next thing i&amp;#39;m going to do we&amp;#39;ve got</text><text start="1096.16" dur="3.2">a little utility file which um downloads</text><text start="1098.4" dur="3.76">some of the data</text><text start="1099.36" dur="5.199">and this is what it does is if you&amp;#39;re in</text><text start="1102.16" dur="5.36">binder it downloads a subset of the data</text><text start="1104.559" dur="3.921">if you&amp;#39;re anywhere else it loads a</text><text start="1107.52" dur="3.84">larger set</text><text start="1108.48" dur="4.24">um for this particular example we&amp;#39;re</text><text start="1111.36" dur="2.96">dealing with a small data set</text><text start="1112.72" dur="3.28">you see the utility of dark and</text><text start="1114.32" dur="2.88">distributed compute when it generalizes</text><text start="1116" dur="3.28">to larger data sets</text><text start="1117.2" dur="3.599">but for pedagogical purposes um we&amp;#39;re</text><text start="1119.28" dur="2.8">going to sit with a smaller data set so</text><text start="1120.799" dur="3.281">that we can actually run</text><text start="1122.08" dur="3.44">run the code there&amp;#39;s a trade-off there</text><text start="1124.08" dur="4.719">um so</text><text start="1125.52" dur="4.32">actually that was already downloaded it</text><text start="1128.799" dur="2.801">seems but you should</text><text start="1129.84" dur="3.36">all see it download i&amp;#39;m actually going</text><text start="1131.6" dur="4.48">to run that in the binder</text><text start="1133.2" dur="5.28">just to you should start seeing</text><text start="1136.08" dur="5.44">downloading nyc flights data set</text><text start="1138.48" dur="6.4">done extracting creating json data etc</text><text start="1141.52" dur="7.2">okay now what we&amp;#39;re going to do</text><text start="1144.88" dur="5.12">is we&amp;#39;re going to read in this data as a</text><text start="1148.72" dur="3.199">dask data frame and</text><text start="1150" dur="4.4">what i want you to notice is that it</text><text start="1151.919" dur="4.801">really the das code mimics pandas code</text><text start="1154.4" dur="3.36">so instead of pd read csv we&amp;#39;ve got dd</text><text start="1156.72" dur="3.04">read csv</text><text start="1157.76" dur="3.12">um we&amp;#39;ve got you know this is the file</text><text start="1159.76" dur="2.799">path um</text><text start="1160.88" dur="3.679">the first argument we&amp;#39;re doing some</text><text start="1162.559" dur="5.681">parse date setting some data types</text><text start="1164.559" dur="5.841">okay um we&amp;#39;ve got a little um</text><text start="1168.24" dur="3.439">wild card regular expression there to to</text><text start="1170.4" dur="4.159">join uh</text><text start="1171.679" dur="4.24">to do a bunch of them um and then we&amp;#39;re</text><text start="1174.559" dur="3.281">performing a group by</text><text start="1175.919" dur="3.441">okay so we&amp;#39;re grouping by the origin of</text><text start="1177.84" dur="3.28">these flight flight data</text><text start="1179.36" dur="3.6">we&amp;#39;re looking at the the mean departure</text><text start="1181.12" dur="3.919">delay group by origin</text><text start="1182.96" dur="3.12">the the one difference i want to make</text><text start="1185.039" dur="4.321">clear is that</text><text start="1186.08" dur="5.44">in das we need a compute method</text><text start="1189.36" dur="3.679">um that&amp;#39;s because das performs lazy</text><text start="1191.52" dur="3.279">computation it won&amp;#39;t actually</text><text start="1193.039" dur="3.52">do anything because you don&amp;#39;t want it to</text><text start="1194.799" dur="4">do anything on really large data sets</text><text start="1196.559" dur="4.961">until you explicitly tell it tell it to</text><text start="1198.799" dur="5.36">compute so i&amp;#39;m going to execute this now</text><text start="1201.52" dur="4.32">and we should see some information</text><text start="1204.159" dur="3.52">transfer between the scheduler and the</text><text start="1205.84" dur="4.959">workers and we should see tasks</text><text start="1207.679" dur="13.12">starting starting to be done okay</text><text start="1210.799" dur="12">so moment of truth</text><text start="1220.799" dur="3.601">fantastic so we call this a pew pew plot</text><text start="1222.799" dur="3.12">because we see pew pew pew</text><text start="1224.4" dur="3.519">um we saw a bunch of data transfer</text><text start="1225.919" dur="2.64">happening between them these are all our</text><text start="1227.919" dur="3.12">cause</text><text start="1228.559" dur="4.161">and we can see tasks happening um it</text><text start="1231.039" dur="2.961">tells us what tasks there are we can see</text><text start="1232.72" dur="3.92">that most of the time was spent</text><text start="1234" dur="3.6">uh reading reading csvs then we have</text><text start="1236.64" dur="3.44">some um</text><text start="1237.6" dur="3.199">group bias on chunks and and that type</text><text start="1240.08" dur="3.44">of stuff</text><text start="1240.799" dur="4.721">um so that&amp;#39;s a really nice uh diagnostic</text><text start="1243.52" dur="5.12">tool to see what most of your work</text><text start="1245.52" dur="5.2">um is is actually doing uh under dark</text><text start="1248.64" dur="3.039">work as you can see memory used cpu use</text><text start="1250.72" dur="4.4">um uh</text><text start="1251.679" dur="6.641">more fine-grained examples there um so</text><text start="1255.12" dur="7.039">i i&amp;#39;d love to know if um</text><text start="1258.32" dur="7.359">in the q a um</text><text start="1262.159" dur="7.281">i&amp;#39;m going to ask were you able to</text><text start="1265.679" dur="5.441">execute this code and if you were in</text><text start="1269.44" dur="5.76">binder just a thumb up</text><text start="1271.12" dur="7.28">a vote would be no would be fantastic</text><text start="1275.2" dur="4.64">um much appreciated um</text><text start="1278.4" dur="4.159">so as we&amp;#39;ve mentioned i just wanted to</text><text start="1279.84" dur="4.64">say a few things about tutorial goals</text><text start="1282.559" dur="3.681">um the goal is to cover the basics of</text><text start="1284.48" dur="2.72">dark and distributed compute we&amp;#39;d love</text><text start="1286.24" dur="2.319">for you to walk away with an</text><text start="1287.2" dur="3.04">understanding of when to use it when to</text><text start="1288.559" dur="3.521">not what it has to offer we&amp;#39;re going to</text><text start="1290.24" dur="2.48">be covering um the basics of dusk</text><text start="1292.08" dur="3.12">delayed</text><text start="1292.72" dur="4.319">which although not immediately um</text><text start="1295.2" dur="5.52">applicable to data science</text><text start="1297.039" dur="6.64">provides a wonderful framework um</text><text start="1300.72" dur="4.4">for thinking um about dusk how dark</text><text start="1303.679" dur="2.081">works and understanding how it works</text><text start="1305.12" dur="2.24">under the hood</text><text start="1305.76" dur="3.44">then we&amp;#39;re going to go into dark data</text><text start="1307.36" dur="4.96">frames and then machine learning</text><text start="1309.2" dur="5.44">hopefully um due to the technical um</text><text start="1312.32" dur="3.04">considerations with um we&amp;#39;ve got less</text><text start="1314.64" dur="2.96">time than</text><text start="1315.36" dur="4.64">than we thought we would but um we&amp;#39;ll</text><text start="1317.6" dur="5.84">definitely do the best we can</text><text start="1320" dur="5.2">we may have less time to do uh exercises</text><text start="1323.44" dur="3.76">so we&amp;#39;ve had two people who are able to</text><text start="1325.2" dur="4.32">execute this code</text><text start="1327.2" dur="4.08">if you if you tried to execute it in</text><text start="1329.52" dur="5.12">binder and were not able to</text><text start="1331.28" dur="6.399">perhaps post that in the q a um</text><text start="1334.64" dur="4.8">but um we also have several exercises</text><text start="1337.679" dur="3.36">um and i&amp;#39;d like you to take a minute</text><text start="1339.44" dur="3.84">just to do this exercise</text><text start="1341.039" dur="3.681">the i i&amp;#39;m not asking you to do this</text><text start="1343.28" dur="3.279">because i want to know if you&amp;#39;re able to</text><text start="1344.72" dur="2.48">print hello world i&amp;#39;m essentially asking</text><text start="1346.559" dur="2.48">you to do it</text><text start="1347.2" dur="3.12">um so you get a sense of how these</text><text start="1349.039" dur="4.64">exercises work so</text><text start="1350.32" dur="4.239">if you can take 30 seconds to print</text><text start="1353.679" dur="3.441">hello world</text><text start="1354.559" dur="3.921">um then uh we&amp;#39;ll we&amp;#39;ll move on after</text><text start="1357.12" dur="4.72">that so just take um</text><text start="1358.48" dur="3.36">30 seconds now</text><text start="1362.08" dur="2.479">and it seems like we have a few more</text><text start="1363.2" dur="12.64">people who are able to execute code</text><text start="1364.559" dur="11.281">which which was great</text><text start="1382.08" dur="3.599">okay fantastic so you will put your</text><text start="1383.679" dur="4.721">solution there for some reason i have um</text><text start="1385.679" dur="4.321">an extra cell here so i&amp;#39;m just going to</text><text start="1388.4" dur="3.759">clip that</text><text start="1390" dur="3.44">and to see a solution uh i&amp;#39;ll just get</text><text start="1392.159" dur="3.76">you to execute</text><text start="1393.44" dur="4.08">this cell and it provides the solution</text><text start="1395.919" dur="2.081">and then we can execute it and compare</text><text start="1397.52" dur="2.08">it to the</text><text start="1398" dur="4">the output of what you had okay hello</text><text start="1399.6" dur="5.76">world um</text><text start="1402" dur="5.76">so as as we saw i&amp;#39;ve done all this</text><text start="1405.36" dur="3.199">locally you may have done it on binder</text><text start="1407.76" dur="2.32">um</text><text start="1408.559" dur="4">there is an option to work directly from</text><text start="1410.08" dur="4.079">the cloud um and i&amp;#39;ll i&amp;#39;ll take you</text><text start="1412.559" dur="2.561">through this there are many ways to do</text><text start="1414.159" dur="2.64">this</text><text start="1415.12" dur="3.36">as i mentioned we&amp;#39;re working on one way</text><text start="1416.799" dur="2.481">with coil and i&amp;#39;ll explain the rationale</text><text start="1418.48" dur="3.12">behind that</text><text start="1419.28" dur="3.2">in in a second but i&amp;#39;ll show you how</text><text start="1421.6" dur="3.12">easy it is</text><text start="1422.48" dur="4.079">to get a cluster up and running on on</text><text start="1424.72" dur="5.28">aws without even interacting with</text><text start="1426.559" dur="7.441">aws for free for example you can follow</text><text start="1430" dur="5.679">along by uh signing into uh coiled cloud</text><text start="1434" dur="3.12">to be clear this is not a necessity and</text><text start="1435.679" dur="2.561">it does involve you signing up to our</text><text start="1437.12" dur="2.4">product so i just wanted to be</text><text start="1438.24" dur="3.28">absolutely transparent</text><text start="1439.52" dur="3.759">about that it does not involve any</text><text start="1441.52" dur="3.68">credit card information or anything</text><text start="1443.279" dur="3.441">along those lines and in my opinion it</text><text start="1445.2" dur="3.359">does give a really nice</text><text start="1446.72" dur="3.12">uh example of how to run stuff on the</text><text start="1448.559" dur="4.321">cloud um</text><text start="1449.84" dur="4">to do so you can sign in at cloud dot</text><text start="1452.88" dur="3.76">coiled</text><text start="1453.84" dur="4.24">uh dot io you can also pip install</text><text start="1456.64" dur="3.919">coiled and then</text><text start="1458.08" dur="4.4">do authentication you can also spin up</text><text start="1460.559" dur="5.761">this</text><text start="1462.48" dur="5.6">this hosted coiled notebook so i&amp;#39;m</text><text start="1466.32" dur="4.52">going to spin that up now and i&amp;#39;m going</text><text start="1468.08" dur="6.479">to post that</text><text start="1470.84" dur="4.52">here um actually yep i&amp;#39;m gonna post that</text><text start="1474.559" dur="6.401">in the ch</text><text start="1475.36" dur="5.6">chat um if you let me get this right</text><text start="1481.12" dur="3.28">um if you&amp;#39;ve if you&amp;#39;ve never logged in</text><text start="1482.799" dur="3.921">to code before it&amp;#39;ll ask you to sign up</text><text start="1484.4" dur="3.84">using gmail or github so feel free to do</text><text start="1486.72" dur="4.64">that if you&amp;#39;d like</text><text start="1488.24" dur="4.24">if not that&amp;#39;s also also cool um but i</text><text start="1491.36" dur="4.319">just wanted to be explicit</text><text start="1492.48" dur="3.52">uh about that um the reason i want to do</text><text start="1495.679" dur="2.961">this</text><text start="1496" dur="3.36">is to show how dars can be leveraged to</text><text start="1498.64" dur="2.48">do work on</text><text start="1499.36" dur="3.439">really large data sets so you will</text><text start="1501.12" dur="3.76">recall that i had between eight and nine</text><text start="1502.799" dur="6.081">gigs of ram on my local system</text><text start="1504.88" dur="5.52">um oh wow anthony says on ipad unable to</text><text start="1508.88" dur="4.08">execute on binder</text><text start="1510.4" dur="4.08">incredible um i don&amp;#39;t have a strong</text><text start="1512.96" dur="4.88">sense of how binder works on ipad</text><text start="1514.48" dur="5.6">i do know that i was able to um</text><text start="1517.84" dur="4.24">to check to use a binder on my iphone</text><text start="1520.08" dur="3.92">several years ago on my way to scipy</text><text start="1522.08" dur="3.199">doing code review for someone for eric</text><text start="1524" dur="4.559">maher i think for what that</text><text start="1525.279" dur="7.201">that&amp;#39;s worth um but back to this</text><text start="1528.559" dur="6.081">um we have this nyc taxi data set which</text><text start="1532.48" dur="4.88">is over 10 gigs it won&amp;#39;t even</text><text start="1534.64" dur="4.48">i can&amp;#39;t even store that in local memory</text><text start="1537.36" dur="6.16">i don&amp;#39;t have enough ram to store that</text><text start="1539.12" dur="6.96">so we do need um either to do it</text><text start="1543.52" dur="4.48">locally in an out of core mode of some</text><text start="1546.08" dur="3.12">sort or we can we can burst to the cloud</text><text start="1548" dur="1.76">and we&amp;#39;re actually going to burst to the</text><text start="1549.2" dur="4.64">cloud</text><text start="1549.76" dur="7.68">using using coiled um so the notebook</text><text start="1553.84" dur="6.24">is running here um for me and but i&amp;#39;m</text><text start="1557.44" dur="3.839">actually gonna do it uh from my local</text><text start="1560.08" dur="3.52">local notebook</text><text start="1561.279" dur="4">but you&amp;#39;ll see and once again feel free</text><text start="1563.6" dur="3.92">to code along here</text><text start="1565.279" dur="3.121">it&amp;#39;s spinning up a notebook and james</text><text start="1567.52" dur="3.759">who is</text><text start="1568.4" dur="3.68">is my co-instructor here um is to be i&amp;#39;m</text><text start="1571.279" dur="2.481">i&amp;#39;m so grateful</text><text start="1572.08" dur="3.04">all the work is done on our notebooks in</text><text start="1573.76" dur="3.6">coiled you can</text><text start="1575.12" dur="4.64">launch the cluster here and then analyze</text><text start="1577.36" dur="3.52">the entire um over 10 gigs of data there</text><text start="1579.76" dur="5.279">i&amp;#39;m going to do it um</text><text start="1580.88" dur="5.919">here so to do that i import coiled</text><text start="1585.039" dur="3.52">and then i import the dash distributed</text><text start="1586.799" dur="3.36">stuff and then</text><text start="1588.559" dur="2.961">i can create my own software environment</text><text start="1590.159" dur="2.88">cluster configuration i&amp;#39;m not going to</text><text start="1591.52" dur="4.32">do that</text><text start="1593.039" dur="4.721">because the standard coiled cluster</text><text start="1595.84" dur="3.76">configuration software environment works</text><text start="1597.76" dur="3.279">now i&amp;#39;m going to spin up a cluster and</text><text start="1599.6" dur="3.28">instantiate a client</text><text start="1601.039" dur="3.12">now because we&amp;#39;re spinning up a cluster</text><text start="1602.88" dur="3.36">uh in in the cloud</text><text start="1604.159" dur="3.281">um it&amp;#39;ll take it&amp;#39;ll take a minute a</text><text start="1606.24" dur="2.96">minute or two</text><text start="1607.44" dur="3.599">enough time to make a cup of coffee but</text><text start="1609.2" dur="5.44">it&amp;#39;s also enough time for me to just</text><text start="1611.039" dur="6.081">talk a bit about why this is important</text><text start="1614.64" dur="3.6">um and there are a lot of a lot of good</text><text start="1617.12" dur="3.039">good people working on</text><text start="1618.24" dur="3.76">on similar things um but part of the</text><text start="1620.159" dur="3.681">motivation here is that</text><text start="1622" dur="4.559">if you want to you don&amp;#39;t always want to</text><text start="1623.84" dur="5.439">do distributed data science okay um</text><text start="1626.559" dur="4.24">first i&amp;#39;d ask you to look at instead of</text><text start="1629.279" dur="2.321">using dark if you can optimize your</text><text start="1630.799" dur="2.88">pandas code</text><text start="1631.6" dur="3.28">right um second i&amp;#39;d ask if you&amp;#39;ve got</text><text start="1633.679" dur="2.721">big data sets</text><text start="1634.88" dur="3.039">it&amp;#39;s a good question do you actually</text><text start="1636.4" dur="2.96">need all the data so</text><text start="1637.919" dur="2.961">i would if you&amp;#39;re doing machine learning</text><text start="1639.36" dur="3.679">plot your learning curve see how</text><text start="1640.88" dur="3.84">accurate see how your accuracy um</text><text start="1643.039" dur="3.281">or whatever your metric of interest is</text><text start="1644.72" dur="4.319">improves as you increase</text><text start="1646.32" dur="5.04">the amount of data right um and if it</text><text start="1649.039" dur="3.12">plateaus before you get to a large data</text><text start="1651.36" dur="2.799">size then</text><text start="1652.159" dur="4">you may as well most of the time use</text><text start="1654.159" dur="4.24">your small data um</text><text start="1656.159" dur="3.681">see if sub sampling um can actually give</text><text start="1658.399" dur="3.841">you the results you need</text><text start="1659.84" dur="3.76">um so you can get a bigger bigger access</text><text start="1662.24" dur="4.08">to a bigger machine</text><text start="1663.6" dur="3.439">so you don&amp;#39;t have to burst to the cloud</text><text start="1666.32" dur="3.04">but after</text><text start="1667.039" dur="3.601">all these things if you do need to boast</text><text start="1669.36" dur="4.08">burst to the cloud</text><text start="1670.64" dur="3.519">until recently you&amp;#39;ve had to get an aws</text><text start="1673.44" dur="2.719">account</text><text start="1674.159" dur="3.841">um you&amp;#39;ve had to you know set up</text><text start="1676.159" dur="4.481">containers with docker and or</text><text start="1678" dur="3.44">kubernetes um and do all of these kind</text><text start="1680.64" dur="3.519">of</text><text start="1681.44" dur="3.04">i suppose devopsy software engineering</text><text start="1684.159" dur="3.361">foo</text><text start="1684.48" dur="3.28">stuff um which which if you&amp;#39;re into that</text><text start="1687.52" dur="2.639">i</text><text start="1687.76" dur="3.279">i absolutely encourage you encourage you</text><text start="1690.159" dur="2.4">to do that</text><text start="1691.039" dur="3.281">but a lot of working data scientists</text><text start="1692.559" dur="4.881">aren&amp;#39;t paid to do that um</text><text start="1694.32" dur="4.8">and um i don&amp;#39;t necessarily want to</text><text start="1697.44" dur="2.8">um so that&amp;#39;s something we&amp;#39;re working on</text><text start="1699.12" dur="2.96">is thinking about these kind of</text><text start="1700.24" dur="2.799">one-click hosted deployments so you</text><text start="1702.08" dur="3.92">don&amp;#39;t have to do</text><text start="1703.039" dur="4.161">all of that um having said that um i</text><text start="1706" dur="2.159">very much encourage you to try doing</text><text start="1707.2" dur="3.68">that stuff if</text><text start="1708.159" dur="3.601">if you&amp;#39;re interested um we&amp;#39;ll see that</text><text start="1710.88" dur="4.48">the</text><text start="1711.76" dur="8.88">the um cluster has just been created</text><text start="1715.36" dur="8.88">um and what i&amp;#39;m going to do we see that</text><text start="1720.64" dur="6.88">um oh i&amp;#39;m sorry</text><text start="1724.24" dur="5.2">i&amp;#39;ve done something funny here i&amp;#39;m</text><text start="1727.52" dur="4.32">i&amp;#39;m referencing the previous client anna</text><text start="1729.44" dur="2.4">james</text><text start="1733.919" dur="3.12">yeah it looks like you should go ahead</text><text start="1735.36" dur="5.039">and connect a new client to the coil</text><text start="1737.039" dur="4.961">cluster and making sure not to</text><text start="1740.399" dur="5.76">re-execute the cluster</text><text start="1742" dur="7.52">creation exactly so</text><text start="1746.159" dur="5.921">would that be how would i</text><text start="1749.52" dur="3.2">what&amp;#39;s the call here i would just open</text><text start="1752.08" dur="3.68">up a new</text><text start="1752.72" dur="6.079">cell and say client equals</text><text start="1755.76" dur="4.399">um capital client and then pass in the</text><text start="1758.799" dur="5.36">cluster</text><text start="1760.159" dur="4">like open parentheses cluster yeah</text><text start="1764.84" dur="4.92">great</text><text start="1767.76" dur="3.36">okay fantastic and what we&amp;#39;re seeing is</text><text start="1769.76" dur="2.88">a slight version this</text><text start="1771.12" dur="3.12">we don&amp;#39;t need to worry about this this</text><text start="1772.64" dur="4.08">is essentially saying that um</text><text start="1774.24" dur="4.08">the environment on the cloud mis is</text><text start="1776.72" dur="3.52">there&amp;#39;s a slight mismatch with my</text><text start="1778.32" dur="4.16">with my local environment we&amp;#39;re fine</text><text start="1780.24" dur="5.52">with that i&amp;#39;m going to</text><text start="1782.48" dur="5.36">um look here for a certain reason</text><text start="1785.76" dur="3.44">um the the dashboard isn&amp;#39;t quite working</text><text start="1787.84" dur="3.04">here at the moment james would you</text><text start="1789.2" dur="2.8">suggest i just click on this and open a</text><text start="1790.88" dur="4.24">new</text><text start="1792" dur="6.96">yeah click on the ecs uh dashboard link</text><text start="1795.12" dur="7.439">oh yes fantastic</text><text start="1798.96" dur="6.16">so um yep there&amp;#39;s some</text><text start="1802.559" dur="3.281">bug with the local dashboards that we&amp;#39;re</text><text start="1805.12" dur="3.12">we&amp;#39;re currently</text><text start="1805.84" dur="4">currently working on but what we&amp;#39;ll see</text><text start="1808.24" dur="4.559">now</text><text start="1809.84" dur="5.199">just a sec i&amp;#39;m going to remove all of</text><text start="1812.799" dur="2.24">this</text><text start="1815.52" dur="5.2">we&amp;#39;ll see now that i have access to 10</text><text start="1817.2" dur="7.839">workers i have access to 40 cores</text><text start="1820.72" dur="7.839">and i have access to uh over 170 gigs</text><text start="1825.039" dur="4.24">of memory okay so now i&amp;#39;m actually going</text><text start="1828.559" dur="3.201">to</text><text start="1829.279" dur="4.801">import this data set and it&amp;#39;s the entire</text><text start="1831.76" dur="3.84">um year of data from 2019</text><text start="1834.08" dur="4">and we&amp;#39;ll start seeing on on the</text><text start="1835.6" dur="5.76">diagnostics all the all the processing</text><text start="1838.08" dur="7.28">happening okay so oh</text><text start="1841.36" dur="5.84">actually not yet because we haven&amp;#39;t um</text><text start="1845.36" dur="3.199">called compute okay so it&amp;#39;s done this</text><text start="1847.2" dur="4.32">lazily um</text><text start="1848.559" dur="4.081">we&amp;#39;ve imported it um it shows kind of</text><text start="1851.52" dur="3.44">like pandas when you</text><text start="1852.64" dur="3.84">show a data frame um the column names</text><text start="1854.96" dur="4.16">and data types</text><text start="1856.48" dur="3.76">um but it doesn&amp;#39;t show the data because</text><text start="1859.12" dur="3.12">we haven&amp;#39;t loaded it</text><text start="1860.24" dur="3.6">yet it does tell you how many partitions</text><text start="1862.24" dur="2.08">it is so essentially and we&amp;#39;ll see this</text><text start="1863.84" dur="1.839">soon</text><text start="1864.32" dur="3.68">das data frames correspond to</text><text start="1865.679" dur="5.041">collections of pandas data frames</text><text start="1868" dur="4.96">um so they&amp;#39;re really 127 pandas data</text><text start="1870.72" dur="5.28">frames underlying this task data frame</text><text start="1872.96" dur="4.16">so now i&amp;#39;m going to do the compute well</text><text start="1876" dur="4.08">i&amp;#39;m going to</text><text start="1877.12" dur="4.72">set myself up for the computation um to</text><text start="1880.08" dur="2.8">do a group by passenger gown and look at</text><text start="1881.84" dur="2.719">the main tip</text><text start="1882.88" dur="4.24">now that took a very small amount of</text><text start="1884.559" dur="4.641">time we see the ipython magic</text><text start="1887.12" dur="6.08">timing there because we haven&amp;#39;t computed</text><text start="1889.2" dur="6.56">it now we&amp;#39;re actually going to compute</text><text start="1893.2" dur="4.16">um and james if you&amp;#39;ll see in the chat</text><text start="1895.76" dur="3.759">eliana said her coil</text><text start="1897.36" dur="3.199">coiled authentication failed i don&amp;#39;t</text><text start="1899.519" dur="3.121">know if you&amp;#39;re able to</text><text start="1900.559" dur="3.281">to help with that but if you are that</text><text start="1902.64" dur="4.32">would be great</text><text start="1903.84" dur="5.679">um and it may be difficult to debug in</text><text start="1906.96" dur="3.68">but look as we see we have the task</text><text start="1909.519" dur="3.76">stream now</text><text start="1910.64" dur="3.84">um and we see how many you know we&amp;#39;ve</text><text start="1913.279" dur="3.601">got 40 cores</text><text start="1914.48" dur="3.84">working together we saw the processing</text><text start="1916.88" dur="4.639">we saw the bytes stored</text><text start="1918.32" dur="4.479">it&amp;#39;s over 10 gigs as i said um and we</text><text start="1921.519" dur="4.88">see we were able</text><text start="1922.799" dur="6.72">to do our um</text><text start="1926.399" dur="7.681">basic analytics um</text><text start="1929.519" dur="8.4">we were able to do it on a 10 plus gig</text><text start="1934.08" dur="7.28">data set in in 21.3 seconds</text><text start="1937.919" dur="7.201">which is pretty pretty exceptional um</text><text start="1941.36" dur="5.679">if any any code based issues come up</text><text start="1945.12" dur="3.12">and they&amp;#39;re correlated in particular so</text><text start="1947.039" dur="4.321">if you have questions about the</text><text start="1948.24" dur="4.559">code execution please ask in the q a um</text><text start="1951.36" dur="3.12">not in the chat because others cannot</text><text start="1952.799" dur="2.801">vote it and i will definitively</text><text start="1954.48" dur="2.72">prioritize</text><text start="1955.6" dur="2.799">questions on technical stuff</text><text start="1957.2" dur="2.479">particularly ones that up that are</text><text start="1958.399" dur="3.041">upvoted</text><text start="1959.679" dur="3.041">um but yeah i totally agree thanks</text><text start="1961.44" dur="4.959">thanks very much</text><text start="1962.72" dur="8.64">um so yeah let&amp;#39;s jump into</text><text start="1966.399" dur="4.961">into um data frames</text><text start="1973.76" dur="4.639">so of course we write here that in the</text><text start="1975.6" dur="5.36">last exercise um we used ask delayed to</text><text start="1978.399" dur="4.16">parallelize uh loading multiple csv</text><text start="1980.96" dur="4.959">files into a pandas data frame</text><text start="1982.559" dur="4.72">um we&amp;#39;re not we we haven&amp;#39;t done that but</text><text start="1985.919" dur="2">you can definitely go through and have a</text><text start="1987.279" dur="4.4">look at that</text><text start="1987.919" dur="5.681">um but i think perhaps even</text><text start="1991.679" dur="4.081">more immediately relevant for a data</text><text start="1993.6" dur="3.6">science crowd and an analytics crowd is</text><text start="1995.76" dur="3.519">which is what i see here from the</text><text start="1997.2" dur="4">reasons people people have joined um is</text><text start="1999.279" dur="4.561">jumping into dusk data frames</text><text start="2001.2" dur="3.12">um and as i said before adas data frame</text><text start="2003.84" dur="2.959">um</text><text start="2004.32" dur="4.4">really feels like a pandas data frame um</text><text start="2006.799" dur="2.401">but internally it&amp;#39;s composed of many</text><text start="2008.72" dur="2.4">different</text><text start="2009.2" dur="3.44">different data frames this is one one</text><text start="2011.12" dur="2.96">way to think about it that we have all</text><text start="2012.64" dur="3.84">these pandas data frames</text><text start="2014.08" dur="3.199">um and the collection of them is a dark</text><text start="2016.48" dur="3.039">data frame</text><text start="2017.279" dur="3.041">and as we saw before they&amp;#39;re partitioned</text><text start="2019.519" dur="2.481">we saw</text><text start="2020.32" dur="4.719">when we loaded the taxi data set in the</text><text start="2022" dur="5.76">dash data frame was 127 partitions right</text><text start="2025.039" dur="4.24">um where each partition was a normal</text><text start="2027.76" dur="5.039">panda pandas data frame</text><text start="2029.279" dur="5.681">um and they can live on disk as they did</text><text start="2032.799" dur="3.76">early uh in the first example dark in</text><text start="2034.96" dur="3.04">action or they can live on other</text><text start="2036.559" dur="3.761">machines as when i spun up</text><text start="2038" dur="3.44">a coiled cluster and and did it on on</text><text start="2040.32" dur="4.079">aws</text><text start="2041.44" dur="4.8">um something i love about darth&amp;#39;s data</text><text start="2044.399" dur="5.28">frames i mean i ran about this</text><text start="2046.24" dur="4.72">all the time um it&amp;#39;s how it&amp;#39;s the pandas</text><text start="2049.679" dur="5.121">api and and matt</text><text start="2050.96" dur="6.879">matt rocklin actually um uh</text><text start="2054.8" dur="4.48">has a post on on the</text><text start="2057.839" dur="3.121">blog called a brief history of dusk in</text><text start="2059.28" dur="2.319">which he talks about the technical goals</text><text start="2060.96" dur="2.719">of</text><text start="2061.599" dur="4.24">us but also talks about a social goal of</text><text start="2063.679" dur="5.2">task which in matt&amp;#39;s words is to invent</text><text start="2065.839" dur="6.641">nothing he wanted and the team wanted</text><text start="2068.879" dur="6.881">um the dusk api to be as</text><text start="2072.48" dur="4.96">comfortable um and familiar for users</text><text start="2075.76" dur="3.119">as possible and that&amp;#39;s something i</text><text start="2077.44" dur="4.88">really appreciate</text><text start="2078.879" dur="6.081">about it so um we see we have element</text><text start="2082.32" dur="3.12">element uh wires on operations we have</text><text start="2084.96" dur="2.399">the</text><text start="2085.44" dur="3.84">our favorite row eyes selections we have</text><text start="2087.359" dur="3.361">loc we have the common aggregations we</text><text start="2089.28" dur="3.76">saw group buyers before we have</text><text start="2090.72" dur="4.879">is-ins we have date time string</text><text start="2093.04" dur="4.559">accessors</text><text start="2095.599" dur="3.841">um oh james we forgot to i forgot to</text><text start="2097.599" dur="3.52">edit this and i</text><text start="2099.44" dur="3.2">it should be grouped by i don&amp;#39;t know</text><text start="2101.119" dur="3.121">what what a fruit buy is but that&amp;#39;s</text><text start="2102.64" dur="3.84">something um</text><text start="2104.24" dur="3.2">we&amp;#39;ll make sure the next iteration to to</text><text start="2106.48" dur="2">get right at least we&amp;#39;ve got it right</text><text start="2107.44" dur="3.04">there and in the code</text><text start="2108.48" dur="3.68">um but have a look at the dash data</text><text start="2110.48" dur="2.24">frame api docs to check out what&amp;#39;s</text><text start="2112.16" dur="3.28">happening</text><text start="2112.72" dur="4.48">um and a lot of the time dash data</text><text start="2115.44" dur="3.2">frames can serve as drop in replacements</text><text start="2117.2" dur="2.879">for pandas data frames</text><text start="2118.64" dur="3.04">the one thing that i just want to make</text><text start="2120.079" dur="3.601">clear as i did before</text><text start="2121.68" dur="3.36">um is that you need to call compute</text><text start="2123.68" dur="6">because of the</text><text start="2125.04" dur="8.48">lazy laser compute property of das</text><text start="2129.68" dur="4.56">so this is wonderful to talk about when</text><text start="2133.52" dur="3.28">to use</text><text start="2134.24" dur="3.359">data frames so if your data fits in</text><text start="2136.8" dur="3.92">memory</text><text start="2137.599" dur="4.641">use pandas um if your data fits in</text><text start="2140.72" dur="4.8">memory and your code</text><text start="2142.24" dur="5.839">doesn&amp;#39;t run super quickly um</text><text start="2145.52" dur="4.8">i wouldn&amp;#39;t go to dusk i&amp;#39;d try to i&amp;#39;d do</text><text start="2148.079" dur="3.921">my best to optimize my pandas code</text><text start="2150.32" dur="3.519">before trying to get gains gains and</text><text start="2152" dur="4.079">efficiency um</text><text start="2153.839" dur="3.841">but dark itself becomes useful when the</text><text start="2156.079" dur="3.04">data set you want to analyze is larger</text><text start="2157.68" dur="3.84">than your machine&amp;#39;s ram</text><text start="2159.119" dur="3.921">um where you normally run into memory</text><text start="2161.52" dur="3.839">errors and that&amp;#39;s what we saw</text><text start="2163.04" dur="4.32">with the taxicab example the other</text><text start="2165.359" dur="3.441">example that we&amp;#39;ll see when we get to um</text><text start="2167.36" dur="5.999">[Music]</text><text start="2168.8" dur="6.72">machine learning is</text><text start="2173.359" dur="3.921">you can do machine learning on a small</text><text start="2175.52" dur="2.48">data set that fits in memory but if</text><text start="2177.28" dur="3.839">you&amp;#39;re</text><text start="2178" dur="5.28">building big models or training over</text><text start="2181.119" dur="4.081">like a lot of different hyper parameters</text><text start="2183.28" dur="4.48">or different types of models</text><text start="2185.2" dur="4.24">you can you can parallelize that using</text><text start="2187.76" dur="3.359">using dark so there is</text><text start="2189.44" dur="3.84">you know you want to use dash perhaps in</text><text start="2191.119" dur="4.96">the big data or medium to big data limit</text><text start="2193.28" dur="4.799">um as we see here um or in the medium to</text><text start="2196.079" dur="4.481">big model limit where training</text><text start="2198.079" dur="4">for example takes and takes a lot of</text><text start="2200.56" dur="3.84">time okay</text><text start="2202.079" dur="3.841">so without further ado uh let&amp;#39;s get</text><text start="2204.4" dur="4.64">started with das data frames</text><text start="2205.92" dur="5.52">um you likely ran this uh preparation</text><text start="2209.04" dur="4.64">file to get the data in the previous</text><text start="2211.44" dur="3.679">um notebook but if you didn&amp;#39;t execute</text><text start="2213.68" dur="5.679">that um</text><text start="2215.119" dur="4.96">now we&amp;#39;re going to get our file names by</text><text start="2219.359" dur="3.041">doing</text><text start="2220.079" dur="3.76">doing a few joins and we see our file is</text><text start="2222.4" dur="4.719">a string data nyc</text><text start="2223.839" dur="6.401">flights um a wildcard</text><text start="2227.119" dur="6.321">to access all of them dot dot csv</text><text start="2230.24" dur="6.72">and we&amp;#39;re going to import our dusk</text><text start="2233.44" dur="6.159">dust.dataframe and read in our dataframe</text><text start="2236.96" dur="3.84">um parsing some dates setting some</text><text start="2239.599" dur="4.161">sending some data types</text><text start="2240.8" dur="3.52">okay i&amp;#39;ll execute that we&amp;#39;ll see we have</text><text start="2243.76" dur="5.359">10</text><text start="2244.32" dur="6.88">partitions um as we noted before</text><text start="2249.119" dur="3.601">if this was a pandas data frame we&amp;#39;d see</text><text start="2251.2" dur="4.56">a bunch of entries here</text><text start="2252.72" dur="5.76">we don&amp;#39;t we see only the column names</text><text start="2255.76" dur="3.599">and the data types of the columns um and</text><text start="2258.48" dur="3.76">the reason is</text><text start="2259.359" dur="4.401">as we&amp;#39;ve said it explicitly here is the</text><text start="2262.24" dur="2.72">representation of the data frame object</text><text start="2263.76" dur="2.96">contains no data</text><text start="2264.96" dur="3.2">um it&amp;#39;s done dusk has done enough work</text><text start="2266.72" dur="3.28">to read the start of the file</text><text start="2268.16" dur="3.12">um so that we know a bit about it some</text><text start="2270" dur="2.96">of the important stuff and then further</text><text start="2271.28" dur="3.92">column types and</text><text start="2272.96" dur="3.92">column names and data types okay but we</text><text start="2275.2" dur="3.2">don&amp;#39;t once again we don&amp;#39;t let&amp;#39;s say</text><text start="2276.88" dur="3.199">we&amp;#39;ve got 100 gigs of data</text><text start="2278.4" dur="3.439">we don&amp;#39;t want to like do this call and</text><text start="2280.079" dur="2.161">suddenly it&amp;#39;s reading all that stuff in</text><text start="2281.839" dur="2.321">and</text><text start="2282.24" dur="3.04">doing a whole bunch of compute until we</text><text start="2284.16" dur="3.76">explicitly</text><text start="2285.28" dur="4.079">uh tell it to okay now this is really</text><text start="2287.92" dur="3.36">cool if you know a bit of pandas</text><text start="2289.359" dur="3.441">you&amp;#39;ll know that you can um there&amp;#39;s an</text><text start="2291.28" dur="3.28">attribute columns which</text><text start="2292.8" dur="3.279">prints out it&amp;#39;s well it&amp;#39;s actually the</text><text start="2294.56" dur="2.08">columns form an index right the pandas</text><text start="2296.079" dur="2.561">index</text><text start="2296.64" dur="3.04">object um and we get the we get the</text><text start="2298.64" dur="4.08">column names there</text><text start="2299.68" dur="4.96">cool pandas in dark form</text><text start="2302.72" dur="4.16">we can check out the data types as well</text><text start="2304.64" dur="3.68">um as we would in pandas we see we&amp;#39;ve</text><text start="2306.88" dur="3.84">got some ins for the day of the week</text><text start="2308.32" dur="6">we&amp;#39;ve got some floats for departure time</text><text start="2310.72" dur="3.92">um maybe we&amp;#39;d actually um prefer that to</text><text start="2314.32" dur="1.84">be</text><text start="2314.64" dur="3.439">you know a date time at some point we&amp;#39;ve</text><text start="2316.16" dur="4.32">got some objects which generally are the</text><text start="2318.079" dur="6">most general on</text><text start="2320.48" dur="5.76">objects so generally strings um</text><text start="2324.079" dur="3.921">so that&amp;#39;s all pandasey type stuff in</text><text start="2326.24" dur="4.64">addition das data frames have</text><text start="2328" dur="4.32">an attribute um n partitions which tells</text><text start="2330.88" dur="2.08">us the number of partitions and we saw</text><text start="2332.32" dur="2.48">before</text><text start="2332.96" dur="3.36">that that&amp;#39;s 10 so i&amp;#39;d expect to see 10</text><text start="2334.8" dur="5.36">here hey look at that</text><text start="2336.32" dur="7.12">um now this is something that</text><text start="2340.16" dur="6.16">um we talk about a lot in the</text><text start="2343.44" dur="3.84">delayed notebook is really the task</text><text start="2346.32" dur="3.36">graph</text><text start="2347.28" dur="4.799">and i don&amp;#39;t want to say too much about</text><text start="2349.68" dur="5.12">that but really it&amp;#39;s a</text><text start="2352.079" dur="4.641">visual schematic of of the order in</text><text start="2354.8" dur="5.2">which different types of compute happen</text><text start="2356.72" dur="6">okay um and so the task graph for</text><text start="2360" dur="3.68">read csv tells us what happens when we</text><text start="2362.72" dur="4.08">call compute</text><text start="2363.68" dur="5.52">and essentially it reads csv um</text><text start="2366.8" dur="3.44">10 ten times zero indexed of course</text><text start="2369.2" dur="4.32">because python</text><text start="2370.24" dur="5.04">um it reads csv uh ten different times</text><text start="2373.52" dur="2.559">into these ten different pandas pandas</text><text start="2375.28" dur="2.24">data frames</text><text start="2376.079" dur="4.081">and if there were group buys or stuff</text><text start="2377.52" dur="4.4">after that we&amp;#39;d see them happen in</text><text start="2380.16" dur="3.28">in the in the graph there and we may see</text><text start="2381.92" dur="4.8">an example of this in a second</text><text start="2383.44" dur="6.399">um so once again as with pandas</text><text start="2386.72" dur="5.68">um we&amp;#39;re going to view the the head of</text><text start="2389.839" dur="2.561">the data frame</text><text start="2392.56" dur="5.68">great and we see a bunch of stuff um</text><text start="2396.16" dur="3.36">you know we we see the first first five</text><text start="2398.24" dur="4">rows um</text><text start="2399.52" dur="3.76">i&amp;#39;m actually also gonna gonna have a</text><text start="2402.24" dur="3.2">look at the</text><text start="2403.28" dur="3.68">the tail the final five rows that may</text><text start="2405.44" dur="3.76">take longer um</text><text start="2406.96" dur="3.52">because it&amp;#39;s accessing the the final i</text><text start="2409.2" dur="4.56">um</text><text start="2410.48" dur="4.16">i there&amp;#39;s a joke and it may not even be</text><text start="2413.76" dur="3.28">a joke how much</text><text start="2414.64" dur="4">um data analytics is actually biased by</text><text start="2417.04" dur="2.799">people looking at the first five rows</text><text start="2418.64" dur="4.4">before actually</text><text start="2419.839" dur="5.76">you know interrogating the data uh more</text><text start="2423.04" dur="4">more seriously um so how would all of</text><text start="2425.599" dur="5.041">our results look different</text><text start="2427.04" dur="5.36">if um if our files were ordered in</text><text start="2430.64" dur="3.679">in a different way that&amp;#39;s another</text><text start="2432.4" dur="4.439">conversation for a more philosophical</text><text start="2434.319" dur="5.121">conversation for another time</text><text start="2436.839" dur="3.561">um so now i want to show you some</text><text start="2439.44" dur="4.879">computations</text><text start="2440.4" dur="5.919">with uh dark data frames okay so</text><text start="2444.319" dur="3.201">since dash data frames implement a</text><text start="2446.319" dur="3.52">pandas like api</text><text start="2447.52" dur="3.28">um we can just write our familiar pandas</text><text start="2449.839" dur="4.401">codes so</text><text start="2450.8" dur="5.519">i want to look at the column um</text><text start="2454.24" dur="3.839">uh departure delay and look at the</text><text start="2456.319" dur="3.601">maximum of that column</text><text start="2458.079" dur="4.161">i&amp;#39;m going to call that max delay so you</text><text start="2459.92" dur="4.08">can see we&amp;#39;re selecting the column</text><text start="2462.24" dur="5.359">and then applying the max method as we</text><text start="2464" dur="6.72">would with pandas oh what happened there</text><text start="2467.599" dur="6.401">gives us some uh da scala</text><text start="2470.72" dur="4.56">series um and</text><text start="2474" dur="3.2">what&amp;#39;s happened is we haven&amp;#39;t called</text><text start="2475.28" dur="4.88">compute right so it hasn&amp;#39;t actually done</text><text start="2477.2" dur="4.32">the compute yet um</text><text start="2480.16" dur="2.8">we&amp;#39;re going to do compute but first</text><text start="2481.52" dur="2.16">we&amp;#39;re going to visualize the task graph</text><text start="2482.96" dur="4.399">like we did</text><text start="2483.68" dur="5.679">here and let&amp;#39;s try to reason what the</text><text start="2487.359" dur="4.72">task graph would look like right so</text><text start="2489.359" dur="3.76">the task graph first it&amp;#39;s going to read</text><text start="2492.079" dur="4.401">in</text><text start="2493.119" dur="6.72">all of these things and then</text><text start="2496.48" dur="7.76">it&amp;#39;ll probably perform this selector</text><text start="2499.839" dur="7.121">on each of these</text><text start="2504.24" dur="4.079">different pandas data frames comprising</text><text start="2506.96" dur="4.08">the dash data frame</text><text start="2508.319" dur="4.8">and then it will compute the max of each</text><text start="2511.04" dur="2.799">of those and then do a max on all those</text><text start="2513.119" dur="2.641">maxes</text><text start="2513.839" dur="4.561">i think that&amp;#39;s what i would assume is</text><text start="2515.76" dur="2.64">happening here</text><text start="2518.8" dur="3.44">great so that&amp;#39;s what we&amp;#39;re what we&amp;#39;re</text><text start="2520.16" dur="2.56">doing we&amp;#39;re reading this so we read the</text><text start="2522.24" dur="2.56">first</text><text start="2522.72" dur="3.119">um perform the first read csv get this</text><text start="2524.8" dur="3.2">das data frame</text><text start="2525.839" dur="3.841">um get item i think is that selection</text><text start="2528" dur="3.119">then we&amp;#39;re taking the max</text><text start="2529.68" dur="3.679">we&amp;#39;re doing the same for all of them</text><text start="2531.119" dur="3.921">then we take all of these max&amp;#39;s</text><text start="2533.359" dur="4.24">and aggregate them and then take the max</text><text start="2535.04" dur="4">of that okay so that</text><text start="2537.599" dur="3.841">that&amp;#39;s essentially what&amp;#39;s happening when</text><text start="2539.04" dur="4.64">i call compute which i&amp;#39;m going to do</text><text start="2541.44" dur="2.24">now</text><text start="2548.48" dur="6">moment of truth okay</text><text start="2552.079" dur="3.441">so uh that took around eight seconds and</text><text start="2554.48" dur="4.48">it tells us the max</text><text start="2555.52" dur="6.559">and i i&amp;#39;m sorry let&amp;#39;s let&amp;#39;s just get</text><text start="2558.96" dur="7.119">out some of our dashboards up</text><text start="2562.079" dur="4">as well um</text><text start="2569.839" dur="5.841">huh i think in this notebook we are</text><text start="2573.44" dur="3.76">using the single machine scheduler hugo</text><text start="2575.68" dur="3.04">so i don&amp;#39;t think there is a dashboard to</text><text start="2577.2" dur="4.08">be seen exactly</text><text start="2578.72" dur="3.76">yeah thank you for that that that catch</text><text start="2581.28" dur="4.88">james um</text><text start="2582.48" dur="5.92">great um is even better</text><text start="2586.16" dur="4.159">um uh james we have a question around</text><text start="2588.4" dur="4.959">using dark for</text><text start="2590.319" dur="6.721">um reinforcement learning can you</text><text start="2593.359" dur="6.48">can you speak to that um yeah so</text><text start="2597.04" dur="3.52">uh it depends on this i mean yeah short</text><text start="2599.839" dur="2.24">answer</text><text start="2600.56" dur="3.44">yes you can use gas to train</text><text start="2602.079" dur="3.921">reinforcement learning models</text><text start="2604" dur="4.319">um so there&amp;#39;s a package that hugo will</text><text start="2606" dur="5.04">talk about called desk ml that we&amp;#39;ll see</text><text start="2608.319" dur="4">in the next notebook uh for distributing</text><text start="2611.04" dur="4.799">machine learning</text><text start="2612.319" dur="7.121">um that paralyzes and and distributes</text><text start="2615.839" dur="5.28">um some existing models uh using desks</text><text start="2619.44" dur="4.8">so for instance things like</text><text start="2621.119" dur="6.801">random forces forest inside kit learn</text><text start="2624.24" dur="5.76">um so so yes you can use das to uh</text><text start="2627.92" dur="3.439">uh do distributed training for models</text><text start="2630" dur="1.92">i&amp;#39;m not actually sure if gaskml</text><text start="2631.359" dur="2.561">implements</text><text start="2631.92" dur="3.04">any reinforcement learning models in</text><text start="2633.92" dur="2.56">particular</text><text start="2634.96" dur="3.2">um but that is certainly something that</text><text start="2636.48" dur="3.52">that can be done</text><text start="2638.16" dur="3.439">yeah and i&amp;#39;ll i&amp;#39;ll build on that by</text><text start="2640" dur="2.079">saying we are about to jump into machine</text><text start="2641.599" dur="3.76">learning</text><text start="2642.079" dur="3.76">um i don&amp;#39;t think as james said i don&amp;#39;t</text><text start="2645.359" dur="2.401">think</text><text start="2645.839" dur="3.041">there&amp;#39;s reinforcement learning um</text><text start="2647.76" dur="4.319">explicitly that</text><text start="2648.88" dur="4.56">that one can do um but um you of course</text><text start="2652.079" dur="4.641">can use the das</text><text start="2653.44" dur="4.96">scheduler yourself to um you know to</text><text start="2656.72" dur="2.08">distribute any reinforcement learning</text><text start="2658.4" dur="2.32">stuff</text><text start="2658.8" dur="3.6">you you have as well and that&amp;#39;s actually</text><text start="2660.72" dur="4.16">another another point to make that maybe</text><text start="2662.4" dur="4.4">james can speak to a bit more is that um</text><text start="2664.88" dur="3.68">the dark team of course built all of</text><text start="2666.8" dur="2.64">these high-level collections and task</text><text start="2668.56" dur="2.88">arrays and</text><text start="2669.44" dur="3.12">dust data frames and were pleasantly</text><text start="2671.44" dur="4.159">surprised when</text><text start="2672.56" dur="4.64">you know maybe even up to half the</text><text start="2675.599" dur="2.961">people using dust came in all like we</text><text start="2677.2" dur="3.919">love all that but we&amp;#39;re going to use</text><text start="2678.56" dur="5.12">the scheduler for our own bespoke use</text><text start="2681.119" dur="2.561">cases right</text><text start="2684.48" dur="3.44">yeah exactly yeah the original intention</text><text start="2686.24" dur="3.2">was to like make basically a num</text><text start="2687.92" dur="3.28">like a parallel numpy so that was like</text><text start="2689.44" dur="4.879">the desk array stuff like run</text><text start="2691.2" dur="6.72">run numpy and parallel on your laptop um</text><text start="2694.319" dur="4.161">and and yeah so in order to do that we</text><text start="2697.92" dur="3.52">ended up</text><text start="2698.48" dur="3.839">building a distributed scheduler um</text><text start="2701.44" dur="3.919">which sort of does</text><text start="2702.319" dur="4.721">arbitrary task uh computations so</text><text start="2705.359" dur="3.281">not just things like uh you know</text><text start="2707.04" dur="2.96">parallel numpy but</text><text start="2708.64" dur="3.36">really whatever you&amp;#39;d like to throw at</text><text start="2710" dur="2.72">it and uh it turns out that ended up</text><text start="2712" dur="3.04">being really</text><text start="2712.72" dur="3.119">useful for people um and so yeah now</text><text start="2715.04" dur="3.76">people use that</text><text start="2715.839" dur="4.801">um sort of on their own uh just using</text><text start="2718.8" dur="4.799">the distributed scheduler to do</text><text start="2720.64" dur="3.439">totally custom algorithms um in parallel</text><text start="2723.599" dur="2.161">um</text><text start="2724.079" dur="3.681">in addition to these like nice</text><text start="2725.76" dur="5.2">collections like you saw hugo presents</text><text start="2727.76" dur="5.2">the dash data frame um api is you know</text><text start="2730.96" dur="3.76">the same as the panda&amp;#39;s api so there is</text><text start="2732.96" dur="2.24">this like familiar space you can use</text><text start="2734.72" dur="2.16">things</text><text start="2735.2" dur="3.04">like the high-level collections but you</text><text start="2736.88" dur="3.92">can also run</text><text start="2738.24" dur="3.839">uh whatever custom like hugo said</text><text start="2740.8" dur="4.24">bespoke computations</text><text start="2742.079" dur="4.24">you might have exactly and it&amp;#39;s it&amp;#39;s</text><text start="2745.04" dur="3.279">been wonderful to see</text><text start="2746.319" dur="3.681">so many people so many people do that</text><text start="2748.319" dur="3.52">and the first thing</text><text start="2750" dur="3.04">as we&amp;#39;ll see here the first thing to</text><text start="2751.839" dur="2.48">think about is if</text><text start="2753.04" dur="3.039">if you&amp;#39;re doing lifestyle compute if</text><text start="2754.319" dur="3.921">there&amp;#39;s anything you can you know</text><text start="2756.079" dur="3.04">parallelize embarrassingly as they say</text><text start="2758.24" dur="2.96">right so just</text><text start="2759.119" dur="3.041">if you&amp;#39;re doing a hyper parameter search</text><text start="2761.2" dur="4.159">you just</text><text start="2762.16" dur="5.04">run some on one worker and some on</text><text start="2765.359" dur="3.121">the other and there there&amp;#39;s no</text><text start="2767.2" dur="2.96">interaction effect so you don&amp;#39;t need to</text><text start="2768.48" dur="6.08">worry about that as opposed to</text><text start="2770.16" dur="6">if you&amp;#39;re trying to do um</text><text start="2774.56" dur="3.12">you know train on streaming data where</text><text start="2776.16" dur="4.8">you may require it all</text><text start="2777.68" dur="5.919">to happen on on on the same worker okay</text><text start="2780.96" dur="4.879">um yeah so even think about trying to</text><text start="2783.599" dur="5.52">compute the standard deviation of a</text><text start="2785.839" dur="6.961">of a a univariate data set right um</text><text start="2789.119" dur="4.881">in in that case um you can&amp;#39;t just send</text><text start="2792.8" dur="2.64">you can&amp;#39;t just compute the standard</text><text start="2794" dur="3.359">deviation on two workers and then</text><text start="2795.44" dur="3.28">combine the result in some some way you</text><text start="2797.359" dur="2.561">need to do something slightly slightly</text><text start="2798.72" dur="3.28">more nuanced and slightly</text><text start="2799.92" dur="3.28">slightly clever more clever um i mean</text><text start="2802" dur="3.44">you still can actually in</text><text start="2803.2" dur="3.84">in that case but you can&amp;#39;t just do it as</text><text start="2805.44" dur="4">naively as that</text><text start="2807.04" dur="3.76">um but so now we&amp;#39;re talking about</text><text start="2809.44" dur="2.639">parallel and distributed machine</text><text start="2810.8" dur="3.68">learning we have 20 minutes left so this</text><text start="2812.079" dur="6.321">is kind of going to be a whirlwind tour</text><text start="2814.48" dur="6.72">but um you know whirlwinds when safe uh</text><text start="2818.4" dur="4">exciting and informative um i just want</text><text start="2821.2" dur="2.56">to make clear the material in this</text><text start="2822.4" dur="2.719">notebook is based on the open source</text><text start="2823.76" dur="3.12">content from darsk&amp;#39;s</text><text start="2825.119" dur="3.921">tutorial repository as there&amp;#39;s a bunch</text><text start="2826.88" dur="3.28">of stuff we&amp;#39;ve shown you today</text><text start="2829.04" dur="2.4">the reason we&amp;#39;ve done that is because</text><text start="2830.16" dur="2.56">they did it so well so i just want to</text><text start="2831.44" dur="2.32">give a shout out to all the das</text><text start="2832.72" dur="4.32">contributors</text><text start="2833.76" dur="5.04">okay so what we&amp;#39;re going to do now is um</text><text start="2837.04" dur="3.519">just break down machine learning scaling</text><text start="2838.8" dur="3.68">problems into two categories</text><text start="2840.559" dur="3.201">just review a bit of psychic learn in</text><text start="2842.48" dur="2.72">passing um</text><text start="2843.76" dur="3.92">solve a machine learning problem with</text><text start="2845.2" dur="4.399">single michelle single michelle</text><text start="2847.68" dur="3.28">um i don&amp;#39;t know who she is but single</text><text start="2849.599" dur="3.441">michelle wow</text><text start="2850.96" dur="3.52">single machine and parallelism with</text><text start="2853.04" dur="3.279">psychic learning job lib</text><text start="2854.48" dur="3.04">then solve an l problem with an ml</text><text start="2856.319" dur="4.24">problem with multiple machines and</text><text start="2857.52" dur="4.799">parallelism using uh dark as well</text><text start="2860.559" dur="3.28">and we won&amp;#39;t have time to burst for the</text><text start="2862.319" dur="1.921">cloud i don&amp;#39;t think but you can also</text><text start="2863.839" dur="3.121">play</text><text start="2864.24" dur="5.04">play around with that okay so as i</text><text start="2866.96" dur="4.24">mentioned before</text><text start="2869.28" dur="2.96">when thinking about distributed compute</text><text start="2871.2" dur="2.639">a lot of people do it when they have</text><text start="2872.24" dur="3.04">large data they don&amp;#39;t necessarily think</text><text start="2873.839" dur="3.76">about the large model limit</text><text start="2875.28" dur="3.6">um and this schematic kind of speaks to</text><text start="2877.599" dur="2.881">that um</text><text start="2878.88" dur="3.04">if you&amp;#39;ve got a small model that fits in</text><text start="2880.48" dur="2.56">ram you don&amp;#39;t need to think about</text><text start="2881.92" dur="3.36">distributed compute</text><text start="2883.04" dur="3.279">if your data size if your data is larger</text><text start="2885.28" dur="3.44">than your ram</text><text start="2886.319" dur="3.921">um so your computer&amp;#39;s ram bound then you</text><text start="2888.72" dur="3.76">want to start going to a distributed</text><text start="2890.24" dur="4">setting or if your model is big and cpu</text><text start="2892.48" dur="3.28">bound um such as like large-scale</text><text start="2894.24" dur="3.44">hyper-parameter searches or like</text><text start="2895.76" dur="3.12">ensembl blended models of like machine</text><text start="2897.68" dur="4.48">learning algorithms</text><text start="2898.88" dur="3.84">um whatever it is and then of course we</text><text start="2902.16" dur="3.12">have the</text><text start="2902.72" dur="4.399">you know big data big model uh limit</text><text start="2905.28" dur="3.6">where um distributed computer desk is</text><text start="2907.119" dur="5.121">incredibly handy as i&amp;#39;m sure</text><text start="2908.88" dur="4.8">you could uh imagine okay and</text><text start="2912.24" dur="3.2">that&amp;#39;s really what i&amp;#39;ve what i&amp;#39;ve gone</text><text start="2913.68" dur="3.679">through here</text><text start="2915.44" dur="3.6">um a bird&amp;#39;s-eye view of the strategies</text><text start="2917.359" dur="3.681">we think about um</text><text start="2919.04" dur="4.16">if it&amp;#39;s in memory in the bottom left</text><text start="2921.04" dur="3.36">quadrant just use scikit-learn or your</text><text start="2923.2" dur="3.84">favorite ml library</text><text start="2924.4" dur="3.76">um otherwise known as psychic learn um</text><text start="2927.04" dur="3.44">for me anyway</text><text start="2928.16" dur="2.32">um</text><text start="2931.2" dur="3.119">i was going to make a note about xg</text><text start="2932.8" dur="4.64">boost but i but i won&amp;#39;t</text><text start="2934.319" dur="5.28">um for large models</text><text start="2937.44" dur="3.36">uh you can use joblib and your favorite</text><text start="2939.599" dur="4.24">circuit learn estimator</text><text start="2940.8" dur="5.2">for large data sets uh use our dark ml</text><text start="2943.839" dur="4.24">estimators so we&amp;#39;re gonna do a whirlwind</text><text start="2946" dur="4.16">tour of psychic learn in</text><text start="2948.079" dur="4.561">in five minutes we&amp;#39;re going to load in</text><text start="2950.16" dur="4.159">some data so we&amp;#39;ll actually generate it</text><text start="2952.64" dur="4.56">we&amp;#39;ll import scikit-learn for our ml</text><text start="2954.319" dur="5.121">algorithm create an estimator</text><text start="2957.2" dur="5.6">and then check the accuracy of the model</text><text start="2959.44" dur="6.56">okay so once again i&amp;#39;m actually going to</text><text start="2962.8" dur="4.72">clear all outputs after restarting the</text><text start="2966" dur="4.88">kernel</text><text start="2967.52" dur="5.039">okay so this is a utility function of</text><text start="2970.88" dur="3.76">psychic learn to create some data sets</text><text start="2972.559" dur="5.121">so i&amp;#39;m going to make um</text><text start="2974.64" dur="5.52">a classification data set with four</text><text start="2977.68" dur="4.159">features and 10 000 samples and just</text><text start="2980.16" dur="4.72">have a quick view</text><text start="2981.839" dur="6.641">um of some of it um</text><text start="2984.88" dur="6.959">so just a reminder on ml</text><text start="2988.48" dur="4.079">um x is the samples matrix um the size</text><text start="2991.839" dur="4.081">of x</text><text start="2992.559" dur="5.28">is um the number of samples</text><text start="2995.92" dur="3.28">uh in terms of rows number of features</text><text start="2997.839" dur="4.401">as columns</text><text start="2999.2" dur="6.879">um and then a feature or an attribute</text><text start="3002.24" dur="7.2">is uh what we&amp;#39;re trying to predict</text><text start="3006.079" dur="6.24">essentially okay um so why um</text><text start="3009.44" dur="3.84">is the predictor variable uh which we&amp;#39;re</text><text start="3012.319" dur="2.961">where</text><text start="3013.28" dur="3.44">um which we&amp;#39;re or the target variable</text><text start="3015.28" dur="2.24">which we&amp;#39;re trying to predict so let&amp;#39;s</text><text start="3016.72" dur="3.599">have a quick view</text><text start="3017.52" dur="3.76">of why it&amp;#39;s zeros and ones in in this</text><text start="3020.319" dur="4.481">case</text><text start="3021.28" dur="4">okay so um yep that&amp;#39;s what i&amp;#39;ve said</text><text start="3024.8" dur="2.48">here</text><text start="3025.28" dur="4.079">why are the targets which are real</text><text start="3027.28" dur="3.039">numbers for regression tasks or integers</text><text start="3029.359" dur="4.161">for classification</text><text start="3030.319" dur="4.721">or any other discrete sets of values um</text><text start="3033.52" dur="3.44">no words about unsupervised learning at</text><text start="3035.04" dur="2.96">the moment we&amp;#39;re just going to support</text><text start="3036.96" dur="3.28">we&amp;#39;re going to</text><text start="3038" dur="3.44">fit a support vector classifier for this</text><text start="3040.24" dur="3.44">example</text><text start="3041.44" dur="3.6">so let&amp;#39;s just load the appropriate</text><text start="3043.68" dur="3.919">scikit-learn</text><text start="3045.04" dur="4.799">module we don&amp;#39;t really need to discuss</text><text start="3047.599" dur="2.24">what</text><text start="3050.24" dur="3.76">support vector classifiers are at the</text><text start="3051.839" dur="3.201">moment now this is one of the</text><text start="3054" dur="3.04">very beautiful things about the</text><text start="3055.04" dur="5.2">scikit-learn api</text><text start="3057.04" dur="6.64">in terms of fitting the the model</text><text start="3060.24" dur="4.56">we instantiate um a classifier and we</text><text start="3063.68" dur="4.879">want to fit it</text><text start="3064.8" dur="5.519">to the features with respect to the</text><text start="3068.559" dur="3.441">target okay so the first argument is the</text><text start="3070.319" dur="5.201">features second argument</text><text start="3072" dur="8.4">is the target variable</text><text start="3075.52" dur="6.16">so we&amp;#39;ve done that now i&amp;#39;m not going to</text><text start="3080.4" dur="3.679">worry about inspecting the learn</text><text start="3081.68" dur="5.52">features um i just want to see how</text><text start="3084.079" dur="4.881">accurate it was okay and once we see how</text><text start="3087.2" dur="4.639">accurate it was i&amp;#39;m not gonna do this</text><text start="3088.96" dur="3.359">but then we can um make a prediction</text><text start="3091.839" dur="3.52">right</text><text start="3092.319" dur="4.24">using uh estimator dot predict on a new</text><text start="3095.359" dur="4.24">a new data set</text><text start="3096.559" dur="5.04">um so this estimator will tell us</text><text start="3099.599" dur="3.041">so this score will tell us the accuracy</text><text start="3101.599" dur="4.72">and essentially</text><text start="3102.64" dur="8.16">that&amp;#39;s the proportion or percentage</text><text start="3106.319" dur="7.361">a fraction of um</text><text start="3110.8" dur="4.559">the uh results that were that the</text><text start="3113.68" dur="2.96">estimator got correct and we&amp;#39;re doing</text><text start="3115.359" dur="3.2">this on the training</text><text start="3116.64" dur="3.28">data set we&amp;#39;ve just trained the model on</text><text start="3118.559" dur="3.201">this so this is telling us</text><text start="3119.92" dur="3.76">um the accuracy on the on the training</text><text start="3121.76" dur="4">data set okay so it&amp;#39;s 90</text><text start="3123.68" dur="3.2">accurate on the training data set if you</text><text start="3125.76" dur="2">dive into this a bit more you&amp;#39;ll</text><text start="3126.88" dur="3.28">recognize that</text><text start="3127.76" dur="3.359">um if we we really want to know the</text><text start="3130.16" dur="4.159">accuracy</text><text start="3131.119" dur="5.921">on a holdout set or a test set</text><text start="3134.319" dur="4.481">um and it should be probably a bit lower</text><text start="3137.04" dur="2.559">because this is what we use to fit it</text><text start="3138.8" dur="2.96">okay</text><text start="3139.599" dur="3.201">but all that having been said i expect</text><text start="3141.76" dur="2.96">um you know</text><text start="3142.8" dur="3.68">if if this is all resonating with you it</text><text start="3144.72" dur="5.76">means we can really move on to the</text><text start="3146.48" dur="7.04">distributed stuff um um in in a second</text><text start="3150.48" dur="4.8">um but the other thing that that&amp;#39;s</text><text start="3153.52" dur="2.4">important to note is that we&amp;#39;ve trained</text><text start="3155.28" dur="2.4">it but</text><text start="3155.92" dur="4">a lot of model a lot of estimators and</text><text start="3157.68" dur="3.6">models have um hyper parameters that</text><text start="3159.92" dur="4.159">affect the fit but you</text><text start="3161.28" dur="4.4">that we need to specify up front um</text><text start="3164.079" dur="2.48">instead of being learned during training</text><text start="3165.68" dur="2.56">so</text><text start="3166.559" dur="3.361">you know there&amp;#39;s a c parameter here</text><text start="3168.24" dur="3.68">there&amp;#39;s a uh</text><text start="3169.92" dur="3.04">are we using shrinking or not um so we</text><text start="3171.92" dur="2.399">specify those</text><text start="3172.96" dur="3.44">we didn&amp;#39;t need to specify them because</text><text start="3174.319" dur="3.441">there are default values but here we</text><text start="3176.4" dur="4.959">specify them</text><text start="3177.76" dur="6.64">okay and um</text><text start="3181.359" dur="6">then we&amp;#39;re going to um</text><text start="3184.4" dur="2.959">look at the score now</text><text start="3187.76" dur="4.079">okay this is amazing we&amp;#39;ve got 50</text><text start="3190.72" dur="2.879">accuracy um</text><text start="3191.839" dur="3.841">which is the worst score possible just</text><text start="3193.599" dur="3.841">think about this if if you&amp;#39;ve got binary</text><text start="3195.68" dur="3.52">classification task and you&amp;#39;ve got 40</text><text start="3197.44" dur="3.76">accuracy then you just flip the labels</text><text start="3199.2" dur="3.52">and that changes to 60 accuracy so it&amp;#39;s</text><text start="3201.2" dur="4.72">amazing that we&amp;#39;ve actually hit</text><text start="3202.72" dur="3.839">50 accuracy we&amp;#39;re to be congratulated on</text><text start="3205.92" dur="2.639">that</text><text start="3206.559" dur="3.921">um and what i want to note here is that</text><text start="3208.559" dur="3.361">we have two sets of hyper parameters</text><text start="3210.48" dur="4.48">we&amp;#39;ve used one&amp;#39;s</text><text start="3211.92" dur="6.24">created 90 actual model with 90 accuracy</text><text start="3214.96" dur="4.96">another one one with 50 accuracy um</text><text start="3218.16" dur="3.36">so we want to find the best hyper</text><text start="3219.92" dur="3.36">parameters essentially and that&amp;#39;s why</text><text start="3221.52" dur="4.319">hyper parameter optimization</text><text start="3223.28" dur="4.88">is is so important um there are several</text><text start="3225.839" dur="4.48">ways to do hyper parameter optimization</text><text start="3228.16" dur="3.919">one is called grid search uh cross</text><text start="3230.319" dur="2.721">validation i won&amp;#39;t talk about cross</text><text start="3232.079" dur="4.48">validation</text><text start="3233.04" dur="5.76">um it&amp;#39;s essentially um a more robust</text><text start="3236.559" dur="4.401">analogue of train test split where you</text><text start="3238.8" dur="4.16">uh train on a subset of your data and</text><text start="3240.96" dur="4.879">compute the accuracy on a test</text><text start="3242.96" dur="4.24">on a holdout set or a test set um cross</text><text start="3245.839" dur="3.121">validation is</text><text start="3247.2" dur="3.359">a as i said a slightly more robust</text><text start="3248.96" dur="2.96">analog of this</text><text start="3250.559" dur="3.121">it&amp;#39;s called grid search because we have</text><text start="3251.92" dur="4.08">a grid of hyper parameters so</text><text start="3253.68" dur="3.76">we have you know in this case we have a</text><text start="3256" dur="2.48">hyper parameter c we have a hyper</text><text start="3257.44" dur="3.2">parameter kernel</text><text start="3258.48" dur="4">and we can imagine them in a in a grid</text><text start="3260.64" dur="4.959">and we&amp;#39;re performing</text><text start="3262.48" dur="5.44">um we&amp;#39;re checking out um the score</text><text start="3265.599" dur="4.161">over all this gr over this entire grid</text><text start="3267.92" dur="4.96">of hyper parameters okay</text><text start="3269.76" dur="4">so to do that um i import grid search</text><text start="3272.88" dur="4.4">csv</text><text start="3273.76" dur="7.839">now i&amp;#39;m going to um</text><text start="3277.28" dur="6.48">compute um the estimator over</text><text start="3281.599" dur="3.121">over these train the estimator over over</text><text start="3283.76" dur="3.44">this grid um</text><text start="3284.72" dur="3.2">and as you see this is taking time now</text><text start="3287.2" dur="3.359">okay</text><text start="3287.92" dur="5.439">um and what i wanted to make clear and i</text><text start="3290.559" dur="5.121">think should be becoming</text><text start="3293.359" dur="3.521">clearer now is that if we have a large</text><text start="3295.68" dur="3.679">hyper parameter</text><text start="3296.88" dur="4.32">uh sweep we want to do on a small data</text><text start="3299.359" dur="4">set das can be useful for that</text><text start="3301.2" dur="3.119">okay because we can send some of the</text><text start="3303.359" dur="2.72">parameters to</text><text start="3304.319" dur="4.161">one worker some to another and they can</text><text start="3306.079" dur="4.561">perform them um in parallel so that&amp;#39;s</text><text start="3308.48" dur="4.56">embarrassingly parallel because</text><text start="3310.64" dur="3.52">you&amp;#39;re you&amp;#39;re doing the same work as you</text><text start="3313.04" dur="2.88">would otherwise</text><text start="3314.16" dur="3.76">um but sending it to a bunch of</text><text start="3315.92" dur="4.08">different workers we saw that took 30</text><text start="3317.92" dur="3.84">seconds which is in my realm of</text><text start="3320" dur="3.119">comfort as a data scientist i&amp;#39;m happy to</text><text start="3321.76" dur="3.44">wait 30 seconds</text><text start="3323.119" dur="3.041">um if i had to wait much longer if this</text><text start="3325.2" dur="2.08">grid was bigger</text><text start="3326.16" dur="4">i&amp;#39;d start to get probably a bit</text><text start="3327.28" dur="6.079">frustrated um</text><text start="3330.16" dur="6.32">but we see that it computed um</text><text start="3333.359" dur="3.521">it for c is equal to all combinations of</text><text start="3336.48" dur="3.44">these</text><text start="3336.88" dur="6">essentially okay um so that&amp;#39;s really all</text><text start="3339.92" dur="4.639">i wanted to say there um and then we can</text><text start="3342.88" dur="4.679">see the best parameters</text><text start="3344.559" dur="6.721">and the best score so the best score was</text><text start="3347.559" dur="5.56">0.098 and it was c10 and the kernel um</text><text start="3351.28" dur="3.12">rbf a radial basis function it doesn&amp;#39;t</text><text start="3353.119" dur="4.321">even matter what that is though</text><text start="3354.4" dur="4.159">um for the purposes of this so we&amp;#39;ve got</text><text start="3357.44" dur="2.96">10 minutes left we&amp;#39;re going to</text><text start="3358.559" dur="3.921">we&amp;#39;re going to make it i can feel it i</text><text start="3360.4" dur="5.52">have a good i have a good sense</text><text start="3362.48" dur="5.119">um a good after the</text><text start="3365.92" dur="3.679">i mean this demo is actually going</text><text start="3367.599" dur="5.841">incredibly well given um the initial</text><text start="3369.599" dur="6.161">technical hurdles so touchwood hugo um</text><text start="3373.44" dur="4.32">okay so what we&amp;#39;ve done is we&amp;#39;ve really</text><text start="3375.76" dur="4.4">segmented ml scaling problems into</text><text start="3377.76" dur="3.359">two categories cpu bound and ram bound</text><text start="3380.16" dur="4.72">um and i</text><text start="3381.119" dur="5.521">i really i can&amp;#39;t emphasize that enough</text><text start="3384.88" dur="3.12">because i see so many people</text><text start="3386.64" dur="3.36">like jumping in to use new cool</text><text start="3388" dur="4.799">technologies um without</text><text start="3390" dur="3.04">perhaps taking it being a bit mindful</text><text start="3392.799" dur="1.841">and</text><text start="3393.04" dur="3.68">intentional about it and reasoning about</text><text start="3394.64" dur="2.719">when things are useful and and when not</text><text start="3396.72" dur="3.04">um</text><text start="3397.359" dur="4.161">i suppose the one point there is that</text><text start="3399.76" dur="3.76">sure data science is a technical</text><text start="3401.52" dur="3.2">discipline but there are a lot of other</text><text start="3403.52" dur="4.319">aspects to it</text><text start="3404.72" dur="5.28">um involving this type of reasoning</text><text start="3407.839" dur="4.561">as well so we then carried out a typical</text><text start="3410" dur="4.4">sklearn workflow for ml problems</text><text start="3412.4" dur="3.439">um with small models and small data and</text><text start="3414.4" dur="2.439">we reviewed hyper parameters and hyper</text><text start="3415.839" dur="4.161">parameter</text><text start="3416.839" dur="5.48">optimization um so in this section</text><text start="3420" dur="3.839">um we&amp;#39;ll see how job lib which is a set</text><text start="3422.319" dur="2.48">of tools to provide lightweight</text><text start="3423.839" dur="3.921">pipelining</text><text start="3424.799" dur="5.361">um in python uh gives us parallelism on</text><text start="3427.76" dur="5.52">our laptop and then we&amp;#39;ll see how dark</text><text start="3430.16" dur="6.959">ml can give us um awesome parallelism</text><text start="3433.28" dur="6.559">uh on on clusters okay so essentially</text><text start="3437.119" dur="4.161">um what i&amp;#39;m doing here is i&amp;#39;m doing</text><text start="3439.839" dur="3.121">exactly the same as above</text><text start="3441.28" dur="4">with a grid search but i&amp;#39;m using the</text><text start="3442.96" dur="5.04">quark the keyword argument n</text><text start="3445.28" dur="4">jobs which tells you how many tasks uh</text><text start="3448" dur="2.96">to run in parallel</text><text start="3449.28" dur="4.079">using the cause available on your local</text><text start="3450.96" dur="3.04">workstation and specifying minus one</text><text start="3453.359" dur="3.681">jobs</text><text start="3454" dur="3.599">means the it just runs them the maximum</text><text start="3457.04" dur="4.799">possible</text><text start="3457.599" dur="4.24">okay so i&amp;#39;m going to execute that</text><text start="3472.839" dur="3">great</text><text start="3483.04" dur="3.519">so we should be done in a second feel</text><text start="3484.64" dur="5.6">free to ask um any questions</text><text start="3486.559" dur="6.8">in the chat oh alex</text><text start="3490.24" dur="3.839">um has a great question in the q a does</text><text start="3493.359" dur="4">das have</text><text start="3494.079" dur="5.881">uh see a sequel and query optimizer</text><text start="3497.359" dur="4.321">um i&amp;#39;m actually so excited that um</text><text start="3499.96" dur="3.32">[Music]</text><text start="3501.68" dur="3.36">and james maybe you can provide a couple</text><text start="3503.28" dur="3.44">of links to this um</text><text start="3505.04" dur="3.6">we&amp;#39;re really excited to have seen dark</text><text start="3506.72" dur="5.44">dust sql um</text><text start="3508.64" dur="7.439">developments there uh recently um</text><text start="3512.16" dur="6.959">so that&amp;#39;s dark hyphen hyphen sql um</text><text start="3516.079" dur="4.801">and we&amp;#39;re actually we&amp;#39;re working on some</text><text start="3519.119" dur="3.041">some content and a blog post and maybe a</text><text start="3520.88" dur="4.32">live live coding session</text><text start="3522.16" dur="5.12">about that in in the near future um so</text><text start="3525.2" dur="3.599">if anyone if you want updates from from</text><text start="3527.28" dur="2.72">coyle feel free to go to our website and</text><text start="3528.799" dur="2.8">sign up for our mailing list</text><text start="3530" dur="3.04">and we&amp;#39;ll let you know about all of this</text><text start="3531.599" dur="3.281">type of stuff but the short answer is</text><text start="3533.04" dur="3.759">yes alex and it&amp;#39;s getting better and um</text><text start="3534.88" dur="3.04">if james is able to post post a link</text><text start="3536.799" dur="2.081">there that would be that would be</text><text start="3537.92" dur="4.96">fantastic</text><text start="3538.88" dur="6.49">um so we&amp;#39;ve done link in the chat</text><text start="3542.88" dur="5.199">fantastic um</text><text start="3545.37" dur="5.749">[Music]</text><text start="3548.079" dur="4.211">and so we&amp;#39;ve we&amp;#39;ve seen how</text><text start="3551.119" dur="4.24">we have um</text><text start="3552.29" dur="6.269">[Music]</text><text start="3555.359" dur="4.161">single machine parallelism here um using</text><text start="3558.559" dur="4.24">the</text><text start="3559.52" dur="4.16">um using the end jobs quark um and in</text><text start="3562.799" dur="3.201">the final minutes</text><text start="3563.68" dur="3.6">let&amp;#39;s see multiple multi-machine</text><text start="3566" dur="5.28">parallelism</text><text start="3567.28" dur="8">with dusk okay um so</text><text start="3571.28" dur="7.92">what i&amp;#39;m going to do is i&amp;#39;m going to</text><text start="3575.28" dur="6.72">uh do my imports and create my</text><text start="3579.2" dur="3.84">client incentive my client and check it</text><text start="3582" dur="4.88">out</text><text start="3583.04" dur="7.92">okay so once again i&amp;#39;m working locally</text><text start="3586.88" dur="4.08">um i hit search and that&amp;#39;ll</text><text start="3591.28" dur="3.519">task is pretty smart in terms of like</text><text start="3592.88" dur="2.719">knowing uh which which client i want to</text><text start="3594.799" dur="3.841">check out</text><text start="3595.599" dur="5.841">do the tasks stream</text><text start="3598.64" dur="4.56">because it&amp;#39;s my favorite i&amp;#39;ll do the</text><text start="3601.44" dur="3.119">cluster map otherwise known as the pew</text><text start="3603.2" dur="4.72">pew map</text><text start="3604.559" dur="6.961">um and then</text><text start="3607.92" dur="6.72">i want some progress we all we all crave</text><text start="3611.52" dur="5.44">progress don&amp;#39;t we um</text><text start="3614.64" dur="2.32">and</text><text start="3619.92" dur="5.76">maybe my workers tab okay great so um</text><text start="3623.359" dur="5.281">we&amp;#39;ve got that up and running now i&amp;#39;m</text><text start="3625.68" dur="6">going to do a slightly uh</text><text start="3628.64" dur="6.24">larger hyper parameter search okay um</text><text start="3631.68" dur="6.08">so remember we had just a couple for c</text><text start="3634.88" dur="4.56">a couple for kernel um we&amp;#39;re going to do</text><text start="3637.76" dur="2.48">more we have some for shrinking now i&amp;#39;m</text><text start="3639.44" dur="2.159">actually</text><text start="3640.24" dur="3.2">going to comment that out because i</text><text start="3641.599" dur="4.24">don&amp;#39;t know how long that&amp;#39;s going to take</text><text start="3643.44" dur="3.52">um if you&amp;#39;re coding them on binder now</text><text start="3645.839" dur="4.96">this may actually take</text><text start="3646.96" dur="6">far far too long for you um but we&amp;#39;ll</text><text start="3650.799" dur="5.841">we&amp;#39;ll see so i&amp;#39;ll execute this code and</text><text start="3652.96" dur="5.2">we should see</text><text start="3656.64" dur="3.199">just sick no we shouldn&amp;#39;t see any work</text><text start="3658.16" dur="4.72">happening yet um</text><text start="3659.839" dur="6.96">but what i&amp;#39;m doing here is</text><text start="3662.88" dur="6.08">oh looks like okay my clusters back up</text><text start="3666.799" dur="3.841">great</text><text start="3668.96" dur="3.2">we&amp;#39;re doing our grid search but we&amp;#39;re</text><text start="3670.64" dur="3.919">going to use um</text><text start="3672.16" dur="4">dask as as the back end right and this</text><text start="3674.559" dur="3.601">is a context manager where we&amp;#39;re</text><text start="3676.16" dur="3.76">asserting that um</text><text start="3678.16" dur="3.52">and and we can just discuss the the</text><text start="3679.92" dur="3.6">syntax there but it&amp;#39;s not particularly</text><text start="3681.68" dur="4.159">important currently i&amp;#39;m going to execute</text><text start="3683.52" dur="2.319">this now</text><text start="3688.079" dur="2.24">and</text><text start="3692" dur="2.559">let&amp;#39;s see</text><text start="3696.319" dur="4">fantastic we&amp;#39;ll see all this um data</text><text start="3698.96" dur="2.08">transfer happening here we&amp;#39;ll see our</text><text start="3700.319" dur="2.721">tasks</text><text start="3701.04" dur="3.759">um happening here we can see these big</text><text start="3703.04" dur="4.24">batches of fit and score</text><text start="3704.799" dur="4.641">fit um so fitting fitting the models</text><text start="3707.28" dur="5.76">then finding um</text><text start="3709.44" dur="6.8">how well they perform uh via this</text><text start="3713.04" dur="3.2">k-fold cross validation</text><text start="3717.28" dur="2.799">which is really cool</text><text start="3720.72" dur="2.24">and</text><text start="3724" dur="7.2">let&amp;#39;s just yep we can see um</text><text start="3727.599" dur="3.601">what&amp;#39;s happening here we can see we</text><text start="3734.079" dur="4.24">currently have 12 processing we&amp;#39;ve got</text><text start="3736.24" dur="4.079">seven in memory and we have um</text><text start="3738.319" dur="3.201">several more that we need to do uh our</text><text start="3740.319" dur="4.561">desk workers we can see</text><text start="3741.52" dur="5.599">us oh we can see our cpu usage</text><text start="3744.88" dur="4.159">we can see how we can see cpu usage</text><text start="3747.119" dur="3.44">across all the workers which is which is</text><text start="3749.039" dur="4.241">pretty cool seeing that distribution</text><text start="3750.559" dur="3.441">is uh is really nice whenever some form</text><text start="3753.28" dur="3.36">of b swarm</text><text start="3754" dur="3.68">plot if you have enough um would would</text><text start="3756.64" dur="3.84">be useful there</text><text start="3757.68" dur="4.399">or even um some form of cumulative</text><text start="3760.48" dur="2.079">distribution function or something like</text><text start="3762.079" dur="3.441">that</text><text start="3762.559" dur="4.48">um not a histogram people okay um you</text><text start="3765.52" dur="3.599">can go to my bayesian tutorial</text><text start="3767.039" dur="4.161">um that i&amp;#39;ve taught here before to hear</text><text start="3769.119" dur="5.761">me rave about um</text><text start="3771.2" dur="5.68">the the horrors of histograms um</text><text start="3774.88" dur="3.36">so we saw that talk a minute which is</text><text start="3776.88" dur="4.08">great and we split it across</text><text start="3778.24" dur="4.48">you know eight cores or whatever it is</text><text start="3780.96" dur="3.28">and now we&amp;#39;ll have a look</text><text start="3782.72" dur="3.76">once again we get the same best</text><text start="3784.24" dur="3.04">performer which is which is a sanity</text><text start="3786.48" dur="4.4">check</text><text start="3787.28" dur="5.759">um and that&amp;#39;s pretty cool</text><text start="3790.88" dur="4.8">i think um we have a we actually have a</text><text start="3793.039" dur="6.481">few minutes left so i am gonna</text><text start="3795.68" dur="3.84">just see if i can um</text><text start="3799.599" dur="2.881">oh let me think</text><text start="3803.839" dur="4.081">yeah i will see if i can burst burst to</text><text start="3806.079" dur="4.881">the cloud and and</text><text start="3807.92" dur="4.639">and do this um that will take uh a</text><text start="3810.96" dur="4.639">minute</text><text start="3812.559" dur="5.681">a minute or two to create the cluster</text><text start="3815.599" dur="5.841">again um but while we&amp;#39;re while we&amp;#39;re</text><text start="3818.24" dur="6.559">doing that i&amp;#39;m wondering if we have any</text><text start="3821.44" dur="5.359">any questions um or if</text><text start="3824.799" dur="4.161">anyone has any feedback on on this</text><text start="3826.799" dur="5.841">workshop i very much welcome</text><text start="3828.96" dur="4.96">welcome that um perhaps if there are any</text><text start="3832.64" dur="3.12">final messages you&amp;#39;d</text><text start="3833.92" dur="3.6">you&amp;#39;d like to say james while we&amp;#39;re</text><text start="3835.76" dur="3.839">spinning this up you can</text><text start="3837.52" dur="3.599">you can let me know yeah sure i just</text><text start="3839.599" dur="3.601">also first off wanted to say thanks</text><text start="3841.119" dur="3.68">everyone for attending and like bearing</text><text start="3843.2" dur="3.839">bearing with us uh with the technical</text><text start="3844.799" dur="5.28">difficulties really appreciate that</text><text start="3847.039" dur="4.241">um real quick i&amp;#39;m just yeah so if you</text><text start="3850.079" dur="2.881">have if you have questions please post</text><text start="3851.28" dur="2.96">in the q a section while the cold</text><text start="3852.96" dur="4.48">cluster&amp;#39;s spinning up uh</text><text start="3854.24" dur="5.04">theodore posted in the last largest</text><text start="3857.44" dur="3.679">example of grid search</text><text start="3859.28" dur="5.68">how much performance gain did we get</text><text start="3861.119" dur="6.801">from using das and not just in jobs</text><text start="3864.96" dur="3.76">hmm that&amp;#39;s a great question and we</text><text start="3867.92" dur="4.159">actually</text><text start="3868.72" dur="7.92">didn&amp;#39;t see um let&amp;#39;s see</text><text start="3872.079" dur="8.641">so it took 80 seconds</text><text start="3876.64" dur="4.959">ah let me get this they&amp;#39;re actually not</text><text start="3880.72" dur="4.24">comparable</text><text start="3881.599" dur="6.161">um because i did the grid search over</text><text start="3884.96" dur="4.48">a different set of hyper parameters i</text><text start="3887.76" dur="2.799">did it over a larger set of hyper</text><text start="3889.44" dur="4.8">parameters</text><text start="3890.559" dur="6.321">um right so when i did um</text><text start="3894.24" dur="4.16">end jobs i did it there were only um it</text><text start="3896.88" dur="2.64">was a two by two grid of hyper</text><text start="3898.4" dur="4.159">parameters</text><text start="3899.52" dur="3.68">whereas when i did it um with with dusk</text><text start="3902.559" dur="4.56">it was a</text><text start="3903.2" dur="6.08">one two three four five six six by three</text><text start="3907.119" dur="3.44">so let&amp;#39;s just reason about that um this</text><text start="3909.28" dur="3.68">one was</text><text start="3910.559" dur="4.161">eighteen six by three is eighteen which</text><text start="3912.96" dur="5.52">took eighty seconds</text><text start="3914.72" dur="7.96">um and this one was two by two</text><text start="3918.48" dur="7.119">uh so it was four and it took</text><text start="3922.68" dur="5.639">26 seconds um</text><text start="3925.599" dur="3.44">so a minor gain i think with this hyper</text><text start="3928.319" dur="3.201">parameter</text><text start="3929.039" dur="3.121">search if you multiply that by by four</text><text start="3931.52" dur="3.279">you&amp;#39;ll</text><text start="3932.16" dur="4.24">well 4.2 4.5 you&amp;#39;ll need that would have</text><text start="3934.799" dur="3.681">taken maybe two minutes or something</text><text start="3936.4" dur="3.28">something like that so we saw some</text><text start="3938.48" dur="3.92">increase in efficiency</text><text start="3939.68" dur="3.679">not a great deal but um james maybe you</text><text start="3942.4" dur="2.08">can say more to this</text><text start="3943.359" dur="3.041">part of the reason for that is that</text><text start="3944.48" dur="3.599">we&amp;#39;re doing it on kind of a very small</text><text start="3946.4" dur="2.8">example so we won&amp;#39;t necessarily see the</text><text start="3948.079" dur="3.681">gains in efficiency</text><text start="3949.2" dur="4.32">with a data set this size and with um a</text><text start="3951.76" dur="2.88">small hyper parameter suite like this is</text><text start="3953.52" dur="4.24">that right</text><text start="3954.64" dur="4.719">yeah yeah and um yeah exactly and i</text><text start="3957.76" dur="2.799">guess also this is more of an uh kind of</text><text start="3959.359" dur="3.44">an illustrative point here</text><text start="3960.559" dur="3.921">i guess uh so you&amp;#39;re just using uh</text><text start="3962.799" dur="4.081">directly using in jobs with</text><text start="3964.48" dur="4.72">something like job lib um by default</text><text start="3966.88" dur="4.64">we&amp;#39;ll use local threads and processes</text><text start="3969.2" dur="3.2">on like whatever machine you happen to</text><text start="3971.52" dur="3.36">be running on so</text><text start="3972.4" dur="4.719">like in this case on hugo&amp;#39;s laptop um</text><text start="3974.88" dur="5.12">one of the real advantages of using</text><text start="3977.119" dur="3.841">uh job lib with the das back in will</text><text start="3980" dur="3.359">actually dispatch</text><text start="3980.96" dur="4.32">back to um to run tasks on a dash</text><text start="3983.359" dur="4.161">cluster is that your cluster can</text><text start="3985.28" dur="3.039">expand beyond what local resources you</text><text start="3987.52" dur="3.599">have</text><text start="3988.319" dur="4.48">so you can run um you know you can</text><text start="3991.119" dur="2.801">basically scale out like for instance</text><text start="3992.799" dur="4.401">using the coil cluster</text><text start="3993.92" dur="4.639">uh to have many many cpus and</text><text start="3997.2" dur="3.359">a large amount of ram that you wouldn&amp;#39;t</text><text start="3998.559" dur="3.28">have on your locally uh table to run and</text><text start="4000.559" dur="3.52">there you&amp;#39;ll see</text><text start="4001.839" dur="3.601">both large performance gains as well as</text><text start="4004.079" dur="3.361">you&amp;#39;ll be able to expand</text><text start="4005.44" dur="3.2">your the set of possible problems you</text><text start="4007.44" dur="3.599">can solve uh</text><text start="4008.64" dur="5.199">to larger than ram uh scenarios so</text><text start="4011.039" dur="4.481">you&amp;#39;re out of out of core training</text><text start="4013.839" dur="3.121">exactly and thank you jack this was</text><text start="4015.52" dur="2.559">absolutely unplanned and we didn&amp;#39;t plan</text><text start="4016.96" dur="2.24">that question but that&amp;#39;s a wonderful</text><text start="4018.079" dur="3.201">segue into</text><text start="4019.2" dur="4">me now performing exactly the same</text><text start="4021.28" dur="4.16">compute with the same code</text><text start="4023.2" dur="3.28">using uh the dasc as the parallel back</text><text start="4025.44" dur="4.159">end um on a</text><text start="4026.48" dur="4">on a coiled cluster which is an aws</text><text start="4029.599" dur="3.601">cluster right</text><text start="4030.48" dur="4.079">um so we can i&amp;#39;m more currently anyway</text><text start="4033.2" dur="4.159">so i will execute</text><text start="4034.559" dur="3.76">this code um and it&amp;#39;s exactly the same</text><text start="4037.359" dur="4">as we did</text><text start="4038.319" dur="3.04">um whoa</text><text start="4041.44" dur="4.96">okay great um so</text><text start="4047.119" dur="5.601">we see our tasks task stream here</text><text start="4050.24" dur="2.48">um</text><text start="4054.48" dur="3.52">you see once again we see the majority</text><text start="4056.88" dur="3.959">is being batch</text><text start="4058" dur="4.079">um uh fit and and getting the scores out</text><text start="4060.839" dur="4.361">um</text><text start="4062.079" dur="3.681">similarly we see the same result being</text><text start="4065.2" dur="2.48">the best</text><text start="4065.76" dur="3.92">i&amp;#39;ll just notice that for this for this</text><text start="4067.68" dur="3.679">small task doing it on the cloud took 20</text><text start="4069.68" dur="4.32">seconds</text><text start="4071.359" dur="4.48">uh doing it locally for me took um 80</text><text start="4074" dur="2.64">seconds so that&amp;#39;s a four-fold increase</text><text start="4075.839" dur="3.041">in performance</text><text start="4076.64" dur="3.6">on a very small task so imagine what</text><text start="4078.88" dur="3.12">that does if you can</text><text start="4080.24" dur="3.839">take the same code as you&amp;#39;ve written</text><text start="4082" dur="3.839">here and burst to the cloud</text><text start="4084.079" dur="3.441">uh with with one click or however</text><text start="4085.839" dur="3.76">however you do it um</text><text start="4087.52" dur="3.68">i think that that&amp;#39;s incredibly powerful</text><text start="4089.599" dur="2.561">and that the fact that your code</text><text start="4091.2" dur="3.76">and what&amp;#39;s happening in the back end</text><text start="4092.16" dur="4.96">with dusk um generalizes immediately to</text><text start="4094.96" dur="4.08">the new setting of working on a cluster</text><text start="4097.12" dur="3.76">i personally find very exciting and if</text><text start="4099.04" dur="3.279">you work with larger data sets or</text><text start="4100.88" dur="3.12">building larger models or big hyper</text><text start="4102.319" dur="3.52">parameter sweeps i&amp;#39;m pretty sure</text><text start="4104" dur="3.839">um it&amp;#39;s an exciting option for all of</text><text start="4105.839" dur="4.88">you also um</text><text start="4107.839" dur="4.4">so on that note um i&amp;#39;d like to reiterate</text><text start="4110.719" dur="2.64">james what james said and thanking you</text><text start="4112.239" dur="4.08">all so much</text><text start="4113.359" dur="3.92">for joining us um for asking great</text><text start="4116.319" dur="2.801">questions</text><text start="4117.279" dur="3.601">and for bearing with us through some</text><text start="4119.12" dur="3.52">some technical technical hurdles</text><text start="4120.88" dur="3.6">but it made it even even funner when</text><text start="4122.64" dur="2.8">when we got up and running uh once again</text><text start="4124.48" dur="2.799">i&amp;#39;d love to thank</text><text start="4125.44" dur="3.759">mark christina and and the rest of the</text><text start="4127.279" dur="2.56">organizers for doing such a wonderful</text><text start="4129.199" dur="3.361">job</text><text start="4129.839" dur="4">um and doing such a great service to uh</text><text start="4132.56" dur="3.279">the data science and machine learning</text><text start="4133.839" dur="5.121">community and ecosystem worldwide so</text><text start="4135.839" dur="3.121">thank you once again for having us</text><text start="4140.159" dur="4.241">thank you hugo and james um i have to</text><text start="4143.279" dur="2.641">say like with all the technical</text><text start="4144.4" dur="3.04">difficulties i was actually giggling</text><text start="4145.92" dur="3.759">because it was kind of funny um</text><text start="4147.44" dur="3.44">yeah but we&amp;#39;re very sorry and we thank</text><text start="4149.679" dur="4.321">you for your patience</text><text start="4150.88" dur="6.16">and sticking through it and um</text><text start="4154" dur="6">i will um be editing this video</text><text start="4157.04" dur="3.6">to um you know make it as efficient as</text><text start="4160" dur="3.839">possible</text><text start="4160.64" dur="6.48">and have that available tim supercard</text><text start="4163.839" dur="5.281">thank you um great and i&amp;#39;ll just ask you</text><text start="4167.12" dur="3.52">if you are interested in checking out</text><text start="4169.12" dur="2.4">coiled go to our website if you want to</text><text start="4170.64" dur="4.24">check out our product</text><text start="4171.52" dur="4.56">go to cloud.coil.io we started building</text><text start="4174.88" dur="3.12">this company in february</text><text start="4176.08" dur="4.32">um we&amp;#39;re really excited about building a</text><text start="4178" dur="4">new product um so if you&amp;#39;re interested</text><text start="4180.4" dur="2.959">reach out we&amp;#39;d love to chat with you</text><text start="4182" dur="1.839">about what we&amp;#39;re doing and what we&amp;#39;re up</text><text start="4183.359" dur="3.44">to</text><text start="4183.839" dur="12.4">um and it&amp;#39;s wonderful to be in the same</text><text start="4186.799" dur="9.44">community as you all so thanks</text></transcript>