{"presenter": "Rebecca Kelly", "title": "kdb Time Series Database", "year": "2020", "transcript_md": "11-rebecca-kelly-kdb.md", "meetup_url": "https://www.meetup.com/nyc-data-umbrella/events/271984379/", "yt_video_id": "92YticvZzCM", "slides_url": "N.A.", "repo_url": "N.A.", "notebook_url": "N.A.", "transcriber": "Coretta Johnson", "extra_references": "N.A.", "video_href_w": "25%", "video_href": "http://www.youtube.com/watch?feature=player_embedded&v=92YticvZzCM", "video_href_src": "http://img.youtube.com/vi/92YticvZzCM/0.jpg", "video_href_alt": "Bkdb Time Series Database", "formatted_transcript": "### Intro\n\nHi everybody, welcome to Data Umbrellas\nwebinar. I'm just gonna go over a brief\nintroduction. \n\nSo I'm gonna introduce Data\nUmbrella of Rebecca Kelly is going to do\nher talk and then you can ask questions\nin the Q&A tab or you can ask in the\nchat and I'll just loop the questions\nover to the Q&A tab and depending on how\nthe questions come in we can sort of I\nmight interrupt Rebecca if it's a\ngood time to interrupt her but we'll get\nthe questions answered and this webinar\nis being recorded.\n\nAbout me I am the\nfounder of Data Umbrella. I'm a\nstatistician by training and a data\nscientist and I also organize for the\nNew York City chapter of Py ladies and\nI'm on Twitter @reshamas. \n\nThe mission of\nData Umbrella is to provide a welcoming\neducation inclusive space for\nunderrepresented persons in data science\nand we are a volunteer run organization.\n\nPy ladies is a international group of\nPython ladies and gender minorities\nand you know it's basically got Python\nand all all things related to Python\ncheck up check out our homepage and\nfollow us on Twitter. \n\nI just want to go\nover our code of conduct we're dedicated\nto providing harassment free\nprofessional experience for everybody\nand please keep that in mind any of the\nchat messages as well. \n\nI took a screenshot of some of the the website\nfor Data Umbrella and it has a lot of\nresources and those resources include on\nopen source on sources for learning\nPython and are about accessibility and\nresponsibility and data science and I\nencourage you to check it out. \n\nSo for any upcoming event or data umbrella the best\nplace to find them is on the meetup page\nso if you just want to become a member\nthat's really the best place. We do also\nshare them on Twitter and LinkedIn\nand Facebook so depending on the\nplatform the choice that's the if you\nfind out what we're up to\nvia those social media avenues. \n\nAnd I'm going to turn this over to Rebecca\nand let Rebecca sort of introduce\nherself and provide information about\nher background as well. Great. Thank you,\nvery much. \n\nWell I just take over the\nscreen there okay okay so can everyone\nsee my screen? Here let me just.\nOh actually have lost the questions now\nthat I'm in this view. Let me think.\nIs there a way?\n\nOh maybe I can pop this out. Okay we'll\ndo that. Can you see this on my screen?\nOkay okay because I can. I can read the \nquestions and that will be fine. Okay.\n\nMaybe if you can interrupt me if there's any questions\njust because I don't think I'll be able to see them.\nApologies. Okay no problem.\nThank you. Thanks appreciate it.\n\nUm so hello everybody I'm Rebecca. I'm\nvery happy to be here talking to you all\ntoday about Kdb+. I work in New\nYork myself so I thought there was a lot\nof people in from from local to me so so\nthat's great.\nYou can you can find me on LinkedIn.\nAfter this if you'd like if maybe we can\ngrab a coffee whenever that's socially\nacceptable again so I'm based in New\nYork and I work as a Technical\nEvangelist 4kx.\n\nI've worked with the company for I was\njust actually chatting with this work\ncompany for about five years started as\na developer and Kdb+ worked my way up to\na Solutions Architect left went back to\ncollege did a master's in machine\nlearning in the University of Edinburgh\nin Scotland which was great and then I\ncame back to work in New York as the\nTechnical Evangelist so I've had a bit\nof a journey kind of in between then\nwith the with the company I worked in a\nfew different places\nlocation-wise which was which was fun\nand I'll actually get into that a little\nbit when I talk about some more about\ntech. \n\nSo the way I've approached it today.\nI did obviously you know look around and\nI saw that the mission statement for\nData Umbrella is to provide a welcoming\nand educational space so really tried to\nfocus on putting as much educational\ncontent as I can in into this\npresentation. So I'm hoping to provide a\nvery good grounding on you know what is\nKx as a company. What does the technology\nlook like? Who uses this. What's the user\nfor? And then what does that mean for you?\n\nBefore I jump into the demo and the demonstration will\ndo my absolute best to cover as much as\nI possibly can of this language which is\nquite quite broad.\nSo hopefully you'll come away with a\nvery good understanding and knowing\nwhere to go for more resources.\n\nSo Kx is a division of First Derivatives Kx is a\nsoftware company and are the software\nthat we produced is a high-performance\ntime series database. So we've actually\nbeen around for quite a number of years\nnow we started in the finance space. So\nkind of you know FinTech and really kind\nof focused on the problem of how to deal\nwith big data. \n\nSo when people talk about\nbig data they tend to kind of split it\ninto these the V's so the the four V's\nthere's a volume, velocity, variety and\nveracity is typically the the kind of\nprimary four that I think IBM force\nproduct and if it helps you to think\nabout big data in that way.\nThen you know the place that we work in\nwould be the the high volume and high\nvelocity space so anywhere where there's\nhuge amounts of data coming at you\nvery very quickly is where we would\ntypically where our technology is most\ncommonly used. \n\n### Other Verticals (6:54)\n\nWell we did start in\nfinance and that's not that's not the only\nplace we operate now we've actually been\ndoing a lot of work in other verticals.\nSo some of the ones that we've probably\ngotten the most traction with would be\nIOT and manufacturing so I've kind of\nkept the focus on finance for a lot of\nthis presentation because it being New\nYork certainly for all of the people\nthat I work with in this in this place\nand this time zone it tends to be a lot\nof finance related stuff but like I said\nwe are kind of working in these other\nverticals and it's actually really\ninteresting to see the the difference and\nthe similarity between the datasets. \n\nSo if you think about finance\nin one way in which it will be very\ndifferent from something like IOT or\nmanufacturing is the the pickiness of\nthe data if that makes sense\nyou know if the markets are busy you get\na very busy day you have very very high\ndata volumes.\nWhereas in IOT or manufacturing you tend\nnot to encounter that same distribution\nlet's say of the data. The way in which\nthey might be very different is that\nwith finance you can get downtime you\nknow there's those times when you're not\nyou know so like on weekends the markets\nmight not be open\nwhereas in IOT or manufacturing it's\npretty much 24/7 but they are united in\nthat they have tend to have you know\nhuge volumes of data and very fast\nvelocities. So you're talking about\nsensor readings every every like\nmillisecond coming off multiple multiple\ndevices and kind of aggregating that\ntogether which is not so dissimilar to\ncapturing all of the the data from the\nmarkets and trying to action that in the\nin the finance space. \n\nSo just to lay a\nlittle bit of the scene the technology\nso anybody who might already be familiar\nwith a little bit the technology will be\nmay have heard people refer to it as\neither kdb+, kdb or q so this is my\nlittle infographic you'll see I'm quite\nfond of pictures to tell to to tell\nstories.\nIt is this and this to me represents how\nyou can think of the software. So Kdb+\nis the database layer that's where the\ndata is stored on disk and then q is the\ninterface around it. So that's a\nturing-complete programming language\nthat you use to actually query and\nretrieve the data from the database and\nthat's the language that I'll be showing\nyou in the demo in a moment. \n\nAgain more effective because pictures tell a\nthousand words. I'm gonna break down I\nsuppose some of the key features of the\nlanguage and in a way that I hope will\nhelp understand how it might be different to\nor similar to other languages that\npeople have worked with before. So it's a\nfunctional language. That means that you\nknow there's no it's not right compile\ntest you're you're actually working\ninteractively with the data similar to\nPython in that sense and that you're you\nknow you can start with the problem you\ncan code around this you can work your\nway through it and it's certainly I\nspend a lot of my time talking to the\ndevelopers who actually use the language\nand this is one of the things that I'm\nthat that they most kind of appreciate\nabout it because you know if you're\ntalking about working with big data and\ntrying to work through a problem it's\nactually quite beneficial that the the\nspeed of execution is it's short you\ndon't have to go and take a big tea\nbreak every time you want to to run some\ncode for example.\n\nIt's a columnar\nbase to database so if you're familiar\nwith other databases like MySQL for\nexample those are typically although\nit's becoming more common but they're\ncolumnar base but those are often row\nbased databases. The main difference I\nsuppose is how the data is retrieved so\nif you're working with a row based\ndatabase and anytime you want any piece\nof information in that row you have to\npull the entire row back into memory but\nif you think about big data and how\npeople normally work with or use big\ndata you're not you know it's rare that\nyou would say give me all of the data\noften you you're you know you're trying\nto filter is you're trying to work with\na subset you're trying to kind of\naggregate it to all those things. So it's\nvery common that you would do something\nlike I would like all the data on this\nday's for the Apple stock for example. \n\nSo what they columnar new database that means\nyou can kind of go to so that you know\nyou can go to the particular date that\nyou care about you can go to the you\nknow the the the stock name and you can\nkind of filter based off of those\ncriteria and then only pull back the\nrest of the data for the rows where\nwhere it's actually met your filter. \nSo it's a much more memory efficient way \nof trying to work with these bigger data sets. \n\nIt's very fast and yeah I like to throw this one\nand I upgraded my car from the first\nversion because we actually do work with\nawesome marks marking Red Bull Racing. So\nif you kind of think about the analogy\nbetween the the two cars that's kind of\nthe the performance scale that we're\nlooking at it's concise of a language. So\nthat's what this little target is trying\nto indicate that doesn't really that\nsounds kind of like a throwaway comment\nat first but I suppose the difference is\nyou're not writing pages and pages pages\nof code to achieve a particular result\nit's often a case of you might write one\nor two lines and that means that you\nknow you you spend more time thinking\nabout what it is you're trying to do and\nwhen it comes back to you know\nsupporting it or actually maintaining\nthat code you're working with a much\nsmaller kind of code sash which is often\nquite beneficial for for for these kind\nof systems. \n\nThis is the table. Sometimes\npeople don't know what this is. Tables\nare first-order data type in the\nlanguage. So it's a valid statement to\nsay table 1 plus table 2 and that will\ndepending on what you have in your\ntables work but this is really to\nemphasize the fact that the language and\nthe software grew up being focused on\nthe problem of data as not just kind of\na secondary consideration but as the\nthe primary motivation for the software.\nSo so tables are really front and entered.\n\nThis is time and this is this is really\nabout I suppose the fluidity of it and being\nable to kind of work between different\ntimes so you know data is is a difficult\nand complicated but but things don't\ntend not to necessarily coincide a lot\nof the time if we're talking about fine\nto markets you know like when you get a\ntrade you don't necessarily have a quote\nfor that trade at the exact time of that\ntrade. So you need to do you need to kind\nof recreate the market context by by by\nby retrieving the the quote that was\npresent in the market at the time of\nthat trade they've gotta be very common.\nFor example for a lot of the people that\nuse our software they might use it for\nquality of execution reporting that's a\nvery big one. So they'll they'll they'll\nneed to know you know did was this the\nbest possible trade price that could\nhave been achieved have we done the best\nthat we possibly could do for our for a\ncustomer that we're executing on behalf\nof so there are a number of joins in the\nin the language that are specific to\ntime that are based around that. \n\nI will show you the As of join when we get to\nthe demo but I suppose to kind of go the\nmore generalized side of things there\nthere is a window join in there that\nwould let you say for every row that you\nhave in one of your tables you can\ndefine a specific period of time around\neach of those records and join that\noriginal table with information from\nanother table so for example if you had\na sensor reading you could set a window\nof time maybe you know ten minutes after\nif maybe five minutes before and seven\nminutes after whatever you like and\nretrieve from another table let's say\nlike the temperature that that was\npresent in the room at the time for that\nfor that sensor during that time period.\nSo it's really all about kind of context\nand time is I think the the ultimate\ncontext without without running the risk\nof sounding far to um introspective\nbut a but that's that's the idea. \n\nWindowing is the you know bread and butter\nworking with big data you know \nyou have so much data you need to\noftentimes you know work with it and\nthese in these kind of smaller time\nperiods being able to kind of\nchop and slice and dice all the data\nthat you're working with. So windowing\nis important.\n\nThis is about aggregation\ndata aggregation data filtering all of\nthose good things again Big Data so you\nknow if you're talking about petabytes\nof data you're not going to have\nsomebody there looking at it row after\nrow because that's just not feasible.\nIt's not a good spend of anybody's\ntime. So it's all about being able to cut\nto roll it off into the format that you\nthat you want to see it in the kind of\none of the statistics that you care\nabout on the on the subsets of data that\nyou care about and and kind of continue\nfrom there. \n\nSo this is actually a little\nbit of an older reference maybe then\nthen people might be familiar with but\nthis is a lambda here and it refers to\nkind of the original concept of a lambda\narchitecture. So that's being able to\ntake the the data from very first kind\nof point of ingest to being able to work\nwith it in real time and then finally\nbeing able to work with it in its wider\nhistorical stage. So that whole kind of\nidea of being able to to seamlessly deal\nwith the data in whatever format it\ncomes in and that's something that I'll\nthat I'll show to you in the in the demo\nhow as a language it allows the\nflexibility to to do that to work with\nthe data and kind of whatever format it\nis you're getting it in. The benefit of\nthis as well is that you know you're not\nwriting three different sets of code if\nyou have a particular function that you\nwant to use with data that you want to\napply to the data you don't have to\nwrite something special for for if you\nif you're working in in real time\nanother for in memory another for on\ndisk and maybe another for streaming but\nthat's not something that that you have\nto to worry about. \n\nThis is it looks like\na waterfall. It's supposed to be\nstream. I wanted to highlight the the\nstreaming capabilities because I think\nfor people who already have a good range\nand breadth of languages as I imagine\nmay well be the case with this with this\ngroup of people here it is certainly\nworth highlighting the the streaming\ncapabilities of the language because I\nthink it's a it's really a place where\nwe excel and where maybe there's not\nalways a great fit with some other\nlanguages. \n\nSo I've said a few times about\nhow great it is and obviously I would. So\nthis is just to say you don't have to\ntake my word for it and we do\nparticipate in benchmarks so to\ncontextualize the benchmarks if you if\nyou recall I was saying that the the\nthat we started in finance so that I\nsuppose as a software we're a little\nover I think we celebrated our\n25 years maybe a few years ago so that\nwhereby we kind of came in to the the\nmarket as I suppose high frequency\ntrading and all that stuff was on the\nrise and the situation that a lot of the\nfinancial institutions at the time faced\nwas how do I decide what software to use\nfor this problem that I have and\nparticularly when everyone is going to\ntell me that their software is the right\nsoftware to use so stack is a if it's a\ngroup an independent body that kind of\nsat down with all these financial\ninstitutions and said well what do you\ncare about we'll define appropriate\ntests to to to track all of these things\nand then that will help you be able to\ndecide which you know which which\nsoftware fits your needs the best. \nSo for us the ones that we tend to really\ndominate in and where we I suppose see\nthe most use both in our customers as\nwell is in the in-memory compute and\nmass of data at rest so I've kind of\nexpected. So that so that's that's just\nto say you don't have to just trust me.\n\nHere's a quick a little bit of a\nwhistle-stop tour of some of our clients\npartners. I whoops go back. It's running\naway from me. um I've had the fortunate\nbenefit of working with a few of these\nso up on the top right hand corner there\nASIC\nthat's the Australian Securities and\nInvestment Commission and it was\nactually one of the first projects I got\nto work on when I joined the company. It\nwas great. They sent me to Sydney. I loved\nit.\nThe weather was amazing all the time\neveryone was lovely but but yeah. \n\nSo so that was a project where it was the\nAustralian Securities and Investment\nCommission who are charged with\noverseeing the entire market really. So\nthey guess all the market data it's such\na great place to be. If you're if you are\nyou know if you're someone like me who\njust enjoy seeing that full picture\nand we were working on market\nsurveillance. \n\nSo the company chaox we\nbuilt we brought in a um and built a market\nsurveillance system. So we would take all\nthe data in real time and we were\nlooking for abusive market practices. So\nthere's a number of different the standard \nkind of alerts that you\ncare about in that space. So whether\npeople are layering insider trading all\nof those things had to be kind of\ncodified and then um and then applied to\nthe data. So that was great.\n\nI've also gotten to work with the\nawesome Martin Red Bull racing when that\nwas first kicking off I was involved in\nthe proof-of-concept which was which was\nvery fun. I got to go out and actually\nsee the car in the wind tunnel and work\nwith work with some of the team there\nand that was that was very very\ninteresting. There is an awful lot that\nactually goes on. In in in the in the\nFormula One racing world and a lot more.\n\nI suppose there's not really regulation\nbecause it's not a government body\nregulating them but they kind of group\nof all the people that work in that\nspace or the different Formula One teams\nkind of bands together and put a little\nbit of\nthey set their own regulations on and\nit's very it's very interesting. Like\nlike for example they have a limit to\nhow much compute time they can run. So so\nthey can gather the information from the\nwind tunnel but then they're limited\nwith how much time they can spend\npunching those numbers because they all\nbrought together and decided that this\nwas the good number that they were going\nto say. \n\nSo however many hours um\nso being able to run things faster\nobviously meant that they could get more\nactionable insights and it was just an\nit was an interesting case that we. I\ndon't know that on paper anyone would\nreally have expected as a good as a you\nknow a potential vertical but now we're\nvery active in the automotive space and\nthen I know a lot of these others from\nfrom just my work kind of going around\nand talking to the different teams that\nare based here in New York and and just\nchecking in on them and how they're how\ntheir systems are running and keeping\nthem up-to-date with all the latest\nfeatures and all that. \n\n### Finance Use Cases (24:19)\n\nSo how is it used.\nI figured. I put together a little word\ncloud to hopefully kind of highlight\nsome of the ways that these clients are\nusing else. Um so let me see strategy\nback testing for example would be a very\ncommon one for people who are who are\nusing us as a very big historical\ndatabase so the you know you have\npetabytes of data you have a new trading\nstrategy but you need to test it before\nyou deploy it so this is that'd be\nfairly common a lot of analytics around\nyou know a post trade pre-trade quality\nof execution and just just in general\nfor kind of ad-hoc analysis and and and\ndatabase to database things. \n\n### What about you? (25:07)\n\nSo I thought\nabout what you know who all of you are\nin this audience and what might what\nmight make you interested in learning\nthis software and I suppose the the one\nthing that I would highlight is that it\nis so frequently used in the investment\nbanks\nand we have a lot of big hedge fund\nclients. So if you're somebody who's\nlooking to try and get more like\nmaneuver into a position where you're at\nquant or strat or financial analyst this\nis a great technology to pick up to try\nand differentiate yourself from maybe\nmaybe other people that are also\napplying. \n\nIt's also like I said I find it\nparticularly useful with streaming data\nand again in terms of the effort being\nworthwhile for you rather than just\npicking up another language where you\ncan write hello world. Here's something\nthat might actually help you be able to\naddress different kinds of problems and\nyeah it's it's very interval. \n\nWe have a whole team called our fusion team who\nare dedicated to putting together these\ndifferent adapters so that we play\nnicely with other technologies. Look the\nworld is a big wide interesting place\nand there's so much technology and and\nreally it's used the best tools for the\njob. You know for you know we're not\ngoing to rewrite backprop in our\nsoftware you should keep using probably\nPython and and and and and you should\nhave our freedom that should be\nsomething that you get to decide. So it's\nreally it's really about you know doors\nopen come on in and and the\ndemocratization of data really.\n\nAnd then finally I think time. The most important\nwell in my opinion the most important\nresource that that I think we have as\npeople is our time and you know you want\nto make the most of it. So this is this a\nlittle infographic I got from Forbes\nthat kind of breaks down what data\nscientists typically spend their most\nI'm doing and this three percent all the\nway around to the end of this nineteen\npercent so a little over 80 something\npercent quick math is spent on working\nwith the data getting the data into the\nformat that you wants but so then you\ncan kind of do the quote unquote\ninteresting\nwork of you know building building your\nmodel testing algorithms doing whatever\nit is the kind of the value-added step\nof your processes. So the faster you can\nget through that in theory the the more\nvalue you can be kind of continuing to\nproduce. So it's a it's certainly\nsomething that's I like to you know try\nand try and speed up as much as possible.\n\n### My Pet Peeve (28:11)\n\nThis is this is because I have a forum\nso this is where I normally take the\ntime to talk about it just kind of a\nlittle bit what annoys me and mostly\nit's something that I see a lot and that\nI think you know whatever software\nyou're using whatever it is you're\nworking with you know they're very kind\nof good and bad practices and something\nI see is that people will go to the\ndatabase it you tend to gathered a lot\nin these kind of fractured environments.\n\nSo in the top panel here what I see\nsometimes is that there is a database\nadministration team and then there is\nthe data analytics team and they\nmaybe you know I mean they talk to each\nother they're fine everyone gets along\nbut if you want data to use for your\ndata analytics what happens is you get\nyour extract from the database but\nthat's a database administration team\ngives to you they put it somewhere on\ndisk and you work with that you work\nwith that extract and that's that's kind\nof just what you have but as a data\nscientist I find that very frustrating\nbecause I'll you know you like to be\nable to test against other conditions\nagainst other edge cases and in other\nsituations. So like I might have taken\nall the information for Apple but now I\nwant to check for Microsoft and now I'm\ncaught in that in a two-week\nrequest/response\nsituation and it it it annoys me and\nalso from from a database administration\nperspective this isn't great either\nbecause you're you know you're losing as\nsoon as you take the day that out of the\ndatabase and you put it somewhere else\nusing data governance you know that that\naudit trail for who access that data is\ngone.\n\nSo my recommendation and though it\ndoesn't have to be our database but you\nknow the best thing you possibly do to\nreally empower people to do the best job\nthey can is to give people that freedom\nto to work directly with the data and to\nto be there with it. So I will get off my\npodium now and just move on. \n\n### Python (30:26)\n\nSo I'm aware\nthe audience here is is quite familiar\nwith Python so I wanted to particularly\nhighlight the Python interfaces that we\nhave. So we have two and the one that I'm\ngonna show you in the demo is going to\nbe this one on the left-hand sides that\nwe call embed py so that's where it's\nkind of what it looks like you'll see I\nreuse this pictographic technique of\nputting putting the thing in but but in\nfact but that's what it is it's a it's a\nq process that's running that has a\nPython process in better than to it. So\nthey share the same memory space and\nit's really very efficient and then py q\nis the flip side so it's a Python\nprocess that's got q embedded in the\nsame memory space. So the difference\nwould be the prompt that you're working\nwith. So with embed py the base language is q\nand with py q did the said language is py q \nand that's the main difference.\n\n### Questions (31:22)\n\nRebecca. Yes. This may be a good time to\nask this question from.\nYeah go for it.(Questions)\nWould love to know the reason why q was written.\nWere there some time series data access not supported\nby SQL.\nYeah that's a great question and yeah I\nthink that was effectively it. I think\npeople got tired of trying to figure out\nhow to how to stitch the data back\ntogether to get that context so I I'd\nmentioned the half of join. I'll show it in\nthe demo but the idea is that you can\nhave one table like your tray table and\nhave all of those different times I wish\nall those trades occurred and you can\njust say oh give me the quote table as\nof the time in the tray table now I know\nthere have been\nthat some of these capabilities have\nbeen subsequently added to some of these\nrow based ones but but that you know\nperformance wise you know it just wasn't the same\nif you're trying to make a decision\nabout you know how much credit do you\nwant to extend somewhere that's not\nsomething that you really want to to\nleave waiting for too long you know.\nThese are the kinds of things that can\nend up hurting you financially that\nthe time horizon of when you can get\nthose answers and as well at the ability\nto to to chop up the data and bucket it\nappropriately is is also very kind of\nfundamental to the language as well and\nI think that that has been one of the\nreal drivers too but yeah it was it was\nkind of a two-pronged thing it was being\nable to provide the kind of common\nanalysis techniques that that that that\nare native in the language now such as\nthe the bidding the aggregating the the\ntime contextualized joins and then also\nit's the the speed at which you could\nperform those operations we're kind of\nthe two driving factors behind it so how\ncan we do these things firstly and\nsecondly how can we do it at a speed at\nwhich this will be valuable you know\ninformation still because any\ninformation has a has a maybe not maybe\nnot may not all information now that I\nthink about it\nbut a lot of information has a has a\nfinite value horizon and certainly in\nfinance that's stuff that can be quite\nshort hopefully that answers the\nquestion. \n\nAre there anymore while\nactually well well we're kind of pause?\nNo that was the only question.\nOkay okay well hopefully that answers\nand all continue on yeah. \n\n### Machine Learning (34:06)\n\nSo python has\nbeen in each place that we've\nparticularly focused on because of the\nhuge growth in machine learning and AI.\nWe have a team ourselves in London that\nare exclusively dedicated to machine\nlearning and AI and how how\nwe as a software can you know provide I\nsuppose the most utilities and guidance\nand and benefit to to the users of our\nsoftware in that space\nlike I said we're not planning on\nrewriting backprop and and really the\nthe most fundamental step I think in in\nterms of making that available to our\ncustomers was you know let's let's get\nan easy way for people to work with\nPython and actually you'll see when I\nget to the demo now I think in a second\nthat I'm going to use the Jupyter\nnotebook can a format that hopefully a\nlot of people here will be familiar with\nand this is me.\n\n### Live Demo (35:07)\n\nLive Demo hopefully hopefully not everything.\nHere we go.\n\nSo okay. I've got a Jupyter instance\nrunning and I actually have the ability\nto to run q you so there's a q kernel\navailable for Jupyter for those people\nthat are interested in maybe getting\nthis set up after and when I when I go\nback to the slide I'll talk a little bit\nmore about that but we're available on\nan Anaconda Anaconda or Anaconda and I\ncall it the comic-con is the conference but yeah\nwe're available via Anaconda and I think\nactually in the resources that I that I\nsent on that that I think was linked in\nthe United States is the the link to\nembed py and it goes through the\ninstallation and setup and there's an\nand yeah hopefully that'll arm help you\nafter this if you want to try reproduce\nsome of this.\n\nSo the demo that I'm gonna\ngo through I'm gonna try and give you\nthe biggest whistle software I possibly\ncan of the language. So I'm going to show\nyou, I suppose the basics of it other as\nkind of a column based language and\nreally being kind of fact oriented it's\nquite like numpy or an umpire whatever\nway people want to say it and then you\nknow when it comes to the table querying\nI suppose again with the kind of Python\nanalogy it's it\nyou know I I actually do a comparison\nbetween a query and in pandas versus in\nin our language we actually have an\nsql-like syntax that we refer to as q\nSQL for for that.\n\nI'll show real time so data that like\nworking with data and memory working\ndata on disk and streaming data and then\nfinally the pipe and interoperability. So\nit's very ambitious but what but\nhopefully it'll be a good introduction.\n\nSo I was saying that it's like numpy it\nis it's it's very much vector based so\nin this cell here all I'm doing is\ncreating a vector or an array that I'm\ncalling a and then I'm just adding 10\nand you can see that all the operations\nare are kind of pairwise out of the out\nof the box in this case I'm taking the\nthe kind of the vector b and the vector\na and I'm adding them together and\nthat's just kind of automatically\nworking pairwise so that's that's pretty\nneat and useful in a lot of ways but\nmoving on actually getting our hands on\nsome data. \n\nI have pre-loaded some\nsimulated trade and quote data. So this\nis what my quote table looks like. I've\ngot my symbols the time and the\nstandards kind of quote information and\nsimilarly I have my my trades. They're a\nparadigm version because we don't we\ndon't need a lot to kind of go through\nthis. So in terms of how much data I'm\nworking with I've got my 5 million in\nquotes and 1 million in trades. I also\nbecause I'm running this with embed py\nI've also got that same data in Python\nso that I can hopefully show a little\nbit of the the kind of side-by-side\ndirect comparisons. So just to prove\nthey're the same here we go.\n\nThe good thing is because we're working\nin this Jupyter notebook environment we\ncan actually we can run some cells in q and\nsome cells in Python\nby using these neat little magic\ncommands. \n\nSo I'm going to recreate a very\ncommon requirement for for financial\ndata which is to create a volume\nweighted average price referred to as a\nvwap broken down for each of my different\nstocks so normally people would kind of\ncreate this at the end of the day as an\nindicator to use for some for some\ndifferent models. So if I do that in\nPython you can see it looks like this.\n\nNow obviously, I'm very aware I'm talking\nto a group of people who are quite\nPython literate so any suggestions on on\nrefactoring this are very welcome but\nthis was this was my attempt at doing\nthis in in pandas and here is the\nequivalent in q. So you can see the\nsql-like syntax. So select the columns\nwe care about broken down by the\ncolumns we want to break down by from\nthe table we care about and to show some\nof the. So these items that are\nhighlighted in green are some of the key\nwords and the language. So they kind of\ndo as you as you would expect. I don't\nthink there's anything here that I\nthought that people are struggling with\nbut if there's any questions do you just\nchat wavg is weighted average. \n\nSo yeah so it's very easy to kind of get that\nsame breakdown and just to prove that\nthose are the same and a more\ncomplicated thing that I didn't even\nhonestly attempt to do in Python but it\nbut if somebody knows this please do\nsend me on the code is a time weighted\naverage price. So this is where we search\nto kind of again just highlight that\narea earlier questions that somebody had\nlike why why was through the need for\nthis. This is an example of something\nthat be very common to do that's using\ntime that maybe isn't as easy to do in\nother languages. So what we're getting or\ngenerating here is a time weighted well\nthis is actually a spread but it could\nbe it could be on thing is but its time\nweighted and it's broken down by\nand by stock and here's an\nexample of some of the aggregations so\nthis xbar function here is actually\nperforming pocketing so it's breaking\nout all the times into 30-minute\nbuckets and then this is the the time\nweighted average spread so so hopefully\nthat helps to kind of highlight or make\nit clear how kind of time focused the\nthe language in the syntax is but we do\nhave the the standard joints that you'd\nexpect. \n\nSo if we do have this kind of\nreference table like this info is just\ngiving us the full company names we can\nuse a left join and append that\ninformation on but more interestingly we\nhave this outer join so let me show you\nwhat this looks like, what I'm doing here\nis I'm joining the trade table with the\nquote table and I'm doing it where this\nsymbol is match exactly. So if I'm\nlooking at a trade and Apple I only care\nabout quotes and Apple and then my time\ncolumn is gonna serve as like a soft\nlookup. So I'll see if I can find an\nexample here in the in the in the like\ndirectly in the things okay. So if for\nexample here there was a trade that\nhappened in Apple at 9:30 and then\nall of these milliseconds later actually\nthat's another thing that we have native\nto the language that maybe not a lot of\nother databases support. It's just the\ngranularity of time that we support but\nyes so it happened at this time and\nyou'll see that this quote information\nthat has been joined on is actually from\na quote that occurred just like that one\none increment beforehand. So while this\nwas obviously very close in time that\ncould be much much earlier in the day if\nyou're looking at something that's maybe\nparticularly liquid and that's that's\nkind of the idea behind it being able to\nkind of bring that context in and say\nwell this was the quote and therefore\ntherefore this this works like that.\n\nSo just to show the tables. The benefit\nof being able to do this kind of\nanalysis. So now that I've made this\ntable tack that has my contextualized\ntrade data I can use that to perform\ndifferent analysis so like like I said a\nvery common one is around the quality of\nexecution. So I'm gonna go and I'm gonna\npull out all the cases where where that\nprice that I traded out wasn't within\nthe better ask. So this is something\nunusual that really should be justified\nbecause you know if if it's outside the\nbetter ask it you know there's a very\ngood chance that this wasn't optimal in\nterms of its execution so that needs to\nbe addressed for the clients. The neat\nthing about it being a programming\nlanguage is that you can put code kind\nof directly into your break downs. \n\nSo if you're used to maybe some other\nlanguages you know that the break downs\noften have to be kind of existing\ncolumns. So this would be maybe a\ntwo-step process you'd have to make the\ncolumn and then break down then do your\nbreak down on the kind of the next line\nworking with a new table but because\nit's a full programming language you can\nput kind of whatever you you want in\nhere to kind of get your breakdowns. \n\nSo here I'm deciding if it was a valid\ntrade or not by deciding by checking if\nthe price was within that bidder asked\nfrom the quotes. So these cases here\nwhere it's zero this is a boolean value\nare telling me that that's not true.\nSo there was 84 cases where trades\nhappens that were outside of the quote\nin this in this stock. So that is where\nif I'm the person charged with you know\nensuring quality execution that's where\nI would be starting to to look as my\nkind of basis. So this was a quick\nexample of the syntax I was working with\nin-memory data but now we're going to\njump to on disk data. \n\n### On-disk data (45:32)\n\nSo I have much bigger tables on disk now having\nsaid that I am just running off of this\nlet me see about this Mac this will tell you\nyeah so this is what I'm working off in\nterms of my system spec I'm not\nconnected to anything else in the\nbackend here.\nIt's just my own laptop and I have some\nsome data in a database that I have that\nI've generated that that's running\nlocally and in fact I've got about 95\nmillion quotes and 19 million trade\nrecords. So I can show you what they look\nlike you'll notice that these are schema\nwise this table is the same as it was in\nwhen we were working within a memory\nwith the exception of this new column\nhere which is the date which signifies\nthe date. \n\nSo normally in a kind of a\nstock standard take capture system using\nthe technology people would typically\naccumulate the data intraday in the in\nthe in memory table and then when it\ncomes to the next day they will they'll\nbasically purge that right that I disc\nand then start fresh. So just to to add a\nlittle bit of architectural context to\nit but we can do the same things with\nthis data that we have on disk as we did\nwith the in-memory data so we can get a\nbreakdown of of how many this count i is\njust going to tell us how many records\nwe have for each day you see the syntax\nisn't really any different and we can\nactually run let's run a so another\nthing that people would normally do at\nthe end of the day is you kind of create\nsome summary statistics or you know from\nthe from the trading day that just\noccurred and it's very common for for\ncandlesticks to just create the they\nopen high the low and the close along\nwith this vwap here so we can just\nbill that on the fly. So just a reminder\nagain that we're working off of the I\nthink 19 million records or something\nand it doesn't you can see it doesn't\nreally take an awful lot of time to to\nwork with this which is which is nice so\nnow that we have\nthis data in a daily format this is\nexactly the kind of data that we might\nwant to feed in to a model if you were\ntrying to to have a look at you know\nbeginning to understand the the market\nbehavior and where things are. \n\nSo I can\ntake the actual the series so the the\nthe end of day vwap for each stock and\nextracted from that table and get us so\nthis is a dictionary so it's a key value\npairing of of the the key being the\nstock and then the value being the the\nprice series for each day from the daily\ntable if I want to just see it for one I\ncan do that so this is what the Apple\nprice series looks like getting again to\nthe statistics of it we can look at how\nthat correlates with Microsoft.\nWe can also I'm missing bit of a\nline we can check Apple against every\none with less code. This is actually this\nthis is kind of our version of a it's\none of our versions we have a few\ndifferent ways to do this but it's what\nwe call an iterator in the language and\nit's kind of it's our way of doing loops\nyou know you don't really write for\nloops in the language you you iterate over\nthings so I'm so what we're doing here\nis we're we're court we're checking the\ncorrelation of Apple with each item to\nthe right that's what that's doing which\nis our price series for each to the\ndifferent stocks and then finally we can\nactually go even further and check the.\n\nSo this is looking a little funny\nbecause of my formatting but this is a\nthis is the table if I see my for a\nsecond. Oops.\nYeah so there you can see it looking a\nlittle bit more like the table but I\nwill I will go back in for free ease of\nreadability now but just a highlight\nthat this is less code than when I want\nto just do it for for the two individual\nones but yet I'm doing more so it's\nhopefully a little bit of a peek\nthe kind of elegance of the language so\nto speak. \n\nHmm if there's no questions or\nnothing so far I'm gonna jump into\nstreaming and this were. Oh sorry.\nThere is one question. Oh great. Okay.\nA question from Aditi is q couples\nwith Kdb or can we possibly configure a\nrelational database we already have to\nwork with q. So there are different you might\nhave seen the kind of fusion piece I\nmean you can use it just as a\nprogramming language if that's all you\nwant to use it for for sure we have a\nnumber of different drivers so there's\nlike an ODBC driver there might even be\nlike one or two different versions of\nflash so there's one by Simba there's\nanother ODBC three one there's a JDBC\ndriver. So like if you're trying to work\nwith relational databases yeah you can\nyou can do that you can you can do your\nextract from those and bring it in to\nkind of to this space and actually the\nthe working in Jupyter has been quite um\nquite great for a lot of our of our\ndevelopers really because the. \n\nSo I'm\ncurrently got the q kernel running here\nwith embed py which means that I can\nkind of swap in these different code\ncells between the two different\nlanguages we also have an interface\nembed or so that's my accent can be a\nlittle heavy but that's or for Romeo if\nwe're going to go fanatic with it. So I\nmean the programming language. So if you\nhave both loaded into your q\nenvironment and you're working in\nJupyter you actually kind of have this\nthis great kind of polyglot environment\nwhere you can have a cell and or and\nhave a cell in Python and then swap back\nto q and I mean maybe maybe someones\nworst nightmare but but it's quite it's\nquite helpful if you're if you kind of\nlike to use a few different things like\nI said best tool for the job and all\nthat.\n\nA question. So q is a language and Kdb\nis a database. Is that right? Yeah. \nI mean people tend to use them kind of\ninterchangeably but but technically yes.\nSo Kdb+ is the is the database\nkind of so that historical data\nthat I was looking at would be written\ndown in in kind of that format. Okay.\n\nSo I'm going to let you go back to the\npresentation but I just want to add that\nJupyterCon is coming up. So if you want to\nsubmit a CFP on how you use Jupyter for your\nwork feel free to. We can talk about it later.\nOh cool. Thank you.\nYeah no. I do. I do enjoy it. I think I\nthink I think it's great it's a yep. I'm\ngonna start in a tangent now so I'll stop but\nyeah we should chat about it.\n\n### Streaming Analytics (52:53)\n\nSo yeah finally moving on to streaming.\nSo you we've already looked at this and\nI had the the five million quote 1\nmillion in trade. So I can do you know\nthe syntax that you saw before but what\nI'm going to do now is I'm going to\nsubscribe to some data in on the back\nend and you'll see that this is now\nchanging the number of rows I have in my\ndata is is changing as I get new data in\nto the system. \n\nSo I can still work with\nthat live data even though it's it's\nstreaming and calculate these kind of\nmetrics on the fly. So I think that my\ntotal count broken down by each stock\nfrom my quote table and you can see that\nthose aren't changing. This is where I\nnormally go into my architect mode and\ntalk a little bit about it good system\ndesign.\n\nSo there's a very big difference between\nstreaming analytics and kind of these ad\nhoc api's when it comes to what effect\nit has on the system. So I'll try clarify.\nThe quote table here that we have is\ngoing to be increasing as the day goes\non so every time you get a new set of\ndata that's appended on to the end of\nyour code table and that continues to\ngrow.\nSo that means that when you're running\nthis query that's going to take\ndifferent times to complete depending on\nwhat time of the day it is and even\nthough obviously it's a very fast very\nperformant language like you know this\ndoesn't take really any time to do if\nyou have a lot of people working on the\nsame system and I'm wanting to kind of\ndo these queries then you know that can\nsometimes become non significant and you\ndon't like that if you if you're\ndesigning systems you don't like\nunknowns you don't like not being able\nto you know have things behave in a very\nexpected way. \n\nSo what we do in the kind of\nKdb world is we we use\nstreaming analytics. So streaming\nanalytics are really just getting to\ndecide what you want to do with each bit\nof information you get. So if I get a new\na new quote message what I'm going to\nget I'm going to get a little mini table\nthat will have some amount of quotes so\nthat could be one row or it could be\nmany many rows it depends on kind of the\nupstream system but I decide what I want\nto do every time I get that message. So\nthat means that well actually I'll just\nshow you here so I'm gonna make a new\nstreaming function and it's going to\nhappen every time I get a quote message\nso this here the curly brackets are the\nis how you write a function in in in kdb\n/q and then this here is the\nparameter so I'm saying that this is a\nfunction that takes a parameter q and\nthen I'm writing the code for what I\nwanted to do. So I'm gonna create a new\ntable called qtotal that's going to for\nevery kind of message I get in for the\nquote table so that'll be a series of\nrows or one row or whatever I'm going to\ncalculate this so I'm going to get the\ntotal count broken down by each stock in\njust that message so that could be like\nthree Microsoft messages and one Oracle\nand to IBM that might be what came in in\na bundle and I'm going to them\nadd that on. So this is an example of\nkind of adding two tables really but\nthat I kind of talked about before I'm\ngoing to add that on to this qtotal\ntable so that every time I get a new\nmessage this is updated so the benefit\nof doing that just to show those two\nthings are the same now and they will\ncontinue to be the same is that this\ntable here that I've created this\nqtotal table is small it's got you know a\nfinite number of records because it's\njust based off the stocks I have it's\ngot a pretty well understood execution\ntime you know because I'm it's it's\npretty small it's not really going to to\nhold up anything but yes if everybody\nwanted to come and retrieve that from my\nprocess that's you know this is this is\nthis is going to take take the same\namout of time no matter what time of the\nday they put this in as opposed to the\nfirst query which will have you know\ndifferent different runtimes at the end\nof day versus the beginning.\n\nSo streaming analytics are just very very powerful in\nthat sense and that you can control the\nsystem in a very in a very I suppose\ndiscrete fashion and it does mean that\nthen you can that this is kind of a\nwhole framework that we use for for\nleveraging action and responses so if I\nyou know I can have something in here\nthat would check if you know if the\nstock is Microsoft and if the price is\nbelow this then I'd like you to go and\nmake a trade or or or do something else.\nIt's this whole kind of event trigger\nkind of paradigm that's that but that\nyou can then use which is which is\nreally kind of the powerful thing with\nwith streaming analytics in my opinion\nand I'm also just going to do something\nelse here I'm gonna I'm gonna add to the\ntray table a summary that's going to cut\nkeep a running vwap so keeping a\nrunning vwap actually kind of tricky\nbecause you're you know you've got to\nconstantly be carry carrying forward\nthat waiting and that's that's pretty\neasy to do in a streaming fashion believe\nit or not some things are obviously\nharder than others\nbut but it's it's it's really very\npowerful to be able to work with things\nincrementally. \n\nSo if you're interested at\nall in kind of capturing real-time data\nand maybe driving analytics or decisions\noff of it you know I it is very fun to\nplay with and it's pretty easy to kind\nof set up different different\nconnections. \n\nThe audience being what it\nis. I thought I'd also show some Python\nintegration because we because we all\nlike Python here so that's great\nso I'm gonna build a time series this is\na little bit more complicated than the\none I did before so rather than getting\njust the end of day price from my\nhistorical trades table for e or my\nvolume way to price over the day for\neach day I'm getting an even more\ngranular time series I'm going to get\nthe the kind of opening price in\n15-minute time buckets for each stock\nacross all the days in my in my database.\n\nSo this is built a 15-minute bucketed\ntime series for for each of my stocks\nand then I can import map Bartlett\nSo this dot p dot import is is is is\ndoing that so I'm just saying dot p\ndot import whatever the library is I\nwant from from Python and if I. I'll show\nyou what that looks like that just says that\nit's this module here and then I can use\nthis kind of a syntax to to to to to use\nit so I'm saying I want to go into plush\non the function I care about it is this figure\nand then this pykw is\nsaying it's a Python keyword and I'm\nsaying that associate the Python keyword\nof figsize with with this input so I'm\nkind of meshing the the q the q\nvariables into the in but leveraging the\nPython library and then I just extract\nthe time series for Adele and I plot my\nmy\nmy figure so that's obviously pretty\nstraightforward the nice thing about\nthis is that I can then use the very q\nlike syntax of of wrapping things up in\nfunctions and iterating to to kind of go\na little bit further so I start with\nmaking my my um my kind of basic plot\nsaid I'd the the size and all last and\nthen I wrap all of this into a pipe into\ninto a little function so this is going\nto take each of my time series and like\nI said they're at dictionary. \n\nSo I'm pulling out the I'm pulling out the\nprice and I'm pulling out the the symbol\nfor for each of my sorry for each of my\nrows of my of my table and plot them\nkind of all at once. So that's the this\nthis each year is the is is another\niterator in in in in Kdb. So it'll take\nany function and it will apply it to\neach of these and this time series here\nis the the table up above where I've got\nthis sym and the price for each of the\ndifferent rows that and the those\nbasically get passed through as a kind\nof a dictionary structure into into this\nand you can plot them all so that's\nthat's a very quick whistle stop tour\nof the technology. \n\nI'll flick back to\nslides now if I can yes and just say\nthat if it's available for a\nnon-commercial use so if you want to use\nit for your own projects doing whatever\nit is you would like knock yourselves\nout if you want to use it for academia\nabsolutely we have the citation page and\neverything feel free to play around with\nit and in the resources that that were\nlinked with the talk that you'll see\nthat there are some guides on on\ndoing this installation and for those\npeople who might be interested in\nknowing more about this or any getting\ngetting more hands-on\nI understand that with the best will in\nthe world it's not always easy to\nmotivate it's certainly something I\nstruggle with so just to say that we do\nhave free one-day workshops that we run\nat least once a month in in pretty much\nat least three major time zones but\ncertainly in America. We have at least\none a month in the U.S. time zone so for\npeople that do want to just come along\nthey're free. \n\nThe the one-day workshops\nthe Kx introductory workshops are free\nand so you'd all be very welcome if\nyou'd like to come and get get more\nhands-on with it in in a one day\ntraining. \n\nSo that was me.\nThis is a picture of me if anybody wants\nto stay in touch please do find me I'm\non you can you can email me here rebecca@kx if you have any questions about\nanything today or if you just want to\nknow any more I'd be very happy to talk\nwith you and I hope that you find it you\nknow interesting and even if it's not\nsomething you want to pursue hopefully\nit'll it has helped you know broaden\nyour horizons. So certain so certain bit.\n\nRebecca would you be able to put the link\nto the training in the chat.\nOh yes I'll do that now. Yeah. That would\nbe great. Yeah. So I might stop my screen share if that's okay because I'm getting a little bit of Inception and where I'll find\nthis little oh I popped out my chat\nthat's my problem. Okay. Okay. That's fine.\nAlright, yeah. I actually popped out my chat\nand I don't remember how to get to it.\nI'm struggling a bit.\nIt's okay you know what I can do. I'm going\nto upload the video later and so I'll put the link to the training in there so people will be able to get it. Okay.\n\nThere is one more question which is are\nq and Kdb+ open source and how are the requests for feature enhancements handled.\nThey're not open source. No it's actually a very very very\nsmall code base and the so it's it's\nfree for non-commercial use so\ndefinitely do whatever you like in your\nown time but yeah it's not an\nopen-source software it's written in C\nand it's all very very terse I think\neven I don't know I've seen the glimpse\nof it before and it did not look like\nany C code I've ever seen but in terms\nof feature requests it comes through us\nhere on the evangelism team anytime\npeople say or we'd like to see exploits\nedge we feedback back in to our core\ndevelopment team and we also have a we\nhave a number of kind of support groups\nso there's a a support group makes the\ntime very dramatic but there is a Google\ngroup for for people that you know are\ninvolved with it or want to use it or\nhow many questions in the usage where a\nlot of our more expert engineers will\nkind of respond back and there's also we\nhave a number of meetups and stuff so we\nwell in in in different times they would\nhave been a much more frequent and\nperson thing but yeah we have a we have\ndifferent email groups for for those and\na number of different tools and I didn't\nwant to kind of get into too much today\nbut like an IDE tool and a front-end\ndashboarding tool that are that are also\nworth checking out. That is it for question.\n\n### Thank You\n\nSo I want to thank you first of\nall for turning off your air conditioning to\nreduce the sound. Thank you so much. No problem. Thank you for joining us tonight and your presentation it was really great. No thank you all for joining and thank you\nvery much for having me I was delighted\nto come and hope I hope it was helpful.", "idn": "11", "video_url": "https://youtu.be/92YticvZzCM", "title_kw": "kdb", "meta_json": "11.json", "audio_track": "11_92YticvZzCM.mp4", "audio_text": "11_92YticvZzCM.txt", "has_transcript": true, "status": "Needs reviewer", "notes": "Paragraphs are too long"}