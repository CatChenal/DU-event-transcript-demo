{"presenter": "Matt Brems", "title": "Data Science with Missing Data", "year": "2020", "transcript_md": "08-matt-brems-missing-data.md", "meetup_url": "https://www.meetup.com/data-umbrella/events/271243174/", "yt_video_id": "LJKYXq3WHTw", "slides_url": "https://github.com/matthewbrems/missing_data_short_version/blob/master/missing_data_slides.pdf", "repo_url": "https://github.com/matthewbrems/missing_data_short_version", "notebook_url": "N.A.", "transcriber": "?", "extra_references": "N.A.", "video_href_w": "25%", "video_href": "http://www.youtube.com/watch?feature=player_embedded&v=LJKYXq3WHTw", "video_href_src": "http://img.youtube.com/vi/LJKYXq3WHTw/0.jpg", "video_href_alt": "Doing Data Science with Missing Data", "formatted_transcript": "if you have a\nyou can use the QA function so if you're\non zoom and you hover over the video it\nshould be on the bottom below the video\nit looks like QA and you can ask\nquestions there and then we'll spend\nsome time to answer them we do a break\nin the middle of the talk and then at\nthe end of the talk as well\nyou\nhey Matt I saw you just logged in are\nyou good to go\nyeah good to go whenever you are cool\nyeah I just I'll just do a quick intro\ntalk about the meetup group talk about\nGao take like two minutes I'll start at\n6:05 and then you can hop into your\npresentation sounds good\nhow's the new gig it's going pretty well\nweek two yeah no no complaints yet it's\nbeen dropped a little bit into the deep\nend but that's okay what I was sort of\nlooking for yeah is it what's it like on\nboarding during a pandemic so in my\ncurrent role worst of it it's\ninteresting secondly there wasn't a ton\nof onboarding there's a there's a bunch\nof stuff that I have to do kind of all\non my own time you know compliance and\nall of that but there hasn't been a ton\nof onboarding so I don't know if that's\njust the nature of me joining in the\nmiddle of this project or something else\nbut yeah I know I know about six\ndifferent people and that's about it\nokay yeah that's interesting it's it's\nweird because in a sense like remote\nwork or this environment we're in you\ncut a lot of the flow cut a lot of\nthings and I guess some people might\nthink they're fluff or not but right\nlike a lot of the stuff you do the first\nday at work or the first week it's like\nmeeting people you know all sorts of\nmeetings and things like that that may\nor may not have to do with actually the\nlauric you have to do but they're just\nlike work culture events right and so it\nlooks like some companies are just\ncutting it out and you know it's just\nlike whatever\nyeah it's it'll be interesting when we\nhave to start going back into the\nofficer rather if we we have to do that\nwhat things will look like then if there\nwill be a like a retro onboarding or\nanything like that yeah well a lot of\nuncertainty there is FINRA like I don't\nknow like the culture at FINRA is it\nlike very much like when the government\nopens they will\nopen up because I know in New York City\nyou know like officers opened up and\nobviously companies like GA and other\ntech companies are like let's let's hold\nback but I have some friends that work\nin investment banks they have to like\nshow up to work next week like full time\nwe we haven't gotten anything about that\nyet I think we're following the to my\nknowledge and what I've heard we're\nfollowing the like the schools in DC so\nI didn't schedule we'll see but haven't\nheard anything yet so I I would be\nsurprised if we go back before like\nAugust 15 just based on where we are now\nbut who knows okay well we have like we\nhave yeah we still gonna come people\ntricking in but I'll just start my intro\nthis can everyone can you see my whole\nscreen now yep okay sometimes I forget\nif you go fullscreen and Google Chrome\nso thanks everyone for attending during\nthese uncertain times I hope you'll get\nas much out of this talk as I'm planning\nto so the talk is called good fast cheap\nhow to do data science with missing data\nand Matt brems will actually be leading\nthe talk I'm just here to do an intro\nand moderate and login to my zoom\naccount so just a quick agenda we're\ngonna do an intro I'm just gonna\nintroduce Matt I'm gonna introduce kind\nof our sponsors and the meetup group and\nthen we will open up for Q&A as I\nmentioned before zoom has like a\nbuilt-in Q&A function so if you hover\nover this video with your mouse on the\nbottom below the video you'll see a\nthings called Q&A so you can ask\nquestions directly there this will be\nrecorded it will send it'll be sent out\nand in a couple days so so call it two\nor three days automatically by the GA\nteam if you do not get the recording for\nsome reason it could be in your spam box\nbut otherwise just email your local GA\nkind of\ncampus and and they'll be able to sort\nit out or they'll just email me and then\nI'll email them they're recording so\nMatt brems is the speaker he actually\njust moved to FINRA maybe like two weeks\nago but before that he was at the global\ninstructor for GA Zeta Science immersive\nprogram across the United States and\nbefore that he was working in the\npolitical consulting realm and if you\nwant to follow him on Twitter his symbol\nis or his tag his handle is Matthew\nbrems I wasn't able to figure all by oh\nit was you know it's too long for the\nslide and then the event is run by data\numbrella so the mission of data\numbrellas provide a welcoming and\neducational space for you our G's or\nunderrepresented groups in the fields of\ndata science and machine learning you\ncan learn about upcoming events and the\nmission at data umbrella org and you can\nfollow them at data umbrella on Twitter\ndata umbrella is running weekly events\nduring this time like weekly virtual\nevents so if you check the page there'll\nbe another really interesting data\nscience event within the next seven to\nten days call it pi ladies is also a\nco-sponsor it's the New York City\nchapter of a large national group so\nit's a group for python ladies and nine\nnon-binary people of all levels of\nprogramming experience and you can check\nthe home page of the national\norganization pi ladies calm and then\ntheir Twitter handle is NYC PI ladies\nand then last but not least the company\nwhere I work at and where I met Matt\nbrims and is letting us borrow their\nwebinar zoom account is General Assembly\nso General Assembly is a pioneer in the\neducation and career transformation\nspace specializing in today's most\nin-demand skills so the homepage is\nGeneral Assembly or General Assembly Y\nand then the Twitter handle is at GA we\nrun a lot of cool classes and tech data\nmarketing product management if you're\ninterested in learning\nthe classes you can also ask questions\nto me I can I can help sort that out but\nwithout further ado we will move on to\nMatt's talk awesome just stop staring my\nscreen and then you can yours yeah I'll\ngo ahead and start sharing my screen as\nwell good evening everybody\nlike liked I shared my name is Matt\nfriends I use he/him pronouns I'll go\nahead and share my screen here so\nhopefully you'll be able to see it I\ndropped in the I dropped it in the chat\nbox a link to github where you can find\nmy content so if you would like to\nfollow along you're certainly welcome to\nI've got I'll be focusing exclusively on\nslides here\nhowever in in the repository there are\nin the repository there's code and it\nwill clear breaks in the code show when\nyou can that show kind of when we'll be\nshifting from one slide to the next and\nwhere the code sort of fills in the\nblank I'm seeing that this is not\nsharing so let me actually tie really\nquickly are you able to see my slides so\nno it just says Matt brands has started\nscreen sharing okay well let me go ahead\nand restart sharing that I appreciate\nfolks as patience here with this this is\nmy first time setting up this my first\ntime using this iPad I switched iPad\nsince they no longer I'm using my GA one\nso we will let's see\nyeah if everyone's wondering the slides\nfrom this presentation are in that link\nthat Matt shared in the chat and that\ncould help others PDF right so this is\nnot working so what I'm gonna do I've\ngot the slides on Google slides so I'll\njust go ahead and use the Google slides\nversion and edit through that instead of\ninstead of working on my iPad so I will\nnot get to use my my stylus that that is\nthat is okay so let me stop sharing that\nand I will go ahead and reshare my\nscreen here again appreciate folks is\npatience with this Oh were you trying to\ncast your iPad into the zoom yeah I've\ngot a okay yeah I've got it\nnice desktop thank you uh you would be\nable to see my screen now my Google\nslides have not pulled up can you see\nhim yep we can say it now okay I saw a\nface light up so imagine that yeah that\nthat's coming through now so I won't be\nable to annotate with my with my Apple\npencil but but that's okay so thank you\nvery much time thank you very much\neverybody for for having me join like I\nmentioned my name is Matt friends I use\nhe in pronouns and what I'm here to talk\nwith you about is how to do data science\nwith missing data this is a really\nchallenging problem to work within to\ntry and grapple with but I think it's an\nimportant topic because if you do data\nscience you have inevitably run into\nchallenges with missing data or missing\ninformation and the ways that we try and\nhandle that probably are not the ways\nthat we should be handling that so I've\ngot some stuff here you can see this in\nthe slides ty has already talked about\nmy background so I'll go ahead and skip\nbeyond this you're welcome to to hit me\nup afterward if you'd like to talk about\nany of these experiences but the lay of\nthe land for tonight so we're gonna\nstart by talking about missing data\nwe'll get into strategies for doing data\nscience with missing data and provided\nthat we've got time we're gonna wrap up\nwith some practical considerations and\nwarnings for you to keep in mind as this\ngoes on please feel free to at any\nmoment drop notes in the slack or in the\nzoom channel so I will keep an eye on\nthat as well as the Q&A so at\nany point feel free to do that to drop\nstuff in there and I'll try and respond\nkind of in real time\nso how big of a problem is missing data\nand this is just gonna be the the very\nquick start of it it's a really\nchallenging question for us to answer\nbecause what's going to happen when we\nare trying to work with missing data we\ntry and quantify how big of a problem it\nis is that from a practical point of\nview and it sounds trivial to say this\nwe can only see what we observe so we're\nonly going to be able to actually see\nthe data that we've gathered we don't\nknow the value of that missing thing\nitself so the only way for us to be able\nto quantify or understand the magnitude\nof how big of a problem missing data is\nis there we can use simulated data to\ntry and help and answer that question\nnow in the interest of time we're not\ngoing to go through actually generating\nthe simulated data and looking at this\nbut if I move forward to this slide if\nyou would like to check this out all of\nthe code is pre-written for you in the\nnotebook and in the so in the repository\nI've got three different sets of\nnotebooks\nthere's one with a prefix 0 0 1 with a\nprefix 0 1 and one with a prefix 0 2 so\nyou can run that on your own if you\nwould like but in short what you would\nsee at that notebook is we create some\ndata or we generate a complete data set\nand then we take 20% of those\nobservations and turn them off or we set\nthose to be missing and we see how much\nof an impact that would have on our\nmodel if and what we notice is that if\nwe were to look at the slope and the\ny-intercept of our simple linear\nregression model there's actually a\nreally really really large effect and so\nthat is a quick way for us to try and\nquantify how bad or how big of a problem\nmissing data is now that depends on a\nwhole host of factors how much data do\nyou have what is the type of missing\ndata that you're dealing with how what\ntype of model are you trying to fit how\nmany variables do you have all sorts of\nthings factored into that but in a very\nvery simple case we can see that missing\ndata is really going to undermine a lot\nof our inferences and the conclusions\nthat we may try to make\nso this brings me to what I want to get\ninto which is what is a realistic\napproach for us so if you're familiar\nwith the good fast cheap idea in project\nmanagement what that means is that you\ncan come up with a project that is good\nand fast and if you come up with a\nproject that is good and fast what's\ngoing to end up happening is it's not\ngonna be cheap that is it will be more\nexpensive to be able to do that project\nbecause if somebody wants something done\nquickly and wants something done well\npeople will probably have to pay top\ndollar for that on the other hand you\ncan think about a good and cheap project\nso sometimes people will say hey I need\na project that is high quality and they\ndon't want to pay a ton of money for it\nand that's a very realistic scenario to\ncome up but the challenge is if\nsomebody's not willing to invest in it\nand you want something that's high\nquality generally that will take a large\namount of time in order to deliver that\nsolution so here we see the overlap\nbetween good and cheap is that it will\ntake time to deliver and then finally at\nthe bottom here we see fast and cheap\nmost frequently in my personal\nexperience people will say hey I want to\npay this low dollar amount for a project\nand I need it done tomorrow well what's\ngonna happen is is that if something is\ndone fast and that something is done on\nthe cheap it's not going to be the best\nquality in most cases so because of that\nwe need to think about how can we take\nthis approach and apply it to - missing\ndata the reason that I bring this up is\nthat oftentimes what clients or managers\nor anybody else once they will say I\nwant something that is good and fast and\ncheap but that's not going to be\nfeasible connecting this directly with\nmissing data we think about what missing\ndata means in terms of a fast and cheap\nanalysis that's going to be you just\ndrop all of your missing values or you\ndo a single imputation\nif you want an analysis that is done\nwell and is fairly inexpensive then you\nhave to fill in your missing values or\nhandle missing data in what I would call\nthe proper way so we can talk about\nproper imputation or the pattern sub\nmodel approach and then finally and\nwe'll get into what those means shortly\nand then finally if you want an analysis\nthat's good and your analysis to be very\nquick then you should gather your data\nin a complete manner the downside of\nthat of course is it's incredibly\nexpensive to do that you have to figure\nout how can you collect complete data\nwithout missing any data and oftentimes\nyou have to pay top dollar for that\nsomething that might be a hard sell for\nwhen you're talking with your boss or\nyour client or somebody else so I'm not\nI what you should come away from this\nevening is not this is the specific way\nI need to always handle my missing data\nbut get a better understanding of what\nare the trade-offs and missing data what\nare the different challenges in are the\ntrade-offs if I go with one approach\nversus another approach so given that\nintroduction what I'd like to do is talk\nabout what are strategies for doing data\nscience with missing data so the first\nthing to do is let's talk about how to\navoid missing data so something that I\nthink is really important to note is\nthat it's usually going to be more\nexpensive up front but cheaper in the\nlong run to avoid missing data so as you\nthink and this is probably the this is\nnot the best way of going about best way\ntrying to come up with the right phrase\nfor it this is not the this is not the\nfanciest or coolest approach to missing\ndata for me to spend time to talk about\navoiding missing data you're like yeah\nlike okay I know that it's better for us\nto collect data as opposed to collect\ndata that is missing and I get that but\nit's important to talk about this\nbecause oftentimes in the long run it's\ngoing to be a better thing for you to\navoid that missing data upfront\ndepending on the jobs you have were the\njobs you want to have if you're working\nin an organization where you're\ngathering survey data or you're working\nto collect data in some capacity\nif you have any control over that there\nmay be small to moderate design changes\nthat can be implemented there that\nallowed to gather significantly more\ndata if you gather more data you don't\nhave to invest time in how to handle\nthat missing data later you can just use\nthe entirety of your data if you've got\nmore data your inferences and your\npredictions and everything tend to be\nmore precise your variance is lower and\nso because of all of this it's often\nbetter for us to try and avoid missing\ndata upfront if we can so I want to take\na moment to talk very briefly about some\nof these again you can read all of these\non your screen but some of the things\nthat I think about our for example\ndecreasing the burden on your respondent\nor minimizing the number of questions\nsomebody has to respond I responded to\ntwo surveys earlier today I'm Survey\nMonkey a former colleague had posted\nsome stuff on LinkedIn and I filled\nthose out and they were I was willing to\ndo it even on my phone because they were\nrelatively short surveys I like the\nbeach Chipotle app I eat from Chipotle\nquite frequently and up until actually\nlike last week what they would do is if\nyou ordered online through the app they\nwould follow up about an hour later with\na green smiley face or a red frowny face\nand say hey how was your dinner this\nevening\nand you could click that smiley face or\nthe frowning face and if you click the\nsmiley face they said hey thank you so\nmuch if you click the frowny face you\ngot to put a couple of checkboxes and\nsay this is what I this is what I didn't\nlike or this was this was not\nsatisfactory and that was it so many\nother organizations there were so many\nother data collection mechanisms end up\nrequiring you to fill out 20 30 40 50\nquestions and so what ends up happening\nis that you often will create missing\ndata by the design of what you're\nlooking at is opposed to any anything\nelse\nso making some changes on how you can\ndecrease the burden on your respondent\nmaybe making questions closed-ended\ninstead of open-ended like like a fill\nin the blank question\none other note that I want to make here\nis thinking about in groups thinking\nabout improving accessibility that's a\nvery very important point so there are\nlots of different ways you can get into\nthis we can think about language\naccessibility we can think about\nreadability we can think about\nindividuals who may be hard of hearing\nand ways to gather data from individuals\nbut it's important to think about\naccessibility and inclusivity when we\ndesign that so if you are part of an\norganization where you are gathering\ndata in some capacity is there a way to\nimprove that accessibility to others for\nexample when I was doing polling and\nsurveys in the context of politics we\nwould administer surveys in multiple\nlanguages specifically English and\nSpanish when we were calling different\npopulations or specifically different\nstates that had a a particularly large\nHispanic population or spanish-speaking\npopulation that was something that we\nwanted to do because otherwise we were\njust leaving out broad swaths of the\npopulation which would of course down\nthe road compromise our inferences so\nyou can make a compelling business case\nfor doing something like this it's not I\nmean accessibility in my opinion is in\nand of itself a valuable goal in\naddition to that I think that it's\nimportant to recognize that when\nattempting to encourage other people or\nshare with other people that they should\ntake some action or invest some funds or\nsome energy in that that there are some\npositive business effects to it as well\nmoving onto the next slide so we talked\nabout avoiding missing data how do we\nignore missing data well the very very\nshort summary is that we're going to\nassume that any observation that we've\nobserved is similar to those\nobservations for which we are missing\ndata when we ignore we're making an\nimplicit assumption which may or may not\nbe a valid thing to do when I was in\ngrad school a professor shared that a\nvery general rough guideline is that if\nyou are missing less than five percent\nof all of your data you may be okay\nignoring that data that's missing now if\nyou are trying to do something like\nsupervised learning you fit a model\nwhere you've got a bunch of inputs and\nan output your Y variable if you're\nmissing a ton of data from your Y\nvariable\nthen even if you're missing less than 5%\nof your data overall that may compromise\nyour inferences too much you may not be\nwilling to do that or if there are\ncertain variables that you know were\nbelieved to be really meaningful and\nyou're missing a lot of data from those\nmaybe ignoring missing data isn't the\nright way to go now when we ignore\nmissing data effectively it's what most\nsoftware's are going to do by default in\nour in Python in something else if you\njust put your data into your model and\npress GO\nor dot fit or whatever it is you choose\nto do if you were to do that and didn't\nhandle your missing data in some\ncapacity or in some way then you're\nprobably going to that's effectively\nignoring it your model or your software\nis almost certainly going to drop all of\nyour observations that contain one or\nmore missing values in it and that may\nbe okay to do if that number is\nrelatively small but I want to emphasize\nup here there is an assumption that you\nare making with that and that may or may\nnot be a valid assumption to make\nso the last thing that I want to talk\nabout is how to account for missing data\nI mentioned how to avoid missing data\nupfront if you can't avoid it you might\nsay can I ignore it well here before we\naccount for MIT and if we can't ignore\nit then we have to account for it but\nbefore getting into that I want to shift\nour mindset a little bit because there\nis a a belief that we can just plug in\nthose gaps in our data that you know and\nperhaps you were perhaps someone\nexpected that you would be able to come\nhere tonight and I would give you a new\nPython package that allows you to fill\nin missing data and and you've got that\ntechnique you can put in your work flow\nand in your wallet and kind of move on\nyour way but the problem with that is\nthat you have to do this in a specific\nway or we're really just making up data\nand making up data has all sorts of\nissues a we might be wrong be it's not a\nan ethical thing to do in my opinion and\nso because of this we need to be very\ncareful about how we would fill in some\nof those gaps or how we how we tackle\nmissing data but I like to shift our\nmindset a little bit and say in most\ncases we're not really fixing missing\ndata it's not like we just have this as\na new step in our workflow where I fit\nsome some method in pandas or in\nscikit-learn and then move on with the\nrest of my day we're really just\nlearning how to cope with missing data\nso given that shift in our mindset that\nwe're really just learning how to\neffectively and in a principled way cope\nwith our missing data let's move beyond\nthis so we want to talk about how to\naccount for missing data and there is\ncode in the repository to go through\nboth unit missingness and item\nmissingness so I want to I want to share\nthat with you and that's again in the\nrepository if you'd like to take a look\none note that I want to make is please\nagain feel free to drop questions in the\nchat if there are questions that you\nhave because I want to make sure that I\ncan answer them as we go I really want\nthis to be as helpful as possible for\nfor each so let's talk about unit and\nitem missingness there are a couple of\ndifferent ways that data can be missing\nunit missingness is where we're missing\nall of our values from one observation\nso for example here index 3 if I'm\ngathering data on individuals and let's\nsay that person 3 just did not respond\nto my survey or for whatever reason I\nhave no information from this person if\nI have n A's for all of those that would\nbe an example of unit missingness this\nperson did not share their information\nwith me and so I have no information\nitem missingness or I like to refer to\nit as Swiss cheese missing this is where\nthere are holes in your data so indices\n1 2 in 10,000 have this for example for\nindex 1 we do not have information on\nage or income but we do have information\non sex here for individual 2 or index 2\nwe do not have sex but we do have access\nto age and income data and then all the\nway down to row 10,000 we're missing one\nvalue here in the income\nso the way that we handle unit and item\nmissingness is a little bit different so\nin terms of unit missingness the very\nvery quick summary of how to handle unit\nmissingness and i am let me actually go\nahead and pull up this notebook just to\nvery quickly show what this looks like\nI'm gonna go ahead and pull open this\njupiter notebook if you do not have\njupiter notebook on your computer or if\nyou're not familiar with python or\nanything like that that's okay\ni'm probably only gonna spend about two\nto three minutes talking about this but\ndo want to pull this up as an example\nso when we are that is item missing this\nwhat I meant to do is pull up the unit\nmissing this one I grabbed the wrong one\nso I apologize I'm gonna move back over\nhere I'm going to go ahead and shut that\ndown and open up the jupiter notebook 0\n1 unit missing this\nyou\nand I'll drop that in the chat here zero\none unit missing this IP Y and B which\ncan be found in that repository in my\nexperience the most common method of\nhandling unit missingness where we're\nmissing an entire row of data is if we\nhave supplemental data on that\nindividual to do something called weight\nclass adjustments where we take our\nobservations and we break them into\nclasses and then we will weight them\nbefore doing our analysis so for example\nlet's say that I'm working in HR\nanalytics so I'm working in human\nresources and I want to understand how\nsatisfied are individuals within our\norganization let's say that to make this\nsimple we have two different departments\nwe have a finance department and an\naccounting department for which I want\nto study individuals and let's say that\nmaybe when I administer these surveys\nthat in the finance and accounting team\nthey're split perfectly evenly got 50%\nof people in finance 50% of people in\naccounting but let's say that maybe\npeople in finance had too much other\nstuff to do or were less responsible or\nwhatever kind of motivation you want to\nascribe to that and let's say that\npeople in finance were less likely to\nrespond to my survey and let's say that\npeople in accounting whether it's\nbecause they had less on their plate\nthey're more organized they're more\nconscientious of this they just wanted\nto reply whatever else let's say that\naccounting people responded more to my\nserver and so because of this if I\nscroll down here what we're going to do\nis we're going to see that if I look at\nall of my survey responses so let's say\nI administer this survey 50% of people\nare in accounting 50% of people are in\nfinance but when I get my surveys back I\nget a disproportionate number of\nresponses in the accounting department\nhere about 77% of my responses are in\naccounting meaning that only about 22 or\n23 percent of my responses are in\nfinance well if\nI was going to just take these values\nand I was just gonna do a simple average\nto understand on average how happy are\nmy employees I might be putting some\nadditional bias in my model here and\nthat bias may come in because I received\nway more responses from accounting than\nfrom Finance so what I would like to do\nthe strategy that we can employ is\nsomething called a weight class\nadjustment where I'm going to basically\ndown weight all of my people from\naccounting I'm going to up weight all of\nmy respondents from finance and what\nthat's going to do is going to put them\nback on an equal playing field because\nagain 50% of people were in finance and\n50% of people were in accounting so the\nway that we do that is we take our full\nsample of people do all of the 100\npercent of people who we administered\nthose surveys to both the observed and\nthe missing we're going to lump them all\ntogether and we break them into\nsubgroups based on characteristics that\nwe know in this case I know accounting\nand finance I'm going to give every\nindividual a weight as well so the\nweight for people in group I is going to\nbe what's the true percentage of people\nin that group divided by what's the\npercentage of observed responses in that\ngroup so for example in the accounting\ngroup the true percentage of people who\nwere in accounting is 1/2 divided by the\npercentage of responses from accounting\nwhich was as we saw appear about 77% I\nlet Python do the math and so the weight\nfor each accounting vote is about 64\npoint six percent\nI do the exact same thing for finance\nand what that means is that each finance\nthough gets a weight of 2.2 so for every\nfinance person this is finance and this\nis accounting every single finance\nperson who replied they get a vote\nthat's 2.2 times so one person submits a\nsurvey I'm gonna up with them 2.2 times\nfor every accounting person who\nresponded instead of each person getting\none vote they effectively get point six\nfour five votes if we want to take the\nweights in each of those groups and\nmultiply that by the number of responses\nthat we get that ends up equalizing\nthings so that the total weight from all\nof my accounting responses and the total\nweight from all of my finance responses\nend up being equal or in this case\nalmost exactly equal once you have\ncreated those weights what you can do is\njust pass them in to SK learn so you'll\ncreate a column of weights I've done\nthat here just added a column in pandas\nDF bracket weights\nand then what we can do is use that in\norder to do more complex analyses so if\nI were to just calculate for example the\nraw average of employee satisfaction I\ndid an employee satisfaction score of\nabout five point seven but if I\ncalculate the weighted average based on\nmy employee satisfaction score it's\nsignificantly lower I get here five\npoint four five so that that average\nscore went from five point seven down to\nfive point four five above a decent drop\nand that's because people in accounting\nthis is based on the data I generated up\ntop but people in accounting were on\naverage happier with their jobs people\nin finance were on average less\nsatisfied with their jobs what ends up\nhappening though is when accounting over\nresponds and Finance under responds is\nthat that's gonna skew our results now\nif you want to take this information and\nyou want to build a more sophisticated\nmodel with this you can do so by passing\nDF weight if you're if you're a user of\nPython and specifically scikit-learn\nif you want to you can pass DF bracket\nweight that column or that vector of\nweights when you fit your model and it\nwill weight\nyour models results based on those those\nweights that you've given it so you\ncould do this for a linear regression\nmodel or you could do this for a random\nforest or something else if you would\nlike to two quick things that I want to\ncall out one is that our goal with post\nweighting when we do this weight class\nadjustment is to decrease our bias but\nwhat should we be concerned about well\nwhen we decrease bias\nwe tend to increase variance and so\nthere's an article from the New York\nTimes back in 2016 some of you may be\nfamiliar with this there was an\nindividual I believe it was the the\ntitle of this article is how one\nnineteen year old Illinois man is\ndistorting national polling averages and\nwhat this ended up and so I encourage\nyou to take a look at this if you would\nlike to take a look at and read this but\neffectively they created these buckets\nthey weren't just looking at the\naccounting and Finance Department but\ninstead they looked at for example age\ngeography sex other information maybe\npolitical party all of these different\nbuckets and by creating so many\ndifferent buckets and attaching a weight\nto those buckets based on response rates\nwhat ends up happening is they decrease\nbias but there tends to be an increase\nin variance and so what ended up\nhappening here I would encourage you to\nperhaps take a look at that later if you\nwould like but what ends up happening is\nthat the person who had that who is\ndistorting national polling averages his\nweight as assigned by this approach was\na was thirty times higher than the\naverage individuals weight and was\nactually 300 times more than the person\nwith the smallest weight in this poll so\nthis one individual had an enormous li\noutsized influence on these polling\naverages so it's something to to keep in\nmind related to this I'm making an\nassumption here I'm making an assumption\nand thank you Sam for for sharing that\nlink I'm making an assumption here that\nI know that fifty percent of my people\nare in accounting and fifty percent of\nmy people are in finance but that's not\nalways a realistic assumption so if I\nwant to understand what percentage of\npeople will support the Democratic\ncandidate in the upcoming election and I\nwant to look at things across age groups\neighteen to thirty four thirty five to\nfifty four or fifty five and up I have\nto make a guess about that as I write\nhere hopefully it's an educated guess\nbut what we see in past elections may\nnot be indicative of what we're going to\nsee in this election thinking about the\n2016\nkradic presidential primary my\nunderstanding is far more young people\ncame out and voted in that Democratic\npresidential primary largely in support\nof Bernie Sanders thinking about that\nthat's something that we may not have\nnoticed in 2010 2008 2004\nso if we use past data to predict the\nfuture it can be really challenging to\ndo that in a way that's principled we\nthink about when then-senator Obama was\nrunning in 2008 what ends up happening\nwas when when he was running against\nsecretary or then I should say\nthen-senator Clinton in oh wait what\nends up happening is that if you just\nlooked at information in 2000 and 2004\nyou're likely going to dramatically\nunderestimate the proportion of people\nof color specifically black voters who\ncame out in support of Obama during the\n2008 election so this is this weight\nclass adjustment method is something\nthat you can sometimes do but you can't\nalways do that and so it's important to\nkeep in mind some of the limitations of\nthis we would be assuming that we know\nwhat the distribution of in this case\nwhat I've highlighted the age groups are\nbut that's certainly not a guarantee I'm\ngonna go ahead and shut this and I'm\ngonna move back over to the slides\nyou\nso to try and talk about how to pull\nsome of these pieces together in a work\nflow we have not talked about we have\nnot talked about imputation 4 for unit\nnon-response yet but we'll get into that\nhere in terms of my work flow I start by\nsaying how much missing data do I have\nand is it worth my time to try and\naddress it anytime I'm doing a data\nscience problem that's one of the first\nthings I look at then I say is it\nreasonable to attempt deductive\nimputation which we're going to talk\nabout momentarily then if my goal is to\ngenerate predictions then I'm going to\nuse the pattern sub model approach if I\nwant to conduct inference that I will\nuse the best imputation method available\nideally proper imputation so we'll talk\nabout what each of these are because a\nlot of those bold in terms are things we\nhaven't seen yet in order to get into\nthat though one last thing that I need\nto talk about are three different types\nof missing data\nnow you when you look at your data and\nyou see a bunch of na s in your data or\na handful of na s in your data they all\nlook the same to us but there are\nactually three different types of\nmissing data that are important to know\nso I'm taking inspiration from my friend\nAllison here my friend Allison she is\ngetting her PhD in biology at Notre Dame\nvery proud of her so let's say that\nshe's a grad student in a lab working\nlate and while she's pipetting in the\nlab she reaches for her pen and\naccidentally knocks one petri dish off\nthe desk so from that petri dish my\nfriend Allison loses all of the data\nthat she otherwise would have collected\nso over here I'm looking at what maybe\nthat might look like in terms of data\ngathering so Allison was able to measure\nhow much bacteria or what was the width\nof the bacteria in her petri dish on day\none for all of these different petri\ndishes did the same thing on day two\nbut this here you can see 16 millimeters\nbut really that would be an n/a in our\ndata that's something that we do not\nhave access to we would call that\nmissing completely at random this data\nis missing completely at random because\nthere's no systematic differences\nbetween that data that's missing and the\ndata that we've observed\nmoving on to the next example there's\nsomething called missing at random and I\napologize please do not shoot the\nmessenger I was not in the room when\npeople decided on these terms\nI think these terms are silly and I wish\nthere was a better way to describe them\nbut I apologize on behalf of\nstatisticians here so we talked about\nmissing completely at random here we've\ngot missing at random so let's say that\nwe work for the Department of\nTransportation and we're looking at the\nPennsylvania Turnpike a toll road that a\nhighway that hasn't told me with on it\nso that you can understand people have\nto pay whenever they go through this\ntoll booth in order to use the\nPennsylvania Turnpike and let's say that\nthere's a a sensor set up to track how\nmany cars go through a given gate in a\ngiven time window\nwell that sensor breaks and doesn't\ngather any information between 7 and 10\na.m. what we would describe that as is\ndata that's missing at random and the\nreason that we call it missing at random\nis that conditional on some data that we\ndo have in hand the data of interest is\nnot systematically different so whether\nor not that data point is missing\ndepends on data that we have observed in\nthis case we have observed time that is\ninformation that we do have and time\ncontributes to that missingness that\nmissingness is based on are contingent\nupon those specific hours that data is\nmissing you might imagine this solution\nfor trying to tackle this type of\nmissing data is maybe we want to use the\ntime in order to help us fill in or\ngenerate some value for those number of\nvehicles that are that we are missing\nthe last type of data that's missing the\nlast type of missingness I should say is\ndata that's not missing at random let's\nsay that I administer a survey and that\nsurvey includes a question about income\npeople who have lower incomes are less\nlikely to respond to that question about\nincome what we call that data that's not\nmissing at random because whether or not\nsomething is missing depends on the\nvalue of that missing thing itself here\nfor example we see that people who have\nlower incomes are on average less likely\nto share their incomes with me so if we\nwanted to do something simple like\ncalculate the average of this data I can\ncalculate the average of income but it's\ngoing to be skewed pretty significantly\nupward we're gonna see a value that's\nmuch higher than it should be now this\nis the most complicated type of\nmissingness to work with because we\ndon't have access to these incomes here\nso those are the three different types\nof missingness and i want to talk about\na couple of ways that in the 15 minutes\nwe've got left that we can handle so\nthere are five different methods here\nthat I outline I'm going to move quickly\nthrough them because there's a reporting\nhere and I want to be respectful of\nfolks this time and not hold folks over\nbut again please ask any questions that\nyou have in the chat\nso let's start by talking about\ndeductive imputation deductive is it has\nto do with logic we're going to deduce\nvalues we're going to use logical rules\nto understand how we can fill data in so\nlet's say that there was a survey that\nasks if somebody was the victim of a\ncrime in the last 12 months and that\nperson says no and then of the same\nsurvey has a later question that says\nreally the victim of a violent crime in\nthe last 12 months and that respondent\nleaves the answer blank we can use logic\nwe don't have to make any guesses or we\ndon't have to do any inference we can\nuse logic to say given the answer to my\nfirst question I know the answer to that\nand I can fill in that missing value\nthrough logic this requires specific\ncoding so you would have to as we get\nnew data we have to recognize how do my\nvariables or my columns relate to one\nanother you would have to code that up\nthat's not something that is going to be\nconsistent across all data sets so you\ncan't just download a library to do that\nit can be time-consuming but it's good\nbecause it doesn't require any inference\nand it does not matter what type of\nmissing data you're working with whether\nit's missing at random not at random\ncompletely at random you can do this in\nany of those cases the next thing that I\nwant to bring up is mean median and mode\nimputation so I imagine that many of you\nhave done this at some point or another\nfor any n a value or any missing value\nand your data you just replace your\nmissing value with the mean or the\nmedian or the mode of that column it's a\nquick fix it's easy to implement and it\nseems reasonable but it can really\nsignificantly distort your histogram and\nit underestimates your variance and\nwe'll talk in a minute about why that\nvariance or that variability is so\nimportant it should only be considered\nif your data is missing completely at\nrandom so if you can say based on my\nunderstanding of my data I can truly\nbelieve and maybe some quick analyses\nthat I do in my data you can say look I\nbelieve that my data are missing\ncompletely at random this would only be\nappropriate in that case but even then\nyou probably shouldn't do this in their\nbetter ways of handling missing data so\nthis is an example so what I have up top\nand this is all in that zero two\nnotebook if you want to take a look at\nthat zero two item missingness these\nvisuals come directly from that movement\nso here up top\nthis is the real histogram of data with\nin blue this blue vertical bar shows us\nthe true average of that data down below\nwe're looking at the same data but in\nblue I have which data I've observed and\nthen any value that was missing I filled\nthe mean in so here this orange bar\ngives me the mean you'll notice that\nthat's a pretty dramatic difference\nbetween the top and the bottom if you're\nmissing ten twenty thirty percent of\nvalues in a column which is not the\ncraziest thing in the world to think\nthrough you might get something like\nthis so first off you're going to\ndistort that histogram so that's one\nchallenge of working with this\nthe other thing that I want to run\nthrough is why is under estimating\nvariance a bad thing so here I've got\nthe formula for your sample standard\ndeviation and you can go through this if\nyou would like but in short if you are\nworking with the sample standard\ndeviation what you'll notice is that if\nyou've observed your first K\nobservations and then observation k plus\n1 all the way through n those are\nmissing and you try and use mean\nimputation what you'll do is for K plus\n1 through n you fill the meaning for\nthose values that formula ships instead\nof dividing by K you're gonna divide by\nn that denominator gets bigger but you\ncan rewrite this formula by breaking it\nout for the first K values those values\nyou've observed the actual real data and\nhere k plus 1 through n all of those\nvalues that were missing and you filled\nin the mean for we'll notice here we've\ngot x-bar which is our sample mean minus\nx-bar which is our sample mean that's\nzero zero squared is zero and if you add\na bunch of those up that still gives you\nzero so what ends up happening is that\nthis part of your this part of your\nvariance or this part of your standard\ndeviation remains exactly the same you\ndon't add anything there but your\ndenominator gets bigger the reason I\nwalk through that is by doing this your\nstandard deviation gets smaller your\nstandard deviation is used in a number\nof ways one if you try and generate a\nconfidence interval then your standard\nyou're gonna get maybe a 95% confidence\ninterval but that's gonna get smaller I\napologize if you can you can either see\nthe Lightning or hear the thunder on\nsign so you may see your your confidence\ninterval get much smaller depending on\nhow many of those values you move it\nthat's not because you're getting more\nconfident or that's not because you\ndecrease your level of confidence and\nsay 95\nto 90% confidence that's just because we\nimputed our meat so we might become\nfalsely confident in our results\nsomething similar happens to p-value\nwhere your p-value may shrink so your\np-value of point O 7 now becomes a\np-value of say point O 3 all of the\nsudden we get a significant result but\neven though we but that significant\nresult is only because we filled in this\nmissing data so Ramesh asks and I\napologize if I mispronounce anybody's\nname thank you for your question you\nmesh do you see significant gains in\nmodel performance from imputing using\nmethods like Miss forests or other model\nbased in QT how do they compare it in\nsimpler imputing methods like mean\nimputation so mean median and mode\nimputation and if I move to the next\nslide this is a write-up of that\nconfidence interval gets smaller the\np-value gets smaller but that's not\nsomething that's that's not real that's\nnot valid if we look at mode imputation\nwe see a similar challenge where one\nvalue is artificially inflated at ton\nbut we're still going to see in effect\nwhere that standard deviation is almost\ncertainly going to get smaller and\nartificially so so when it comes to\ntrying to impute properly what you\nshould do and in the interest of time\nI'm gonna skip ahead if you were to try\nand fit a single regression imputation\nwhere you fit a model to your data like\nMiss forests or something else you're\ngonna get values that look like this\nwhere those imputed values are all\nlumped in the middle it's still not\ngoing to work the way that you expect it\nto and that's because when you generate\nthose predictions that deterministic\ncomputation that you do is usually going\nto fall on one line instead you imagine\nthat you probably want to generate\nvalues that look more like this that\nresembled a true variability that you\nsee in your rule\nso because of that in order to properly\nimpute we need to impose because anytime\nyou fill in a value with one number\nyou're treating it like you know that\ntrue number and that's not the case if\nyou fill in any missing value na with a\nzero or a ten or a thousand you're\nacting like you know that value don't so\nthe way to properly impute missing data\nis to make like ten copies of your data\nset to do imputation with some\nstochastic behavior so you can add in\nlike a random error if you were to do a\nregression model or if you were to do\nmiss forest you could you would be able\nto maybe adding some randomness into\nthat you would do that on all ten of\nyour copies of your data sets once\nyou've got ten copies of your data set\nthat are full they've got some different\nvalues in them then you would build like\nyour final model or do your final\nanalysis on each of those data sets then\ncombine your results together just like\nyou would aggregate results in a random\nforest so if you are doing a\nclassification or a regression model you\ncan average your predictions for\nclassification you would do like a vote\nbased prediction across all ten of those\nmodels that you constructed there's a\nvisual here that I think helps to drive\nthat point home again you start with\nyour data there's a bunch of pound signs\nor hash tag symbols in here that\nrepresent missing data you make a bunch\nof copies of that this image has three\ncopies of your data and fills those in\nusing some sort of random imputation\nmethod like a regression model with some\nrandom error then you build your final\nmodel or analysis on those three data\nsets you get your results and then you\ncombine them together if your goal is to\ndo prediction you can do that if your\ngoal is to do inference there's\nsomething called rubens rules we're\ngonna drop that name in the channel here\nRuben's rules\nin order to combine those estimates\ntogether so I've got a little bit of\ncontext on that on Rubens rules so\nthere's documentation in the repository\nif that's something that you want to\nexplore on your\nso there's content in the notebook on\nthat but the last thing that I want to\ngo through is I've talked about those\nfirst four methods of imputation the\npattern sub-model approach is where I\nwant to end up for today and that\npattern sub-model approach for handling\nmissing data is you're gonna take your\ndata set and you break it into subsets\nof data based on how your data is\nmissing then what you're going to do is\nbuild one model on each of those subsets\ncreating many different models you won't\ncombine those models together you will\nend up with many different models so a\nvisual example look at the data set on\nthe left hand side I have a Y I have an\nx1 and an x2 what I can do is I can take\nmy first two rows what here because I've\nobserved the same data I'm gonna call\nthat pattern one my next two rows I'm\ngonna call that pattern to the next two\nrows our pattern 3 and then my final row\nis pattern 4 I'm gonna group my data\ninto four chunks based on how my data\nare observed in missing together and I'm\ngonna fit a different model on each of\nthose so you would end up with four\ndifferent models here one model would be\nif you wanted to do a linear regression\nmodel y equals beta naught plus beta 1\ntimes X 1 plus beta 2 times X 2 and that\nwould be fit on those first few\nobservations then you would fit a\nseparate linear regression model on\nthese next observations where you just\nexclude any value for X 2 because you\ndon't have any value for x 2 so that\nwould be a model like y equals beta\nnaught plus beta 1 times X 1 so you\nwould based on this come up with four\ndifferent models there are a lot of\nadvantages to this and if your goal is\njust to make predictions then you should\nuse the pattern sub model approach that\nI described here that this will\noutperform imputation methods if your\ndata are not missing at random\nand it's gonna perform about on par with\nimputation methods are filling in\nmethods if your data are missing at\nrandom or missing completely at random\nit does not require missingness\nassumptions so that's one of in my\nopinion one of the cool things about\nthat it is a based on my understanding a\nrelatively new method there was a paper\nreleased in I think September of 2018 on\nthat I think it was mentioned a a long\ntime ago in a more esoteric paper but to\nmy knowledge there's not a ton of\nmachinery like in Python to implement\nthis so it has to be a little bit more\nof a manual process so I know that we're\nright at the end of time here and I do\nwant to be respectful of focus time so\nfirst off thank you so much for for\nshowing up I totally understand if you\nneed to hop off so please feel free to\ndo that I'm happy to stick around and\ntie you feel free to come on if you mean\nto give me the hook and pull me off\n", "idn": "08", "video_url": "https://youtu.be/LJKYXq3WHTw", "title_kw": "missing data", "meta_json": "08.json", "audio_track": "08_LJKYXq3WHTw.mp4", "audio_text": "08_LJKYXq3WHTw.txt", "has_transcript": true, "status": "", "notes": ""}