{"presenter": "Hugo Bowne-Anderson and James Bourbeau", "title": "Data Science and Machine Learning at Scale", "year": "2020", "transcript_md": "16-hugo-james-dask.md", "meetup_url": "https://www.meetup.com/data-umbrella/events/273593514/", "yt_video_id": "MHAjCcBfT_A", "slides_url": "N.A.", "repo_url": "https://github.com/coiled/data-science-at-scale", "notebook_url": "https://github.com/coiled/data-science-at-scale/tree/master/notebooks/data-umbrella-2020-10-27", "transcriber": "?", "extra_references": "N.A.", "video_href_w": "25%", "video_href": "http://www.youtube.com/watch?feature=player_embedded&v=BxIoQ0gsxzA", "video_href_src": "", "video_href_alt": "Data Science and Machine Learning at Scale", "formatted_transcript": "okay\nhello and welcome to data umbrella's\nwebinar for\num october\nso i'm just going to go over the agenda\ni'm going to do a brief introduction\nthen there will be the workshop by hugo\nand james and you can ask questions\nalong the way\nin the chat or actually the best place\nto ask questions is the q\na and there's an option to upvote as\nwell\num so yet um asking the q a if you\nhappen to post it on the chat by mistake\ni can\nalso transfer it over to q a so that\nwould be fine too\nand this webinar is being recorded\nuh briefly about me i am a statistician\nand data scientist and i am the founder\nof data umbrella um\ni am on a lot of platforms as rachmas so\nfeel free to follow me on twitter and\nlinkedin we have a code of conduct\nwe're dedicated to providing harassment\nfree experience for everyone\num thank you for helping to make this a\nwelcoming friendly professional\ncommunity for all\nand this code of conduct applies to the\nchat\nas well\nso our mission is to provide an\ninclusive community for underrepresented\npersons in data science and we are an\nall volunteer run organization\nyou can support data umbrella by doing\nthe following things you can follow our\ncode of conduct and\nkeep our community a place where\neverybody wants to keep coming to\nyou can donate to our open collective\nand that helps to pay meet-up dues and\nother operational costs\nand you can check out this link here on\ngithub\nwe have this new initiative where all\nthe videos are being transcribed and so\nis to make them more accessible so we\ntake the youtube videos and we put the\nraw there and so we've had a number of\nvolunteers help us transcribe it\nso feel free to check out this link\nand maybe if you do this video\nmaybe the two speakers will follow you\non twitter i can't promise anything but\nit's possible\num dave umbrella has a job board and\nit's at jobs.org and once this gets\nstarted i'll put some links in the chat\nthe job that we are highlighting today\nis is the machine learning engineer job\nby\ndevelopment seed and development seat is\nbased in washington dc and lisbon\nportugal\nand they do i'm going to go to the next\nslide\nwhat they do is they're doing social\ngood work and so they're doing\nfor instance mapping elections from\nafghanistan to the u.s analyzing public\nhealth and economic data from palestine\nto illinois\nand leading the strategy and development\nbehind data world bank\nand some other organizations and i will\nshare a link to their job posting in the\nchat as well\nas soon as i finish this brief\nintroduction\ncheck out our website for resources\nthere's a lot of resources on learning\npython um and r also for contributing to\nopen source\nalso for guides on accessibility and\nresponsibility\nand allyship\nwe have a monthly newsletter that goes\nout towards the end of the month and it\nhas information\non our upcoming events we have two great\nevents coming up in\nnovember and december on open source so\nsubscribe to our newsletter um to be in\nthe know\nwe are on all social media platforms as\ndata umbrella\num meetup is the best place to join to\nfind out about upcoming events\nour website has resources follow us on\ntwitter\nwe also share a lot of information on\nlinkedin\nand if you want to subscribe to our\nyoutube channel we record all of our\ntalks and post them there\num within about a week of the talk so\nit's a good way to get information\nokay and now we are ready to get started\nso\ni will hand it over to um put myself on\nmute and i will hand it over to\nhugo and james and let you take over but\nthank you all for joining i just want to\nthank reishima um\nchristina and and everyone else who tied\nall the tireless effort that that goes\ninto putting\num these meet-ups and these online\nsessions together\ni i think um one thing i want to say is\nactually the\nthe last in-person workshop i gave\neither at the end of february or early\nmarch\num was data umbrellas in inaugural\ntutorial and meetup if i if i recall\ncorrectly on on bayesian bayesian\nthinking and um hacker statistics and\nsimulation and that type of stuff\nso it's it's just wonderful to be back\nparticularly with um\nmy colleague and friend friend james\nwe're building really cool um\ndistributed\nuh data science products um at coiled\nwe'll say a bit about that\num but we'll do some introductions in in\na bit i just wanted to\num get you all accustomed to it was\nfebruary thank you reishma\num uh we're working um with uh jupiter\nnotebooks\nin a github repository the repository is\npinned uh to the top of the chat this is\num\nwhat it looks like these are all the\nfiles this is the file system\nnow we use something called binder\nwhich is a project um\nout of and related to project project\njupiter which provides infrastructure\nto run um notebooks without any local\ninstalls so there are two ways\nyou can you can code along on this\ntutorial the first is and i won't get\nyou to do this\nyet um is to launch binder the reason i\nwon't get you to do that yet is because\nonce you launch it we have 10 minutes to\nstart coding or the binder session times\nout i've been burnt\nby that before um actually several times\ni'm surprised i even remembered it this\ntime the other thing you can do\nis install everything locally by cloning\nthe repository downloading anaconda\ncreating\na condor environment um if you haven't\ndone that um\ni suggest you do not do that now and you\nlaunch launch the binder um\njames is going to start by telling us a\nfew\na few things about about gas and\ndistributed compute in general\nmy question for you james is if we get\npeople to launch this now\nwill we get to execute a cell\ncode cell in 10 minutes um i would let's\nhold off for now maybe\nyep maybe i'll indicate when we should\nuh launch binder okay\nfantastic cool um and just what i'm\nlooking at right now\nis the github repository on your browser\nokay\nexactly so i will not launch binder now\ni will not get you to now\ni've i'm doing this locally um and we\nsee that i'm in uh notebook zero and if\nyou want to actually have a look at this\nnotebook before launching binder\nit's in the notebooks data umbrella uh\nsubdirectory and its notebook zero and\nwe're going to hopefully make it through\nthe overview then chatting about dusk\ndusk delayed\num and and data framing and machine\nlearning\num great so we have uh hashim has said\nyou could open in vs code as well\nyou could i mean that would require all\nyour local installs and that that type\nof stuff\nas well um but uh we're to introduce\nuh me and james um we we work at coiled\num\nwhere we uh build products for\ndistributed compute in infrastructure as\nwe'll see one of the big problems with\nlike bursting to the cloud is all the\nlike\nkubernetes aws docker stuff so we build\na one-click host of deployments for das\nbut for data science and machine\nlearning\nin general um james maintains task um\nalong with matt matt rocklin um who\ncreated dusk uh\nwith a team people who was working with\ncontinuum anaconda at the time\num and uh james is a software engineer\nat\ncalled and i run data science evangelism\nmarketing work on a bunch of product\nproduct stuff as well um\nwear a bunch of different different hats\noccasionally\num there are many ways to think about uh\ndistributed compute and how to do it in\nin python\nwe're going to present um hey james\nyou're muted\ni'm taking it i went away based on what\ni see in the chat you did you did but\nnow we're back i've introduced you\ni've introduced me i've mentioned that\nthere are many ways to do distributed\ncompute\num in the python ecosystem and we'll be\nchatting about one\ncalled dask um and maybe i'll pass you\nin a second but i'll say one thing that\ni really like about\nmy background isn't in distributed\ncompute my background's in\npythonic data science um when thinking\nabout bursting to larger data sets and\nlarger models\nthere are a variety of options the thing\nthat took me\nattracted me to desk uh originally\ni saw cameron's note the ghost in the\nmachine aren't playing nice tonight i\nthink that ain't that the truth\num is that dark plays so nicely\nwith the entire pi data ecosystem so as\nwe'll see\nif you want to write dash code for data\nframes dash data frames it really mimics\nyour pandas code um same with numpy same\nwith scikit-learn\nokay and the other thing is dark\nessentially\nruns the python code under the hood so\nyour mental model of what's happening\nis actually corresponds to the code um\nbeing\num being executed okay um\nnow i'd like to pass over to james but\nit looks like he's disappeared\nagain i'm still here if you can hear me\ni've just turned my camera off\noh yeah okay great i'm gonna turn my\ncamera hopefully that will help\nyeah and i might do do the same for\nbandwidth bandwidth issues so\nif if you want to jump in and and\ntalk about dark at a high level um i'm\nsharing my screen and we can scroll\nthrough\nyeah that sounds great so um that's sort\nof\nuh a nutshell you can think of it as\nbeing composed of\ntwo main uh uh well components\nthe first we call collections these are\nthe\nuser interfaces that you use to actually\nconstruct a computation you would like\nto compute in parallel or on distributed\nhardware\nthere are a few different interfaces\nthat desk implements uh for instance\nthere's dask array\nfor doing nd array computations there's\ndas data frame for working with tabular\ndata\nyou can think of those as like gask\narray as a parallel version of numpy\ndas data frame has a parallel version of\npandas and so on\nthere are also a couple other interfaces\nthat uh we'll be talking about das\ndelayed for instance we'll talk about\nthat today we'll also talk about the\nfutures api\nthose are sort of for lower level uh\ncustom algorithms\nin sort of paralyzing existing uh\nexisting code\nthe main takeaway is that there are\nseveral sort of familiar apis that desk\nimplements and that will use today\nto actually construct your computation\nso that's the first\npart of desk it is these dash\ncollections you then take these\ncollections\nuh uh set up your steps for your\ncomputation\nand then pass them off to uh the second\ncomponent which are\ndesk schedulers and these will actually\ngo through and\nexecute your computation potentially in\nparallel\nthere are two flavors of schedulers that\ndesk offers the first\nis a are called single machine\nschedulers\nand these just take advantage of your\nlocal hardware they will\nspin up a a local thread or process pool\nand start submitting tasks in your\ncomputation to to be executed in\nparallel\neither on multiple threads or multiple\nprocesses there's also a distributed\nscheduler\nor maybe a better term for would\nactually be called the advanced\nscheduler because it works well on a\nsingle machine\nbut it also scales out to uh multiple\nmachines so for instance as you'll see\nlater we will actually\nspin up a uh distributed scheduler that\nhas workers on uh remote\nmachines on aws so you can actually\nscale out beyond your\nlocal resources like say what's on your\nlaptop\num kind of scrolling down then to the\nimage of the\nuh cluster we can see the main\ncomponents of\nthe distributed scheduler and james i\nmight get people to spin up the binder\nnow\nbecause we're going to execute codes now\nis a good point yep\nso just here's a quick break point\nbefore you know a teaser for\num schedulers and what's happening there\ni'll ask you\nto um in the repository there's also the\nlink to the binder\nclick on launch binder i'm going to open\nit in a new tab and\nwhat this will create is an environment\nin which you can just execute the code\nin in the notebooks okay\nso hopefully by the time we've gotten\ngone through this section\nthis will be ready to start executing\ncode so if everyone wants to do that to\ncode along otherwise just\nwatch or if you're running things\nlocally also cool thanks james\nyeah yeah no problem thank you so so\nyeah looking at the image\nfor the distributed scheduler we're not\ngonna have time to go into\nthe um a lot of detail about the\ndistributed scheduler in this workshop\nso but we do want to provide at least a\nhigh level overview of the\nthe different parts and components of\nthe distributed scheduler\num so the first part i want to talk\nabout is in the diagram what's labeled\nas a client\nso this is the user facing entry point\nto a cluster\nso um wherever you are running your\npython session\nthat could be in a jupiter lab session\nlike we are here\nthat could be in a python script\nsomewhere you will create\nand instantiate a client object that\nconnects\nto the second component which is the das\nscheduler\nso each desk cluster has\na single scheduler in it that sort of uh\nkeeps track of all of the state for all\nof the\nthe state of your cluster and all the\ntasks you'd like to compute so from your\nclient you might start submitting tasks\nto the cluster the schedule will receive\nthose tasks\nand compute things like all the\ndependencies needed for that task like\nsay you're uh implementing you say you\nwant to compute\ntask c but that actually requires first\nyou have to compute task b\nand task a like there are some\ndependency structures there\nit'll compute those dependencies as well\nas keep track of them\nit'll also uh communicate with all the\nworkers to understand\nwhat worker is working on which task and\nas\nspace frees up on the workers it will\nstart farming out uh\nnew tasks to compute to the workers um\nso\nin this particular diagram there are\nthree das distributed workers here\num however you can have as you can have\nthousands of workers if you'd like\nso the workers are the things that\nactually compute the tasks\nthey also store the results of your\ntasks and then serve them back to you\nand the client\nthe scheduler basically manages all the\nstate needed to\nperform the computations um and you\nsubmit tasks\nfrom the client so that's sort of a\nquick\nwhirlwind tour of the different\ncomponents for the distributed scheduler\num and at this point i think it'd be\ngreat to actually see\nsee some of this in action um hugo would\nlike to take over\nabsolutely thank you for that wonderful\nintroduction to darsk and\nand the schedulers in particular and we\nare going to see that um with dark in\naction\nuh i'll just note that this tab in which\ni launched the binder is up and running\nif you're going to execute code here\nclick on notebooks\nclick on data umbrella oop\nand then go to the overview notebook\nand you can drag around we'll see the\nutility of these these dashboards\nin a second but you can you know drag\nyour stuff around to\nto make you know however you want to\nwant to structure it and then you can\nexecute code\nin here i'm not going to do that i'm\ngoing to do this\nlocally at the moment but just to see\ndust in action\nto begin with i'm going to i'm actually\ngoing to\nrestart kernel and clear my outputs\num so i'm going to import uh from dash\ndistributed the client\nthe sorry the other thing i wanted to\nmention is um we made a decision around\ncontent for this\nwe do have a notebook that we we love to\nteach on schedulers but we decided to\nswitch it out for machine learning for\nthis workshop in particular we are\nteaching a similar although distinct\nworkshop um at pi data global\nso we may see some of you there in which\nwe'll be going um more in depth\ninto schedulers as well um so if you\nwant to check that out\ndefinitely do so we instantiate the\nclient\nwhich as james mentioned is kind of what\nwe work with as the user um to submit\nour code\num so that will take take a few seconds\num okay it's got a port in you so it's\ngoing going elsewhere\nwhat i'll just um first get you to\nnotice is that it\ntells us where our dashboard is um and\nwe'll see those tools in a second\ntells us about our cluster that we have\nfour workers eight cores\num between eight and nine gigs of of ram\nokay um now this is something i really\nlove about dusk all the diagnostic um\ntools if i click on the little desk\nthing here\nand we've um modified the binder so that\nthat exists there as well\num we can see i'll hit search and it\nshould that now corresponds to\nthe the scheduler now i want to look at\nthe task\nstream which will tell us in real time\nwhat's happening i also want to look at\nthe\ncluster map so we see um here this is\nalready really cool\num we've got uh all of our workers\naround here and our scheduler\nscheduler there and when we start um\ndoing some compute we'll actually see\ninformation flowing between these um and\nthe other thing\nmaybe i'll yeah i'll\ninclude a little progress um\nand that can be an alternate tab to um\nask um i'm wondering\nperhaps i also want to include something\nabout the workers\nyeah okay great so we've got a bunch of\nstuff\nthat's that's pretty interesting there\nand so\nthe next thing i'm going to do we've got\na little utility file which um downloads\nsome of the data\nand this is what it does is if you're in\nbinder it downloads a subset of the data\nif you're anywhere else it loads a\nlarger set\num for this particular example we're\ndealing with a small data set\nyou see the utility of dark and\ndistributed compute when it generalizes\nto larger data sets\nbut for pedagogical purposes um we're\ngoing to sit with a smaller data set so\nthat we can actually run\nrun the code there's a trade-off there\num so\nactually that was already downloaded it\nseems but you should\nall see it download i'm actually going\nto run that in the binder\njust to you should start seeing\ndownloading nyc flights data set\ndone extracting creating json data etc\nokay now what we're going to do\nis we're going to read in this data as a\ndask data frame and\nwhat i want you to notice is that it\nreally the das code mimics pandas code\nso instead of pd read csv we've got dd\nread csv\num we've got you know this is the file\npath um\nthe first argument we're doing some\nparse date setting some data types\nokay um we've got a little um\nwild card regular expression there to to\njoin uh\nto do a bunch of them um and then we're\nperforming a group by\nokay so we're grouping by the origin of\nthese flight flight data\nwe're looking at the the mean departure\ndelay group by origin\nthe the one difference i want to make\nclear is that\nin das we need a compute method\num that's because das performs lazy\ncomputation it won't actually\ndo anything because you don't want it to\ndo anything on really large data sets\nuntil you explicitly tell it tell it to\ncompute so i'm going to execute this now\nand we should see some information\ntransfer between the scheduler and the\nworkers and we should see tasks\nstarting starting to be done okay\nso moment of truth\nfantastic so we call this a pew pew plot\nbecause we see pew pew pew\num we saw a bunch of data transfer\nhappening between them these are all our\ncause\nand we can see tasks happening um it\ntells us what tasks there are we can see\nthat most of the time was spent\nuh reading reading csvs then we have\nsome um\ngroup bias on chunks and and that type\nof stuff\num so that's a really nice uh diagnostic\ntool to see what most of your work\num is is actually doing uh under dark\nwork as you can see memory used cpu use\num uh\nmore fine-grained examples there um so\ni i'd love to know if um\nin the q a um\ni'm going to ask were you able to\nexecute this code and if you were in\nbinder just a thumb up\na vote would be no would be fantastic\num much appreciated um\nso as we've mentioned i just wanted to\nsay a few things about tutorial goals\num the goal is to cover the basics of\ndark and distributed compute we'd love\nfor you to walk away with an\nunderstanding of when to use it when to\nnot what it has to offer we're going to\nbe covering um the basics of dusk\ndelayed\nwhich although not immediately um\napplicable to data science\nprovides a wonderful framework um\nfor thinking um about dusk how dark\nworks and understanding how it works\nunder the hood\nthen we're going to go into dark data\nframes and then machine learning\nhopefully um due to the technical um\nconsiderations with um we've got less\ntime than\nthan we thought we would but um we'll\ndefinitely do the best we can\nwe may have less time to do uh exercises\nso we've had two people who are able to\nexecute this code\nif you if you tried to execute it in\nbinder and were not able to\nperhaps post that in the q a um\nbut um we also have several exercises\num and i'd like you to take a minute\njust to do this exercise\nthe i i'm not asking you to do this\nbecause i want to know if you're able to\nprint hello world i'm essentially asking\nyou to do it\num so you get a sense of how these\nexercises work so\nif you can take 30 seconds to print\nhello world\num then uh we'll we'll move on after\nthat so just take um\n30 seconds now\nand it seems like we have a few more\npeople who are able to execute code\nwhich which was great\nokay fantastic so you will put your\nsolution there for some reason i have um\nan extra cell here so i'm just going to\nclip that\nand to see a solution uh i'll just get\nyou to execute\nthis cell and it provides the solution\nand then we can execute it and compare\nit to the\nthe output of what you had okay hello\nworld um\nso as as we saw i've done all this\nlocally you may have done it on binder\num\nthere is an option to work directly from\nthe cloud um and i'll i'll take you\nthrough this there are many ways to do\nthis\nas i mentioned we're working on one way\nwith coil and i'll explain the rationale\nbehind that\nin in a second but i'll show you how\neasy it is\nto get a cluster up and running on on\naws without even interacting with\naws for free for example you can follow\nalong by uh signing into uh coiled cloud\nto be clear this is not a necessity and\nit does involve you signing up to our\nproduct so i just wanted to be\nabsolutely transparent\nabout that it does not involve any\ncredit card information or anything\nalong those lines and in my opinion it\ndoes give a really nice\nuh example of how to run stuff on the\ncloud um\nto do so you can sign in at cloud dot\ncoiled\nuh dot io you can also pip install\ncoiled and then\ndo authentication you can also spin up\nthis\nthis hosted coiled notebook so i'm\ngoing to spin that up now and i'm going\nto post that\nhere um actually yep i'm gonna post that\nin the ch\nchat um if you let me get this right\num if you've if you've never logged in\nto code before it'll ask you to sign up\nusing gmail or github so feel free to do\nthat if you'd like\nif not that's also also cool um but i\njust wanted to be explicit\nuh about that um the reason i want to do\nthis\nis to show how dars can be leveraged to\ndo work on\nreally large data sets so you will\nrecall that i had between eight and nine\ngigs of ram on my local system\num oh wow anthony says on ipad unable to\nexecute on binder\nincredible um i don't have a strong\nsense of how binder works on ipad\ni do know that i was able to um\nto check to use a binder on my iphone\nseveral years ago on my way to scipy\ndoing code review for someone for eric\nmaher i think for what that\nthat's worth um but back to this\num we have this nyc taxi data set which\nis over 10 gigs it won't even\ni can't even store that in local memory\ni don't have enough ram to store that\nso we do need um either to do it\nlocally in an out of core mode of some\nsort or we can we can burst to the cloud\nand we're actually going to burst to the\ncloud\nusing using coiled um so the notebook\nis running here um for me and but i'm\nactually gonna do it uh from my local\nlocal notebook\nbut you'll see and once again feel free\nto code along here\nit's spinning up a notebook and james\nwho is\nis my co-instructor here um is to be i'm\ni'm so grateful\nall the work is done on our notebooks in\ncoiled you can\nlaunch the cluster here and then analyze\nthe entire um over 10 gigs of data there\ni'm going to do it um\nhere so to do that i import coiled\nand then i import the dash distributed\nstuff and then\ni can create my own software environment\ncluster configuration i'm not going to\ndo that\nbecause the standard coiled cluster\nconfiguration software environment works\nnow i'm going to spin up a cluster and\ninstantiate a client\nnow because we're spinning up a cluster\nuh in in the cloud\num it'll take it'll take a minute a\nminute or two\nenough time to make a cup of coffee but\nit's also enough time for me to just\ntalk a bit about why this is important\num and there are a lot of a lot of good\ngood people working on\non similar things um but part of the\nmotivation here is that\nif you want to you don't always want to\ndo distributed data science okay um\nfirst i'd ask you to look at instead of\nusing dark if you can optimize your\npandas code\nright um second i'd ask if you've got\nbig data sets\nit's a good question do you actually\nneed all the data so\ni would if you're doing machine learning\nplot your learning curve see how\naccurate see how your accuracy um\nor whatever your metric of interest is\nimproves as you increase\nthe amount of data right um and if it\nplateaus before you get to a large data\nsize then\nyou may as well most of the time use\nyour small data um\nsee if sub sampling um can actually give\nyou the results you need\num so you can get a bigger bigger access\nto a bigger machine\nso you don't have to burst to the cloud\nbut after\nall these things if you do need to boast\nburst to the cloud\nuntil recently you've had to get an aws\naccount\num you've had to you know set up\ncontainers with docker and or\nkubernetes um and do all of these kind\nof\ni suppose devopsy software engineering\nfoo\nstuff um which which if you're into that\ni\ni absolutely encourage you encourage you\nto do that\nbut a lot of working data scientists\naren't paid to do that um\nand um i don't necessarily want to\num so that's something we're working on\nis thinking about these kind of\none-click hosted deployments so you\ndon't have to do\nall of that um having said that um i\nvery much encourage you to try doing\nthat stuff if\nif you're interested um we'll see that\nthe\nthe um cluster has just been created\num and what i'm going to do we see that\num oh i'm sorry\ni've done something funny here i'm\ni'm referencing the previous client anna\njames\nyeah it looks like you should go ahead\nand connect a new client to the coil\ncluster and making sure not to\nre-execute the cluster\ncreation exactly so\nwould that be how would i\nwhat's the call here i would just open\nup a new\ncell and say client equals\num capital client and then pass in the\ncluster\nlike open parentheses cluster yeah\ngreat\nokay fantastic and what we're seeing is\na slight version this\nwe don't need to worry about this this\nis essentially saying that um\nthe environment on the cloud mis is\nthere's a slight mismatch with my\nwith my local environment we're fine\nwith that i'm going to\num look here for a certain reason\num the the dashboard isn't quite working\nhere at the moment james would you\nsuggest i just click on this and open a\nnew\nyeah click on the ecs uh dashboard link\noh yes fantastic\nso um yep there's some\nbug with the local dashboards that we're\nwe're currently\ncurrently working on but what we'll see\nnow\njust a sec i'm going to remove all of\nthis\nwe'll see now that i have access to 10\nworkers i have access to 40 cores\nand i have access to uh over 170 gigs\nof memory okay so now i'm actually going\nto\nimport this data set and it's the entire\num year of data from 2019\nand we'll start seeing on on the\ndiagnostics all the all the processing\nhappening okay so oh\nactually not yet because we haven't um\ncalled compute okay so it's done this\nlazily um\nwe've imported it um it shows kind of\nlike pandas when you\nshow a data frame um the column names\nand data types\num but it doesn't show the data because\nwe haven't loaded it\nyet it does tell you how many partitions\nit is so essentially and we'll see this\nsoon\ndas data frames correspond to\ncollections of pandas data frames\num so they're really 127 pandas data\nframes underlying this task data frame\nso now i'm going to do the compute well\ni'm going to\nset myself up for the computation um to\ndo a group by passenger gown and look at\nthe main tip\nnow that took a very small amount of\ntime we see the ipython magic\ntiming there because we haven't computed\nit now we're actually going to compute\num and james if you'll see in the chat\neliana said her coil\ncoiled authentication failed i don't\nknow if you're able to\nto help with that but if you are that\nwould be great\num and it may be difficult to debug in\nbut look as we see we have the task\nstream now\num and we see how many you know we've\ngot 40 cores\nworking together we saw the processing\nwe saw the bytes stored\nit's over 10 gigs as i said um and we\nsee we were able\nto do our um\nbasic analytics um\nwe were able to do it on a 10 plus gig\ndata set in in 21.3 seconds\nwhich is pretty pretty exceptional um\nif any any code based issues come up\nand they're correlated in particular so\nif you have questions about the\ncode execution please ask in the q a um\nnot in the chat because others cannot\nvote it and i will definitively\nprioritize\nquestions on technical stuff\nparticularly ones that up that are\nupvoted\num but yeah i totally agree thanks\nthanks very much\num so yeah let's jump into\ninto um data frames\nso of course we write here that in the\nlast exercise um we used ask delayed to\nparallelize uh loading multiple csv\nfiles into a pandas data frame\num we're not we we haven't done that but\nyou can definitely go through and have a\nlook at that\num but i think perhaps even\nmore immediately relevant for a data\nscience crowd and an analytics crowd is\nwhich is what i see here from the\nreasons people people have joined um is\njumping into dusk data frames\num and as i said before adas data frame\num\nreally feels like a pandas data frame um\nbut internally it's composed of many\ndifferent\ndifferent data frames this is one one\nway to think about it that we have all\nthese pandas data frames\num and the collection of them is a dark\ndata frame\nand as we saw before they're partitioned\nwe saw\nwhen we loaded the taxi data set in the\ndash data frame was 127 partitions right\num where each partition was a normal\npanda pandas data frame\num and they can live on disk as they did\nearly uh in the first example dark in\naction or they can live on other\nmachines as when i spun up\na coiled cluster and and did it on on\naws\num something i love about darth's data\nframes i mean i ran about this\nall the time um it's how it's the pandas\napi and and matt\nmatt rocklin actually um uh\nhas a post on on the\nblog called a brief history of dusk in\nwhich he talks about the technical goals\nof\nus but also talks about a social goal of\ntask which in matt's words is to invent\nnothing he wanted and the team wanted\num the dusk api to be as\ncomfortable um and familiar for users\nas possible and that's something i\nreally appreciate\nabout it so um we see we have element\nelement uh wires on operations we have\nthe\nour favorite row eyes selections we have\nloc we have the common aggregations we\nsaw group buyers before we have\nis-ins we have date time string\naccessors\num oh james we forgot to i forgot to\nedit this and i\nit should be grouped by i don't know\nwhat what a fruit buy is but that's\nsomething um\nwe'll make sure the next iteration to to\nget right at least we've got it right\nthere and in the code\num but have a look at the dash data\nframe api docs to check out what's\nhappening\num and a lot of the time dash data\nframes can serve as drop in replacements\nfor pandas data frames\nthe one thing that i just want to make\nclear as i did before\num is that you need to call compute\nbecause of the\nlazy laser compute property of das\nso this is wonderful to talk about when\nto use\ndata frames so if your data fits in\nmemory\nuse pandas um if your data fits in\nmemory and your code\ndoesn't run super quickly um\ni wouldn't go to dusk i'd try to i'd do\nmy best to optimize my pandas code\nbefore trying to get gains gains and\nefficiency um\nbut dark itself becomes useful when the\ndata set you want to analyze is larger\nthan your machine's ram\num where you normally run into memory\nerrors and that's what we saw\nwith the taxicab example the other\nexample that we'll see when we get to um\n[Music]\nmachine learning is\nyou can do machine learning on a small\ndata set that fits in memory but if\nyou're\nbuilding big models or training over\nlike a lot of different hyper parameters\nor different types of models\nyou can you can parallelize that using\nusing dark so there is\nyou know you want to use dash perhaps in\nthe big data or medium to big data limit\num as we see here um or in the medium to\nbig model limit where training\nfor example takes and takes a lot of\ntime okay\nso without further ado uh let's get\nstarted with das data frames\num you likely ran this uh preparation\nfile to get the data in the previous\num notebook but if you didn't execute\nthat um\nnow we're going to get our file names by\ndoing\ndoing a few joins and we see our file is\na string data nyc\nflights um a wildcard\nto access all of them dot dot csv\nand we're going to import our dusk\ndust.dataframe and read in our dataframe\num parsing some dates setting some\nsending some data types\nokay i'll execute that we'll see we have\n10\npartitions um as we noted before\nif this was a pandas data frame we'd see\na bunch of entries here\nwe don't we see only the column names\nand the data types of the columns um and\nthe reason is\nas we've said it explicitly here is the\nrepresentation of the data frame object\ncontains no data\num it's done dusk has done enough work\nto read the start of the file\num so that we know a bit about it some\nof the important stuff and then further\ncolumn types and\ncolumn names and data types okay but we\ndon't once again we don't let's say\nwe've got 100 gigs of data\nwe don't want to like do this call and\nsuddenly it's reading all that stuff in\nand\ndoing a whole bunch of compute until we\nexplicitly\nuh tell it to okay now this is really\ncool if you know a bit of pandas\nyou'll know that you can um there's an\nattribute columns which\nprints out it's well it's actually the\ncolumns form an index right the pandas\nindex\nobject um and we get the we get the\ncolumn names there\ncool pandas in dark form\nwe can check out the data types as well\num as we would in pandas we see we've\ngot some ins for the day of the week\nwe've got some floats for departure time\num maybe we'd actually um prefer that to\nbe\nyou know a date time at some point we've\ngot some objects which generally are the\nmost general on\nobjects so generally strings um\nso that's all pandasey type stuff in\naddition das data frames have\nan attribute um n partitions which tells\nus the number of partitions and we saw\nbefore\nthat that's 10 so i'd expect to see 10\nhere hey look at that\num now this is something that\num we talk about a lot in the\ndelayed notebook is really the task\ngraph\nand i don't want to say too much about\nthat but really it's a\nvisual schematic of of the order in\nwhich different types of compute happen\nokay um and so the task graph for\nread csv tells us what happens when we\ncall compute\nand essentially it reads csv um\n10 ten times zero indexed of course\nbecause python\num it reads csv uh ten different times\ninto these ten different pandas pandas\ndata frames\nand if there were group buys or stuff\nafter that we'd see them happen in\nin the in the graph there and we may see\nan example of this in a second\num so once again as with pandas\num we're going to view the the head of\nthe data frame\ngreat and we see a bunch of stuff um\nyou know we we see the first first five\nrows um\ni'm actually also gonna gonna have a\nlook at the\nthe tail the final five rows that may\ntake longer um\nbecause it's accessing the the final i\num\ni there's a joke and it may not even be\na joke how much\num data analytics is actually biased by\npeople looking at the first five rows\nbefore actually\nyou know interrogating the data uh more\nmore seriously um so how would all of\nour results look different\nif um if our files were ordered in\nin a different way that's another\nconversation for a more philosophical\nconversation for another time\num so now i want to show you some\ncomputations\nwith uh dark data frames okay so\nsince dash data frames implement a\npandas like api\num we can just write our familiar pandas\ncodes so\ni want to look at the column um\nuh departure delay and look at the\nmaximum of that column\ni'm going to call that max delay so you\ncan see we're selecting the column\nand then applying the max method as we\nwould with pandas oh what happened there\ngives us some uh da scala\nseries um and\nwhat's happened is we haven't called\ncompute right so it hasn't actually done\nthe compute yet um\nwe're going to do compute but first\nwe're going to visualize the task graph\nlike we did\nhere and let's try to reason what the\ntask graph would look like right so\nthe task graph first it's going to read\nin\nall of these things and then\nit'll probably perform this selector\non each of these\ndifferent pandas data frames comprising\nthe dash data frame\nand then it will compute the max of each\nof those and then do a max on all those\nmaxes\ni think that's what i would assume is\nhappening here\ngreat so that's what we're what we're\ndoing we're reading this so we read the\nfirst\num perform the first read csv get this\ndas data frame\num get item i think is that selection\nthen we're taking the max\nwe're doing the same for all of them\nthen we take all of these max's\nand aggregate them and then take the max\nof that okay so that\nthat's essentially what's happening when\ni call compute which i'm going to do\nnow\nmoment of truth okay\nso uh that took around eight seconds and\nit tells us the max\nand i i'm sorry let's let's just get\nout some of our dashboards up\nas well um\nhuh i think in this notebook we are\nusing the single machine scheduler hugo\nso i don't think there is a dashboard to\nbe seen exactly\nyeah thank you for that that that catch\njames um\ngreat um is even better\num uh james we have a question around\nusing dark for\num reinforcement learning can you\ncan you speak to that um yeah so\nuh it depends on this i mean yeah short\nanswer\nyes you can use gas to train\nreinforcement learning models\num so there's a package that hugo will\ntalk about called desk ml that we'll see\nin the next notebook uh for distributing\nmachine learning\num that paralyzes and and distributes\num some existing models uh using desks\nso for instance things like\nrandom forces forest inside kit learn\num so so yes you can use das to uh\nuh do distributed training for models\ni'm not actually sure if gaskml\nimplements\nany reinforcement learning models in\nparticular\num but that is certainly something that\nthat can be done\nyeah and i'll i'll build on that by\nsaying we are about to jump into machine\nlearning\num i don't think as james said i don't\nthink\nthere's reinforcement learning um\nexplicitly that\nthat one can do um but um you of course\ncan use the das\nscheduler yourself to um you know to\ndistribute any reinforcement learning\nstuff\nyou you have as well and that's actually\nanother another point to make that maybe\njames can speak to a bit more is that um\nthe dark team of course built all of\nthese high-level collections and task\narrays and\ndust data frames and were pleasantly\nsurprised when\nyou know maybe even up to half the\npeople using dust came in all like we\nlove all that but we're going to use\nthe scheduler for our own bespoke use\ncases right\nyeah exactly yeah the original intention\nwas to like make basically a num\nlike a parallel numpy so that was like\nthe desk array stuff like run\nrun numpy and parallel on your laptop um\nand and yeah so in order to do that we\nended up\nbuilding a distributed scheduler um\nwhich sort of does\narbitrary task uh computations so\nnot just things like uh you know\nparallel numpy but\nreally whatever you'd like to throw at\nit and uh it turns out that ended up\nbeing really\nuseful for people um and so yeah now\npeople use that\num sort of on their own uh just using\nthe distributed scheduler to do\ntotally custom algorithms um in parallel\num\nin addition to these like nice\ncollections like you saw hugo presents\nthe dash data frame um api is you know\nthe same as the panda's api so there is\nthis like familiar space you can use\nthings\nlike the high-level collections but you\ncan also run\nuh whatever custom like hugo said\nbespoke computations\nyou might have exactly and it's it's\nbeen wonderful to see\nso many people so many people do that\nand the first thing\nas we'll see here the first thing to\nthink about is if\nif you're doing lifestyle compute if\nthere's anything you can you know\nparallelize embarrassingly as they say\nright so just\nif you're doing a hyper parameter search\nyou just\nrun some on one worker and some on\nthe other and there there's no\ninteraction effect so you don't need to\nworry about that as opposed to\nif you're trying to do um\nyou know train on streaming data where\nyou may require it all\nto happen on on on the same worker okay\num yeah so even think about trying to\ncompute the standard deviation of a\nof a a univariate data set right um\nin in that case um you can't just send\nyou can't just compute the standard\ndeviation on two workers and then\ncombine the result in some some way you\nneed to do something slightly slightly\nmore nuanced and slightly\nslightly clever more clever um i mean\nyou still can actually in\nin that case but you can't just do it as\nnaively as that\num but so now we're talking about\nparallel and distributed machine\nlearning we have 20 minutes left so this\nis kind of going to be a whirlwind tour\nbut um you know whirlwinds when safe uh\nexciting and informative um i just want\nto make clear the material in this\nnotebook is based on the open source\ncontent from darsk's\ntutorial repository as there's a bunch\nof stuff we've shown you today\nthe reason we've done that is because\nthey did it so well so i just want to\ngive a shout out to all the das\ncontributors\nokay so what we're going to do now is um\njust break down machine learning scaling\nproblems into two categories\njust review a bit of psychic learn in\npassing um\nsolve a machine learning problem with\nsingle michelle single michelle\num i don't know who she is but single\nmichelle wow\nsingle machine and parallelism with\npsychic learning job lib\nthen solve an l problem with an ml\nproblem with multiple machines and\nparallelism using uh dark as well\nand we won't have time to burst for the\ncloud i don't think but you can also\nplay\nplay around with that okay so as i\nmentioned before\nwhen thinking about distributed compute\na lot of people do it when they have\nlarge data they don't necessarily think\nabout the large model limit\num and this schematic kind of speaks to\nthat um\nif you've got a small model that fits in\nram you don't need to think about\ndistributed compute\nif your data size if your data is larger\nthan your ram\num so your computer's ram bound then you\nwant to start going to a distributed\nsetting or if your model is big and cpu\nbound um such as like large-scale\nhyper-parameter searches or like\nensembl blended models of like machine\nlearning algorithms\num whatever it is and then of course we\nhave the\nyou know big data big model uh limit\nwhere um distributed computer desk is\nincredibly handy as i'm sure\nyou could uh imagine okay and\nthat's really what i've what i've gone\nthrough here\num a bird's-eye view of the strategies\nwe think about um\nif it's in memory in the bottom left\nquadrant just use scikit-learn or your\nfavorite ml library\num otherwise known as psychic learn um\nfor me anyway\num\ni was going to make a note about xg\nboost but i but i won't\num for large models\nuh you can use joblib and your favorite\ncircuit learn estimator\nfor large data sets uh use our dark ml\nestimators so we're gonna do a whirlwind\ntour of psychic learn in\nin five minutes we're going to load in\nsome data so we'll actually generate it\nwe'll import scikit-learn for our ml\nalgorithm create an estimator\nand then check the accuracy of the model\nokay so once again i'm actually going to\nclear all outputs after restarting the\nkernel\nokay so this is a utility function of\npsychic learn to create some data sets\nso i'm going to make um\na classification data set with four\nfeatures and 10 000 samples and just\nhave a quick view\num of some of it um\nso just a reminder on ml\num x is the samples matrix um the size\nof x\nis um the number of samples\nuh in terms of rows number of features\nas columns\num and then a feature or an attribute\nis uh what we're trying to predict\nessentially okay um so why um\nis the predictor variable uh which we're\nwhere\num which we're or the target variable\nwhich we're trying to predict so let's\nhave a quick view\nof why it's zeros and ones in in this\ncase\nokay so um yep that's what i've said\nhere\nwhy are the targets which are real\nnumbers for regression tasks or integers\nfor classification\nor any other discrete sets of values um\nno words about unsupervised learning at\nthe moment we're just going to support\nwe're going to\nfit a support vector classifier for this\nexample\nso let's just load the appropriate\nscikit-learn\nmodule we don't really need to discuss\nwhat\nsupport vector classifiers are at the\nmoment now this is one of the\nvery beautiful things about the\nscikit-learn api\nin terms of fitting the the model\nwe instantiate um a classifier and we\nwant to fit it\nto the features with respect to the\ntarget okay so the first argument is the\nfeatures second argument\nis the target variable\nso we've done that now i'm not going to\nworry about inspecting the learn\nfeatures um i just want to see how\naccurate it was okay and once we see how\naccurate it was i'm not gonna do this\nbut then we can um make a prediction\nright\nusing uh estimator dot predict on a new\na new data set\num so this estimator will tell us\nso this score will tell us the accuracy\nand essentially\nthat's the proportion or percentage\na fraction of um\nthe uh results that were that the\nestimator got correct and we're doing\nthis on the training\ndata set we've just trained the model on\nthis so this is telling us\num the accuracy on the on the training\ndata set okay so it's 90\naccurate on the training data set if you\ndive into this a bit more you'll\nrecognize that\num if we we really want to know the\naccuracy\non a holdout set or a test set\num and it should be probably a bit lower\nbecause this is what we use to fit it\nokay\nbut all that having been said i expect\num you know\nif if this is all resonating with you it\nmeans we can really move on to the\ndistributed stuff um um in in a second\num but the other thing that that's\nimportant to note is that we've trained\nit but\na lot of model a lot of estimators and\nmodels have um hyper parameters that\naffect the fit but you\nthat we need to specify up front um\ninstead of being learned during training\nso\nyou know there's a c parameter here\nthere's a uh\nare we using shrinking or not um so we\nspecify those\nwe didn't need to specify them because\nthere are default values but here we\nspecify them\nokay and um\nthen we're going to um\nlook at the score now\nokay this is amazing we've got 50\naccuracy um\nwhich is the worst score possible just\nthink about this if if you've got binary\nclassification task and you've got 40\naccuracy then you just flip the labels\nand that changes to 60 accuracy so it's\namazing that we've actually hit\n50 accuracy we're to be congratulated on\nthat\num and what i want to note here is that\nwe have two sets of hyper parameters\nwe've used one's\ncreated 90 actual model with 90 accuracy\nanother one one with 50 accuracy um\nso we want to find the best hyper\nparameters essentially and that's why\nhyper parameter optimization\nis is so important um there are several\nways to do hyper parameter optimization\none is called grid search uh cross\nvalidation i won't talk about cross\nvalidation\num it's essentially um a more robust\nanalogue of train test split where you\nuh train on a subset of your data and\ncompute the accuracy on a test\non a holdout set or a test set um cross\nvalidation is\na as i said a slightly more robust\nanalog of this\nit's called grid search because we have\na grid of hyper parameters so\nwe have you know in this case we have a\nhyper parameter c we have a hyper\nparameter kernel\nand we can imagine them in a in a grid\nand we're performing\num we're checking out um the score\nover all this gr over this entire grid\nof hyper parameters okay\nso to do that um i import grid search\ncsv\nnow i'm going to um\ncompute um the estimator over\nover these train the estimator over over\nthis grid um\nand as you see this is taking time now\nokay\num and what i wanted to make clear and i\nthink should be becoming\nclearer now is that if we have a large\nhyper parameter\nuh sweep we want to do on a small data\nset das can be useful for that\nokay because we can send some of the\nparameters to\none worker some to another and they can\nperform them um in parallel so that's\nembarrassingly parallel because\nyou're you're doing the same work as you\nwould otherwise\num but sending it to a bunch of\ndifferent workers we saw that took 30\nseconds which is in my realm of\ncomfort as a data scientist i'm happy to\nwait 30 seconds\num if i had to wait much longer if this\ngrid was bigger\ni'd start to get probably a bit\nfrustrated um\nbut we see that it computed um\nit for c is equal to all combinations of\nthese\nessentially okay um so that's really all\ni wanted to say there um and then we can\nsee the best parameters\nand the best score so the best score was\n0.098 and it was c10 and the kernel um\nrbf a radial basis function it doesn't\neven matter what that is though\num for the purposes of this so we've got\n10 minutes left we're going to\nwe're going to make it i can feel it i\nhave a good i have a good sense\num a good after the\ni mean this demo is actually going\nincredibly well given um the initial\ntechnical hurdles so touchwood hugo um\nokay so what we've done is we've really\nsegmented ml scaling problems into\ntwo categories cpu bound and ram bound\num and i\ni really i can't emphasize that enough\nbecause i see so many people\nlike jumping in to use new cool\ntechnologies um without\nperhaps taking it being a bit mindful\nand\nintentional about it and reasoning about\nwhen things are useful and and when not\num\ni suppose the one point there is that\nsure data science is a technical\ndiscipline but there are a lot of other\naspects to it\num involving this type of reasoning\nas well so we then carried out a typical\nsklearn workflow for ml problems\num with small models and small data and\nwe reviewed hyper parameters and hyper\nparameter\noptimization um so in this section\num we'll see how job lib which is a set\nof tools to provide lightweight\npipelining\num in python uh gives us parallelism on\nour laptop and then we'll see how dark\nml can give us um awesome parallelism\nuh on on clusters okay so essentially\num what i'm doing here is i'm doing\nexactly the same as above\nwith a grid search but i'm using the\nquark the keyword argument n\njobs which tells you how many tasks uh\nto run in parallel\nusing the cause available on your local\nworkstation and specifying minus one\njobs\nmeans the it just runs them the maximum\npossible\nokay so i'm going to execute that\ngreat\nso we should be done in a second feel\nfree to ask um any questions\nin the chat oh alex\num has a great question in the q a does\ndas have\nuh see a sequel and query optimizer\num i'm actually so excited that um\n[Music]\nand james maybe you can provide a couple\nof links to this um\nwe're really excited to have seen dark\ndust sql um\ndevelopments there uh recently um\nso that's dark hyphen hyphen sql um\nand we're actually we're working on some\nsome content and a blog post and maybe a\nlive live coding session\nabout that in in the near future um so\nif anyone if you want updates from from\ncoyle feel free to go to our website and\nsign up for our mailing list\nand we'll let you know about all of this\ntype of stuff but the short answer is\nyes alex and it's getting better and um\nif james is able to post post a link\nthere that would be that would be\nfantastic\num so we've done link in the chat\nfantastic um\n[Music]\nand so we've we've seen how\nwe have um\n[Music]\nsingle machine parallelism here um using\nthe\num using the end jobs quark um and in\nthe final minutes\nlet's see multiple multi-machine\nparallelism\nwith dusk okay um so\nwhat i'm going to do is i'm going to\nuh do my imports and create my\nclient incentive my client and check it\nout\nokay so once again i'm working locally\num i hit search and that'll\ntask is pretty smart in terms of like\nknowing uh which which client i want to\ncheck out\ndo the tasks stream\nbecause it's my favorite i'll do the\ncluster map otherwise known as the pew\npew map\num and then\ni want some progress we all we all crave\nprogress don't we um\nand\nmaybe my workers tab okay great so um\nwe've got that up and running now i'm\ngoing to do a slightly uh\nlarger hyper parameter search okay um\nso remember we had just a couple for c\na couple for kernel um we're going to do\nmore we have some for shrinking now i'm\nactually\ngoing to comment that out because i\ndon't know how long that's going to take\num if you're coding them on binder now\nthis may actually take\nfar far too long for you um but we'll\nwe'll see so i'll execute this code and\nwe should see\njust sick no we shouldn't see any work\nhappening yet um\nbut what i'm doing here is\noh looks like okay my clusters back up\ngreat\nwe're doing our grid search but we're\ngoing to use um\ndask as as the back end right and this\nis a context manager where we're\nasserting that um\nand and we can just discuss the the\nsyntax there but it's not particularly\nimportant currently i'm going to execute\nthis now\nand\nlet's see\nfantastic we'll see all this um data\ntransfer happening here we'll see our\ntasks\num happening here we can see these big\nbatches of fit and score\nfit um so fitting fitting the models\nthen finding um\nhow well they perform uh via this\nk-fold cross validation\nwhich is really cool\nand\nlet's just yep we can see um\nwhat's happening here we can see we\ncurrently have 12 processing we've got\nseven in memory and we have um\nseveral more that we need to do uh our\ndesk workers we can see\nus oh we can see our cpu usage\nwe can see how we can see cpu usage\nacross all the workers which is which is\npretty cool seeing that distribution\nis uh is really nice whenever some form\nof b swarm\nplot if you have enough um would would\nbe useful there\nor even um some form of cumulative\ndistribution function or something like\nthat\num not a histogram people okay um you\ncan go to my bayesian tutorial\num that i've taught here before to hear\nme rave about um\nthe the horrors of histograms um\nso we saw that talk a minute which is\ngreat and we split it across\nyou know eight cores or whatever it is\nand now we'll have a look\nonce again we get the same best\nperformer which is which is a sanity\ncheck\num and that's pretty cool\ni think um we have a we actually have a\nfew minutes left so i am gonna\njust see if i can um\noh let me think\nyeah i will see if i can burst burst to\nthe cloud and and\nand do this um that will take uh a\nminute\na minute or two to create the cluster\nagain um but while we're while we're\ndoing that i'm wondering if we have any\nany questions um or if\nanyone has any feedback on on this\nworkshop i very much welcome\nwelcome that um perhaps if there are any\nfinal messages you'd\nyou'd like to say james while we're\nspinning this up you can\nyou can let me know yeah sure i just\nalso first off wanted to say thanks\neveryone for attending and like bearing\nbearing with us uh with the technical\ndifficulties really appreciate that\num real quick i'm just yeah so if you\nhave if you have questions please post\nin the q a section while the cold\ncluster's spinning up uh\ntheodore posted in the last largest\nexample of grid search\nhow much performance gain did we get\nfrom using das and not just in jobs\nhmm that's a great question and we\nactually\ndidn't see um let's see\nso it took 80 seconds\nah let me get this they're actually not\ncomparable\num because i did the grid search over\na different set of hyper parameters i\ndid it over a larger set of hyper\nparameters\num right so when i did um\nend jobs i did it there were only um it\nwas a two by two grid of hyper\nparameters\nwhereas when i did it um with with dusk\nit was a\none two three four five six six by three\nso let's just reason about that um this\none was\neighteen six by three is eighteen which\ntook eighty seconds\num and this one was two by two\nuh so it was four and it took\n26 seconds um\nso a minor gain i think with this hyper\nparameter\nsearch if you multiply that by by four\nyou'll\nwell 4.2 4.5 you'll need that would have\ntaken maybe two minutes or something\nsomething like that so we saw some\nincrease in efficiency\nnot a great deal but um james maybe you\ncan say more to this\npart of the reason for that is that\nwe're doing it on kind of a very small\nexample so we won't necessarily see the\ngains in efficiency\nwith a data set this size and with um a\nsmall hyper parameter suite like this is\nthat right\nyeah yeah and um yeah exactly and i\nguess also this is more of an uh kind of\nan illustrative point here\ni guess uh so you're just using uh\ndirectly using in jobs with\nsomething like job lib um by default\nwe'll use local threads and processes\non like whatever machine you happen to\nbe running on so\nlike in this case on hugo's laptop um\none of the real advantages of using\nuh job lib with the das back in will\nactually dispatch\nback to um to run tasks on a dash\ncluster is that your cluster can\nexpand beyond what local resources you\nhave\nso you can run um you know you can\nbasically scale out like for instance\nusing the coil cluster\nuh to have many many cpus and\na large amount of ram that you wouldn't\nhave on your locally uh table to run and\nthere you'll see\nboth large performance gains as well as\nyou'll be able to expand\nyour the set of possible problems you\ncan solve uh\nto larger than ram uh scenarios so\nyou're out of out of core training\nexactly and thank you jack this was\nabsolutely unplanned and we didn't plan\nthat question but that's a wonderful\nsegue into\nme now performing exactly the same\ncompute with the same code\nusing uh the dasc as the parallel back\nend um on a\non a coiled cluster which is an aws\ncluster right\num so we can i'm more currently anyway\nso i will execute\nthis code um and it's exactly the same\nas we did\num whoa\nokay great um so\nwe see our tasks task stream here\num\nyou see once again we see the majority\nis being batch\num uh fit and and getting the scores out\num\nsimilarly we see the same result being\nthe best\ni'll just notice that for this for this\nsmall task doing it on the cloud took 20\nseconds\nuh doing it locally for me took um 80\nseconds so that's a four-fold increase\nin performance\non a very small task so imagine what\nthat does if you can\ntake the same code as you've written\nhere and burst to the cloud\nuh with with one click or however\nhowever you do it um\ni think that that's incredibly powerful\nand that the fact that your code\nand what's happening in the back end\nwith dusk um generalizes immediately to\nthe new setting of working on a cluster\ni personally find very exciting and if\nyou work with larger data sets or\nbuilding larger models or big hyper\nparameter sweeps i'm pretty sure\num it's an exciting option for all of\nyou also um\nso on that note um i'd like to reiterate\njames what james said and thanking you\nall so much\nfor joining us um for asking great\nquestions\nand for bearing with us through some\nsome technical technical hurdles\nbut it made it even even funner when\nwhen we got up and running uh once again\ni'd love to thank\nmark christina and and the rest of the\norganizers for doing such a wonderful\njob\num and doing such a great service to uh\nthe data science and machine learning\ncommunity and ecosystem worldwide so\nthank you once again for having us\nthank you hugo and james um i have to\nsay like with all the technical\ndifficulties i was actually giggling\nbecause it was kind of funny um\nyeah but we're very sorry and we thank\nyou for your patience\nand sticking through it and um\ni will um be editing this video\nto um you know make it as efficient as\npossible\nand have that available tim supercard\nthank you um great and i'll just ask you\nif you are interested in checking out\ncoiled go to our website if you want to\ncheck out our product\ngo to cloud.coil.io we started building\nthis company in february\num we're really excited about building a\nnew product um so if you're interested\nreach out we'd love to chat with you\nabout what we're doing and what we're up\nto\num and it's wonderful to be in the same\ncommunity as you all so thanks\n", "idn": "16", "video_url": "https://youtu.be/MHAjCcBfT_A", "title_kw": "", "meta_json": "16.json", "audio_track": "16_MHAjCcBfT_A.mp4", "audio_text": "16_MHAjCcBfT_A.txt", "has_transcript": true, "status": "", "notes": ""}